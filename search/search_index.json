{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"AGNBoost","text":"<p>A powerful machine learning toolkit for astronomical data analysis using advanced XGBoost techniques.</p>"},{"location":"#overview","title":"Overview","text":"<p>AGNBoost is a specialized Python framework designed for astronomers working with photometric data. Built on the foundation of XGBoostLSS, AGNBoost provides a streamlined workflow for disitrubtional regression with photometric data, with particular focus on Active Galactic Nuclei (AGN) identification and galaxy property estimation.</p> <p>Quick Start</p> <p>New to AGNBoost? Check out our Tuotroials to get started!!</p>"},{"location":"#core-features","title":"Core Features","text":""},{"location":"#astronomy-focused-data-management","title":"Astronomy-Focused Data Management","text":"<ul> <li>Smart Catalog System: Load and manage astronomical data from FITS files, CSV, or pandas DataFrames</li> <li>Band Configuration: Flexible JSON-based configuration for photometric bands with metadata (wavelengths, shorthand names)</li> <li>Automatic Feature Engineering: Built-in color calculations, magnitude transformations, and signal-to-noise filtering</li> <li>Data Validation: Ensures data quality and compatibility across different datasets</li> </ul>"},{"location":"#convenient-pipeline","title":"Convenient Pipeline","text":"<ul> <li>Hyperparameter Optimization: Intelligent tuning with custom parameter grids and early stopping</li> <li>Cross-Validation: Robust model validation with stratified splitting for both classification and regression</li> <li>Model Persistence: Comprehensive saving and loading with full metadata tracking</li> </ul>"},{"location":"#xgboostlss-integration","title":"XGBoostLSS Integration","text":"<ul> <li>Distributional Modeling: Go beyond point estimates with full probability distributions</li> <li>Custom Objectives: Specialized loss functions for astronomical applications</li> <li>Efficient Training: Optimized for large astronomical datasets with GPU acceleration support</li> <li>Uncertainty Quantification: Robust uncertainty estimates for Astronomical analysis</li> </ul>"},{"location":"#research-ready-tools","title":"Research-Ready Tools","text":"<ul> <li>Flexible Data Splitting:  Train/validation/test splits with optional stratification</li> <li>Signal-to-Noise Filtering: Built-in S/N cuts for photometric data quality control</li> <li>Transform Pipeline: Easy-to-use data transformation and augmentation tools</li> <li>Extensible Architecture: Designed for customization and integration with existing workflows</li> </ul>"},{"location":"#installation","title":"Installation","text":"<p>We recommend using a virtual environment to manage dependencies. AGNBoost works best with Python 3.10/3.11 or later.</p>"},{"location":"#using-virtual-conda-environment-recommended","title":"Using Virtual Conda Environment (Recommended)","text":"<pre><code># Create and activate a conda environment\nconda create -n agnboost python=3.11\nconda activate agnboost\n\ngit clone https://github.com/hamblin-ku/AGNBoost.git\ncd AGNBoost\npip install -e\n</code></pre> <p>Otherwise, you can just clone the github repository and install the dependencies with pip:</p>"},{"location":"#quick-install","title":"Quick Install","text":"<p>To directly install the latest development version with pip, please use: <pre><code>git clone https://github.com/hamblin-ku/AGNBoost.git\ncd AGNBoost\npip install -e\n</code></pre></p> <p>Installation</p> <p>The \"-e\" flag installs AGNBoost in \"editable\" mode, which means that local changes to the source code will immediately be available without any further steps. This is particularly useful if you plan to contribute to the project or desire to customize AGNBoost for your research.</p>"},{"location":"#built-on-xgboostlss","title":"Built on XGBoostLSS","text":"<p>AGNBoost leverages the power of XGBoostLSS, a cutting-edge extension of XGBoost that enables distributional modeling. Instead of predicting single point estimates, XGBoostLSS models the entire conditioan distributions, providing:</p> <ul> <li>Full Distributional Predictions: Estimate not just the mean, but the entire shape of the target distribution</li> <li>Robust Uncertainty Quantification: Get principled uncertainty estimates for your astronomical measurements  </li> <li>Flexible Distributions: Choose from a wide variety of probability distributions tailored to your data</li> <li>Advanced Regularization: Built-in techniques to prevent overfitting and improve generalization</li> </ul> <p>This makes AGNBoost particularly powerful for astronomical applications where uncertainty quantification is crucial, such as:</p> <ul> <li>Photometric redshift estimation with confidence intervals</li> <li>AGN identification with uncertainty bounds</li> <li>Stellar mass estimation with full posterior distributions</li> </ul>"},{"location":"#whats-next","title":"What's Next?","text":"<ul> <li> <p>Tutorials</p> <p>Step-by-step guides covering everything from basic usage to advanced training pipelines.</p> <p>View Tutorials \u2192</p> </li> <li> <p>API Reference</p> <p>Complete documentation of all classes, methods, and functions in AGNBoost.</p> <p>API Docs \u2192</p> </li> </ul>"},{"location":"#community-and-support","title":"Community and Support","text":"<p>We welcome contributions, feedback, and collaboration from the community!</p> <ul> <li>GitHub Repository: https://github.com/hamblin-ku/AGNBoost</li> <li>Issues and Discussions: Use GitHub Issues for bug reports and feature requests</li> <li>Contributing: See our Contributing Guide to get involved</li> </ul> <p>AGNBoost is open-source software released under the MIT License. If you use AGNBoost in your research, please see our Citation Guide.</p>"},{"location":"api/","title":"API Reference","text":"<p>This page provides complete documentation for all AGNBoost classes and functions.</p>"},{"location":"api/#overview","title":"Overview","text":"<p>AGNBoost consists of two main classes:</p> <ul> <li>Catalog: For data loading, management, and feature engineering</li> <li>AGNBoost: For machine learning model training, tuning, and prediction</li> </ul>"},{"location":"api/#catalog-class","title":"Catalog Class","text":"<p>The <code>Catalog</code> class is your entry point for working with astronomical data. It handles loading data from various formats, validates photometric bands, creates features, and manages data splits.</p>"},{"location":"api/#key-features","title":"Key Features","text":"<ul> <li>Load FITS files, CSV files, or pandas DataFrames</li> <li>Automatic photometric band validation</li> <li>Feature engineering (colors, transformations)</li> <li>Train/validation/test data splitting</li> <li>Signal-to-noise filtering</li> </ul> <p>A class for loading, managing, and manipulating astronomical data.</p> <p>The Catalog class provides tools for loading astronomical data from various formats, performing feature engineering, data validation, and preparing data for machine learning workflows. It supports FITS files, CSV files, and pandas DataFrames.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>DataFrame</code> <p>The main astronomical dataset.</p> <code>features_df</code> <code>DataFrame</code> <p>Engineered features for machine learning.</p> <code>allowed_bands</code> <code>dict</code> <p>Dictionary storing the information (column name in data, shorthand name, and wavelength) for the used photometric bands</p> <code>valid_columns</code> <code>dict</code> <p>Dictionary storing the the photometric bands from allowed_bands that are found in the stored data.</p> <code>train_indices</code> <code>Index</code> <p>Indices for training data split.</p> <code>val_indices</code> <code>Index</code> <p>Indices for validation data split.</p> <code>trainval_indices</code> <code>Index</code> <p>Indices for combined training + validation data split.</p> <code>test_indices</code> <code>Index</code> <p>Indices for test data split.</p> <code>custom_feature_func</code> <code>callable</code> <p>User created function to create custom features.</p> <p>Examples:</p> <p>Basic usage with a FITS file:</p> <pre><code>from agnboost import Catalog\n\n# Load data\ncatalog = Catalog(path=\"jwst_data.fits\")\n\n# Create features\ncatalog.create_feature_dataframe()\n\n# Split data\ncatalog.split_data(test_size=0.2, val_size=0.2)\n</code></pre> <p>Loading data from a pandas DataFrame:</p> <pre><code>import pandas as pd\ndf = pd.read_csv(\"data.csv\")\ncatalog = Catalog(data=df)\n</code></pre>"},{"location":"api/#agnboost.dataset.Catalog.create_color_dataframe","title":"create_color_dataframe  <code>staticmethod</code>","text":"<pre><code>create_color_dataframe(data, bands)\n</code></pre> <p>Calculate photometric colors from band flux measurements.</p> <p>Creates all possible non-reciprocal photometric colors from the provided band data. Colors are calculated as log10(flux_i / flux_j) where band_i has a longer wavelength than band_j. This follows standard astronomical convention where colors like (J-H)  represent log10(flux_J / flux_H).</p> <p>The bands are automatically sorted by wavelength in descending order (longest to  shortest wavelength) to ensure consistent color naming and calculation. For example, with bands at 2.2\u03bcm, 1.6\u03bcm, and 1.2\u03bcm, this will create colors like: - 2.2\u03bcm/1.6\u03bcm (longer/shorter) - 2.2\u03bcm/1.2\u03bcm  - 1.6\u03bcm/1.2\u03bcm</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing flux measurements for each object. Must have columns corresponding to the band names in the <code>bands</code> dictionary.</p> required <code>bands</code> <code>dict</code> <p>Dictionary mapping band column names to their metadata. Each band entry must contain 'wavelength' and 'shorthand' keys. Example: <pre><code>{\n    'jwst.nircam.F200W': {'wavelength': 1.988, 'shorthand': 'F200W'},\n    'jwst.nircam.F150W': {'wavelength': 1.501, 'shorthand': 'F150W'}\n}\n</code></pre></p> required <p>Returns:</p> Type Description <p>pandas.DataFrame or None: DataFrame containing computed color columns with the same index as the input data. Column names follow the format \"shorthand_i/shorthand_j\" where shorthand_i corresponds to the longer wavelength band. Returns None if fewer than 2 bands are provided.</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If a band name in the <code>bands</code> dictionary is not found as a column in <code>data</code>.</p> <code>ValueError</code> <p>If flux values are zero or negative (causing log10 calculation errors).</p> <p>Examples:</p> <p>Basic color calculation:</p> <p>Using with a Catalog instance:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom agnboost import Catalog\n\ncatalog = Catalog(path=\"jwst_data.fits\")\nvalid_bands = catalog.get_valid_bands()\ncolors = Catalog.create_color_dataframe(catalog.get_data(), valid_bands)\n</code></pre> Notes <ul> <li>Colors are calculated as log10(longer_wavelength_flux / shorter_wavelength_flux)</li> <li>Zero or negative flux values will produce NaN or infinite values in the colors</li> <li>The method creates n*(n-1)/2 colors for n input bands</li> <li>Band sorting by wavelength ensures consistent color naming across different datasets</li> <li>Color names use shorthand identifiers for readability (e.g., 'F200W/F150W' vs full names)</li> </ul> See Also <p>create_feature_dataframe: Higher-level method that includes color creation transform: For applying custom transformations to individual columns</p>"},{"location":"api/#agnboost.dataset.Catalog.remove_negative_fluxes","title":"remove_negative_fluxes  <code>staticmethod</code>","text":"<pre><code>remove_negative_fluxes(data, bands)\n</code></pre> <p>Remove negative and zero flux values by replacing them with NaN.</p> <p>Negative or zero flux measurements are often unphysical in astronomical data and can cause issues with logarithmic calculations (magnitudes, colors). This method identifies such values in photometric band columns and replaces them with NaN to prevent downstream calculation errors while preserving the data structure.</p> <p>This is particularly important for: - Magnitude calculations: mag = -2.5 * log10(flux) - Color calculations: color = log10(flux1 / flux2)  - Feature transforamtions that use logarithmic transformations</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>DataFrame</code> <p>DataFrame containing astronomical measurements. Must include columns corresponding to the band names specified in <code>bands</code>.</p> required <code>bands</code> <code>dict</code> <p>Dictionary mapping band column names to their metadata. Only the keys (band column names) are used for processing. Example: <pre><code>{\n    'jwst.nircam.F200W': {'wavelength': 1.988, 'shorthand': 'F200W'},\n    'jwst.nircam.F150W': {'wavelength': 1.501, 'shorthand': 'F150W'}\n}\n</code></pre></p> required <p>Returns:</p> Type Description <p>pandas.DataFrame or None: Copy of the input DataFrame with negative and zero flux values replaced by NaN in the specified band columns. Other columns remain unchanged. Returns None if an error occurs during processing.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If a band column name is not found in the DataFrame columns.</p> <code>Exception</code> <p>For any other processing errors (logged and returns None).</p> <p>Examples:</p> <p>Clean flux measurements before magnitude calculation:</p> <p>Use with a Catalog instance:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom agnboost import Catalog\n\ncatalog = Catalog(path=\"survey_data.fits\")\nvalid_bands = catalog.get_valid_bands()\nclean_data = Catalog.remove_negative_fluxes(catalog.get_data(), valid_bands)\n\n# Now safe to calculate magnitudes and colors\nmagnitudes = -2.5 * np.log10(clean_data[band_columns])\n</code></pre> <p>Integration with feature engineering:</p> <pre><code># Clean data before creating features\ncatalog = Catalog(path=\"photometry.fits\")\nbands = catalog.get_valid_bands()\n\n# Remove problematic values\nclean_data = Catalog.remove_negative_fluxes(catalog.data, bands)\ncatalog.data = clean_data\n\n# Now create features safely\ncatalog.create_feature_dataframe()\n</code></pre> Notes <ul> <li>Automatically called by the dop_nan() Catalog class method</li> <li>Only modifies columns specified in the <code>bands</code> dictionary</li> <li>Non-band columns (IDs, coordinates, etc.) are left unchanged</li> <li>Creates a copy of the input data, preserving the original</li> <li>Zero flux values are also replaced as they cause issues with log calculations</li> <li>NaN values can be handled by downstream methods like <code>drop_nan()</code></li> <li>The method logs warnings for each column where negative values are found</li> </ul> See Also <p>drop_nan: Remove rows with NaN values after cleaning create_feature_dataframe: Higher-level method that may use cleaned data sn_cut: Alternative approach using signal-to-noise thresholds</p>"},{"location":"api/#agnboost.dataset.Catalog.get_features_from_phots","title":"get_features_from_phots","text":"<pre><code>get_features_from_phots(phot_df)\n</code></pre> <p>Generate the feature data frame for a single row (i.e., a single source) of data.</p> <p>Uses either the default feature creation, or if a custom feature function has previously been used to create features, it will call that.</p> <p>By default, the created features will include: 1. Log photometry: log10(flux) for each band  2. Colors: log10(flux_i / flux_j) for all band pairs  3. Squared colors: (color)\u00b2 terms</p> <p>Parameters:</p> Name Type Description Default <code>phot_df</code> <code>DataFrame</code> <p>DataFrame containing photometric flux measurements. Must have columns corresponding to the valid photometric bands found during catalog initialization. Typically this would be a subset of the main catalog data containing only the validated band columns.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame: DataFrame containing engineered features with the same index as the input photometric data. Feature columns include:</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If photometric DataFrame contains non-positive values that prevent log calculations.</p> <code>AttributeError</code> <p>If custom_feature_func is not callable when provided.</p> <code>KeyError</code> <p>If required band columns are missing from the photometric DataFrame.</p> <p>Examples:</p> <p>Default feature engineering with JWST bands:</p> <pre><code>import pandas as pd\nimport numpy as np\nfrom agnboost import Catalog\n\n# Load catalog with JWST data\ncatalog = Catalog(path=\"jwst_photometry.fits\")\n\n# Get photometric data for valid bands only\nvalid_bands = catalog.get_valid_bands()\nphot_data = catalog.get_data()[list(valid_bands.keys())]\n\n# Generate features\nfeatures = catalog.get_features_from_phots(phot_data)\nprint(f\"Created {features.shape[1]} features from {phot_data.shape[1]} bands\")\n\n# Example output columns:\n# ['jwst.nircam.F115W', 'jwst.nircam.F150W', 'jwst.nircam.F200W',  # log phot\n#  'F200W/F150W', 'F200W/F115W', 'F150W/F115W',                    # colors  \n#  'F200W/F150W^2', 'F200W/F115W^2', 'F150W/F115W^2']             # squared colors\n</code></pre> Notes <ul> <li>Data cleaning: Ensure input data doesn't contain negative or zero flux values   before calling this method, as log calculations will fail</li> <li>Custom functions: If using custom_feature_func, ensure it has only a (data)   non-default parameter and returns a DataFrame with the same index</li> <li>NaN handling: Any NaN values in input photometry will propagate through   the feature calculations</li> </ul> See Also <p>create_feature_dataframe: Higher-level method that handles the complete pipeline create_color_dataframe: Standalone color calculation method remove_negative_fluxes: Data cleaning for problematic flux values</p>"},{"location":"api/#agnboost.dataset.Catalog.create_feature_dataframe","title":"create_feature_dataframe","text":"<pre><code>create_feature_dataframe(custom_func=None, silent=False)\n</code></pre> <p>Create feature dataframe to use with AGNBoost.</p> <p>Uses either the default feature creation, or if a custom feature function is provided, that function will be used for feature creation.</p> <p>By default, the created features will include: 1. Log photometry: log10(flux) for each band  2. Colors: log10(flux_i / flux_j) for all band pairs  3. Squared colors: (color)\u00b2 terms</p> <p>The resulting feature dataframe is stored as <code>self.features_df</code> and can be used directly with AGNBoost models for training and prediction. If a custom feature function is provided, it will saved into self.custom_func</p> <p>Parameters:</p> Name Type Description Default <code>custom_func</code> <code>callable</code> <p>Custom function to generate features from the data. Must accept the catalog's data as input and return a pandas DataFrame with the same index as the original data. If None, uses the default feature pipeline. Defaults to None.</p> <code>None</code> <code>silent</code> <code>bool</code> <p>If True, suppresses logging output about feature creation. Useful for batch processing or when called repeatedly. Defaults to False.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.DataFrame or None: DataFrame containing engineered features with the same index as the catalog data. Also stored as <code>self.features_df</code>. Returns None if feature creation fails or if no valid data/columns are available.</p> <p>Raises:</p> Type Description <code>TypeError</code> <p>If <code>custom_func</code> is provided but is not callable, or if custom function doesn't return a pandas DataFrame.</p> <code>ValueError</code> <p>If custom function returns DataFrame with different length than catalog data.</p> <code>Exception</code> <p>If custom function returns DataFrame with mismatched index.</p> <p>Examples:</p> <p>Default feature engineering:</p> <pre><code>from agnboost import Catalog\n\n# Load astronomical data\ncatalog = Catalog(path=\"jwst_observations.fits\")\n\n# Create default features\nfeatures = catalog.create_feature_dataframe()\nprint(f\"Created {features.shape[1]} features for {features.shape[0]} objects\")\n\n# Features are automatically stored\nassert catalog.features_df is not None\nprint(\"Feature columns:\", list(catalog.features_df.columns))\n</code></pre> <p>Using with custom feature function:</p> <pre><code>def polynomial_features(data):\n    \"\"\"Create polynomial features from photometric data.\"\"\"\n    import pandas as pd\n    import numpy as np\n\n    # Get valid bands (this would be passed from catalog context)\n    feature_df = pd.DataFrame(index=data.index)\n\n    # Example: create polynomial features for first few bands\n    band_cols = [col for col in data.columns if 'jwst' in col][:3]\n\n    for band in band_cols:\n        if band in data.columns:\n            # Linear, quadratic, and cubic terms\n            feature_df[f\"{band}_log\"] = np.log10(data[band])\n            feature_df[f\"{band}_log2\"] = np.log10(data[band])**2\n            feature_df[f\"{band}_log3\"] = np.log10(data[band])**3\n\n    return feature_df\n\n# Use custom function\ncatalog = Catalog(path=\"survey_data.fits\")\ncustom_features = catalog.create_feature_dataframe(custom_func=polynomial_features)\n</code></pre> Notes <ul> <li>Data validation: Requires valid photometric bands identified during catalog initialization</li> <li>Index preservation: All feature operations maintain the original data index</li> <li>Memory usage: Default pipeline roughly triples the feature count (bands + colors + squared colors)</li> <li>NaN handling: Method reports NaN counts but doesn't remove them (use <code>drop_nan()</code> if needed)</li> <li>Custom functions: Must return DataFrame with identical index to original data</li> <li>Storage: Result is automatically stored as <code>self.features_df</code> for later use</li> <li>Error recovery: Returns None on failure while logging specific error details</li> </ul> Default Feature Set <p>For n photometric bands, creates approximately: - n log-photometry features: One per validated band - n(n-1)/2 color features: All unique band pair combinations - n(n-1)/2 squared color features: Squared versions of colors - Total: n + n(n-1) \u2248 n\u00b2 features for large n</p> See Also <p>get_features: Retrieve stored features (creates them if needed) remove_negative_fluxes: Clean data before feature creation split_data: Create train/validation/test splits of features get_features_from_phots: Lower-level feature creation from photometry subset</p>"},{"location":"api/#agnboost.dataset.Catalog.transform","title":"transform","text":"<pre><code>transform(column_name, transform_func, new_column_name=None, inplace=False)\n</code></pre> <p>Apply a transformation function to a column and save the result.</p> <p>This method provides a flexible way to transform data in a specified column using any callable function. The transformation can either replace the  original column (inplace=True) or create a new column with the transformed data.</p> <p>Parameters:</p> Name Type Description Default <code>column_name</code> <code>str</code> <p>Name of the column to transform. Must exist in the  loaded dataset. The column can contain any data type that the transformation function can handle.</p> required <code>transform_func</code> <code>callable</code> <p>Function to apply to the column. Should accept  a pandas.Series as input and return a pandas.Series, array-like object,  or scalar values. Examples: <pre><code>lambda x: x * 2              # Mathematical scaling\nnp.log                       # Logarithmic transformation\n</code></pre></p> required <code>new_column_name</code> <code>str or None, default=None</code> <p>Name for the transformed  column. If None and not inplace, will automatically generate a name using f\"{column_name}transformed\" or f\"{column_name}\"  if the function has a recognizable name attribute.</p> <code>None</code> <code>inplace</code> <code>bool, default=False</code> <p>If True, overwrite the original column  with transformed data. If False, create a new column with the  transformed data, preserving the original column.</p> <code>False</code> <p>Returns:</p> Type Description <p>pandas.Series or None: The transformed column as a pandas.Series if  successful, with proper indexing matching the original DataFrame. Returns None if transformation fails due to errors, invalid inputs, or missing data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the specified column_name is not found in the DataFrame.</p> <code>TypeError</code> <p>If transform_func is not callable.</p> <code>Exception</code> <p>For any transformation execution errors (logged and returns None).</p> <p>Examples:</p> <pre><code>Photometry logarithmic transformation:\n\n```python\n# Apply log10 transformation directly to the column\nimport numpy as np\ncatalog.transform('phot_col', np.log10)\n```\n</code></pre> Notes <ul> <li>The method automatically handles conversion of transformation results to pandas.Series</li> <li>Validates all input parameters before attempting transformation</li> <li>Maintains original DataFrame index in the transformed data</li> <li>Function names are automatically detected for column naming when possible</li> </ul>"},{"location":"api/#agnboost.dataset.Catalog.get_targets","title":"get_targets","text":"<pre><code>get_targets(target_names)\n</code></pre> <p>Extract target columns from the data.</p> <p>Parameters:</p> Name Type Description Default <code>target_names</code> <code> (str or list</code> <p>Name(s) of target column(s) to extract.</p> required <code>drop_na</code> <code> (bool, default=False</code> <p>If True, drop rows with NA values in any of the target columns.</p> required <p>Returns:</p> Type Description <p>pandas.DataFrame or pandas.Series: Target column(s). Returns a Series if a single target name is provided, or a DataFrame if multiple targets are requested.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_features","title":"get_features","text":"<pre><code>get_features()\n</code></pre> <p>Get the feature dataframe, creating it if it doesn't exist.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: The feature dataframe.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_feature_names","title":"get_feature_names","text":"<pre><code>get_feature_names()\n</code></pre> <p>Get the feature names dataframe, creating it if it doesn't exist.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: The names of the feature dataframe columns.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_length","title":"get_length","text":"<pre><code>get_length()\n</code></pre> <p>Get the length saved data from the Catalog instance.</p> <p>Returns:</p> Type Description <p>int or None: The length of the saved data, or else None</p>"},{"location":"api/#agnboost.dataset.Catalog.print_data_summary","title":"print_data_summary","text":"<pre><code>print_data_summary()\n</code></pre> <p>Print a formatted summary of the loaded data.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_valid_bands","title":"get_valid_bands","text":"<pre><code>get_valid_bands()\n</code></pre> <p>Get information about valid band columns in the data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of valid band columns with their metadata.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_valid_bands_list","title":"get_valid_bands_list","text":"<pre><code>get_valid_bands_list()\n</code></pre> <p>Get information about valid band columns in the data.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary of valid band columns with their metadata.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_band_wavelengths","title":"get_band_wavelengths","text":"<pre><code>get_band_wavelengths()\n</code></pre> <p>Get a dictionary mapping band columns to their wavelengths.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary mapping column names to wavelengths in microns.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_band_shorthands","title":"get_band_shorthands","text":"<pre><code>get_band_shorthands()\n</code></pre> <p>Get a dictionary mapping band columns to their shorthand names.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary mapping column names to shorthand identifiers.</p>"},{"location":"api/#agnboost.dataset.Catalog.sn_cut","title":"sn_cut","text":"<pre><code>sn_cut(columns=None, threshold=3.0, inplace=False, suffix='_err')\n</code></pre> <p>Perform a signal-to-noise ratio cut on the specified columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>dict or None</code> <p>Dictionary mapping value columns to their error columns. Example:      <pre><code>{\n'jwst.miri.F770W': 'jwst.miri.F770W_err',\n'jwst.miri.F1500W': 'jwst.miri.F1500W_err'\n}\n</code></pre> If None, automatically detect band columns and use {band: band+suffix} pairs.</p> <code>None</code> <code>threshold</code> <code>float, default=3.0</code> <p>Minimum S/N ratio to keep a row.</p> <code>3.0</code> <code>inplace</code> <code>bool, default=False</code> <p>If True, modifies the data in place; otherwise, returns a copy.</p> <code>False</code> <code>suffix</code> <code>str, default='_err'</code> <p>Suffix to append to band names to find error columns, if columns=None.</p> <code>'_err'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame or None: Filtered dataframe if inplace=False, otherwise None.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_data","title":"get_data","text":"<pre><code>get_data()\n</code></pre> <p>Return the loaded dataframe.</p> <p>Returns:</p> Type Description <p>pandas.DataFrame: The loaded data.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_columns","title":"get_columns","text":"<pre><code>get_columns()\n</code></pre> <p>Return the column names of the loaded dataframe.</p> <p>Returns:</p> Name Type Description <code>list</code> <p>List of column names.</p>"},{"location":"api/#agnboost.dataset.Catalog.get_subset","title":"get_subset","text":"<pre><code>get_subset(columns=None, rows=None)\n</code></pre> <p>Get a subset of the data by columns and/or rows.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list, default=None</code> <p>List of column names to include. If None, includes all columns.</p> <code>None</code> <code>rows</code> <code>slice, list, default=None</code> <p>Rows to include.  Can be a slice (e.g., slice(0, 100)), a list of indices,or None to include all rows.</p> <code>None</code> <p>Returns:</p> Type Description <p>pandas.DataFrame or None: Subset of the data, or None if no data to load.</p>"},{"location":"api/#agnboost.dataset.Catalog.save_to_csv","title":"save_to_csv","text":"<pre><code>save_to_csv(output_path, index=False)\n</code></pre> <p>Save the current data to a CSV file.</p> <p>Parameters:</p> Name Type Description Default <code>output_path</code> <code>str</code> <p>Path to save the CSV file.</p> required <code>index</code> <code>bool, default=False</code> <p>Whether to include the index in the saved file.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if successful, False otherwise.</p>"},{"location":"api/#agnboost.dataset.Catalog.drop_nan","title":"drop_nan","text":"<pre><code>drop_nan(columns=None, inplace=False, how='any')\n</code></pre> <p>Drop rows with NaN values in specified columns or validated columns.</p> <p>Parameters:</p> Name Type Description Default <code>columns</code> <code>list or None, default=None</code> <p>List of column names to check for NaN.  If None, uses all validated columns.</p> <code>None</code> <code>inplace</code> <code>bool, default=False</code> <p>If True, performs operation in-place and returns None. If False, returns a copy of the data with rows dropped.</p> <code>False</code> <code>how</code> <code>str, default='any'</code> <p>How to drop nan values. Options are: 'any' : Drop if any of the specified columns has NaN 'all' : Drop only if all of the specified columns have NaN</p> <code>'any'</code> <p>Returns:</p> Type Description <p>pandas.DataFrame or None: The DataFrame with NaN rows dropped, or None if inplace=True.</p>"},{"location":"api/#agnboost.dataset.Catalog.split_data","title":"split_data","text":"<pre><code>split_data(test_size=0.2, val_size=0.2, random_state=None, stratify_col=None, n_bins=10)\n</code></pre> <p>Split the data into train, validation, and test sets.</p> <p>Parameters:</p> Name Type Description Default <code>test_size</code> <code>float, default=0.2</code> <p>Proportion of the data to use for testing (0.0 to 1.0).</p> <code>0.2</code> <code>val_size</code> <code>float, default=0.2</code> <p>Proportion of the data to use for validation (0.0 to 1.0).</p> <code>0.2</code> <code>random_state</code> <code>int or None, default=None</code> <p>Random seed for reproducibility.</p> <code>None</code> <code>stratify_col</code> <code>str or None, default=None</code> <p>Column name to use for stratified splitting.  If provided, ensures proportional representation of this column's values in all splits. For continuous columns, binning will be applied.</p> <code>None</code> <code>n_bins</code> <code>int, default=10</code> <p>Number of bins to use when stratifying on a continuous column. Only used when stratify_col is a continuous variable.</p> <code>10</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(train_indices, val_indices, test_indices) - Indices for each split</p>"},{"location":"api/#agnboost.dataset.Catalog.get_split_df","title":"get_split_df","text":"<pre><code>get_split_df(split_type='train', include_features=True, include_target=None, return_DMatrix=False, missing=np.nan)\n</code></pre> <p>Get a dataframe for the specified data split.</p> <p>Parameters:</p> Name Type Description Default <code>split_type</code> <code>str, default='train'</code> <p>Which data split to use. Options: 'train', 'val'/'validation', or 'test'.</p> <code>'train'</code> <code>include_features</code> <code>bool, default=True</code> <p>If True, include the feature columns in the result.</p> <code>True</code> <code>include_target</code> <code>str, list, or None, default=None</code> <p>Target column(s) to include.  If None, no target columns are included.</p> <code>None</code> <code>return_DMatrix</code> <code>bool, default = False</code> <p>Whether to return a XGBoost DMatrix instead of a pandas Dataframe.</p> <code>False</code> <code>missing</code> <code>int, float, or None, default=np.nan</code> <p>Value to represent missing values in XGBoost.</p> <code>nan</code> <p>Returns:</p> Type Description <p>pandas.DataFrame: DataFrame for the specified split with requested columns.</p>"},{"location":"api/#agnboost-class","title":"AGNBoost Class","text":"<p>The <code>AGNBoost</code> class provides the machine learning functionality, including model training, hyperparameter tuning, and prediction with uncertainty quantification.</p>"},{"location":"api/#key-features_1","title":"Key Features","text":"<ul> <li>XGBoostLSS integration for distributional modeling</li> <li>Hyperparameter optimization</li> <li>Model persistence and loading</li> <li>Multi-target support</li> <li>Comprehensive logging and validation</li> </ul> <p>A machine learning framework for simultaneous AGN identification and photometric redshift estimation.</p> <p>AGNBoost utilizes the XGBoostLSS algorithm to predict both the fraction of mid-IR  3-30 \u03bcm emission attributable to an AGN power law (fracAGN) and photometric redshift  from JWST NIRCam and MIRI photometry. The framework analyzes 121 input features  derived from multi-band infrared observations, including original magnitudes, color  indices, and their squared values to enhance AGN discrimination. The computational  efficiency and scalability make AGNBoost well-suited for large-scale JWST surveys  requiring rapid AGN identification and redshift estimation.</p> <p>Attributes:</p> Name Type Description <code>models_dir</code> <code>str</code> <p>Directory path for storing/loading trained models.</p> <code>models</code> <code>dict</code> <p>Dictionary containing trained XGBoostLSS models for each target variable.</p> <code>model_info</code> <code>dict</code> <p>Dictionary storing metadata and performance information for each model.</p> <code>feature_names</code> <code>list</code> <p>Feature (column) names used for model input.</p> <code>target_variables</code> <code>dict</code> <p>Dictionary mapping target variable names to their probability distributions.</p> <code>logger</code> <code>Logger</code> <p>Logger instance for this AGNBoost object.</p> <code>ALLOWED_DISTS</code> <code>list</code> <p>Class-level list of allowed probability distributions ['ZABeta', 'Beta'].</p> <p>Examples:</p> <p>Basic usage with default settings:</p> <pre><code>from agnboost import AGNBoost\n\n# Initialize with defaults\nagnboost = AGNBoost()\n\n# Train models (assuming you have training data)\nagnboost.train(training_data)\n\n# Make predictions\npredictions = agnboost.predict(test_data)\n</code></pre> <p>Initialize with custom target variables:</p> <pre><code>targets = {'fagn': 'ZABeta', 'redshift': 'Beta'}\nagnboost = AGNBoost(target_variables=targets)\n</code></pre> <p>Initialize with custom features:</p> <pre><code>features = ['F115W', 'F150W', 'F200W', 'F277W', 'F770W', 'F1000W', 'F1130W']\nagnboost = AGNBoost(feature_names=features)\n</code></pre>"},{"location":"api/#agnboost.model.AGNBoost.get_models","title":"get_models","text":"<pre><code>get_models()\n</code></pre> <p>Get the feature dataframe, creating it if it doesn't exist.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Dictionary containing all models. Model values will be None if a model has not been loaded or trained.</p>"},{"location":"api/#agnboost.model.AGNBoost.tune_model","title":"tune_model","text":"<pre><code>tune_model(model_name, param_grid, dtune, split_type='train', max_minutes=10, nfold=2, early_stopping_rounds=100)\n</code></pre> <p>Tune hyperparameters for the specified model with Optuna.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to tune.  Must be in self.model_names.</p> required <code>param_grid</code> <code>dict</code> <p>Dictionary of hyperparameter ranges to search. Example:  <pre><code>param_grid = {\n    'max_depth': [\"int\", {\"low\": 1,      \"high\": 10,    \"log\": False}], \n    'eta': [\"float\", {\"low\": 1e-5,   \"high\": 1,     \"log\": True}]\n}\n</code></pre></p> required <code>dtune</code> <code>DMatrix or DataFrame or Catalog</code> <p>Data to use for tuning.  If DataFrame or Catalog, will be converted to DMatrix.</p> required <code>max_minutes</code> <code>int, default=10</code> <p>Maximum duration for tuning in minutes.</p> <code>10</code> <code>nfold</code> <code>int, default=2</code> <p>Number of cross-validation folds.</p> <code>2</code> <code>early_stopping_rounds</code> <code>int, default=100</code> <p>Number of rounds without improvement before early stopping.</p> <code>100</code> <p>Returns:</p> Type Description <p>dict or None: Dictionary containing best parameters and tuning metrics.  Returns None if error is encountered.</p>"},{"location":"api/#agnboost.model.AGNBoost.retrieve_dist","title":"retrieve_dist  <code>staticmethod</code>","text":"<pre><code>retrieve_dist(dist_name, response_fn=None, stab=None, loss_fn=None, params=None)\n</code></pre> <p>Retrieve a configured XGBoostLSS model.</p> <p>Creates and returns an XGBoostLSS object with the specified probability distribution and configuration parameters. </p> <p>Parameters:</p> Name Type Description Default <code>dist_name</code> <code>str</code> <p>Name of the probability distribution to use. Must be one of the values in ALLOWED_DISTS ['ZABeta', 'Beta'].</p> required <code>response_fn</code> <code>str</code> <p>Response function for the distribution parameters. Common options include 'exp', 'softplus'. If None and params is provided, uses value from params. Defaults to None.</p> <code>None</code> <code>stab</code> <code>str</code> <p>Stabilization method for gradients and Hessians to improve model convergence. Options include 'None', 'L2', 'MAD'. If None and params is provided, uses value from params. Defaults to None.</p> <code>None</code> <code>loss_fn</code> <code>str</code> <p>Loss function to use for training. Typically 'nll'  (negative log-likelihood). If None and params is provided, uses value from params. Defaults to None.</p> <code>None</code> <code>params</code> <code>dict</code> <p>Dictionary containing distribution parameters. If provided, overrides individual parameter arguments. Expected keys: 'stabilization', 'response_fn', 'loss_fn'. Defaults to None.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>XGBoostLSS</code> <p>Configured XGBoostLSS model with the specified distribution, or None if an error occurs during object creation.</p> <p>Raises:</p> Type Description <code>Exception</code> <p>If there's an error creating the XGBoostLSS object, logs the error and returns None.</p> <p>Examples:</p> <p>Create a ZABeta distribution with default parameters: <pre><code>dist_obj = AGNBoost.retrieve_dist(\"ZABeta\")\n</code></pre></p> <p>Create a Beta distribution with custom parameters: <pre><code>dist_obj = AGNBoost.retrieve_dist(\"Beta\", response_fn=\"softplus\", stab=\"L2\")\n</code></pre></p> <p>Create using a parameters dictionary: <pre><code>params = {'response_fn': 'softplus', 'stabilization': 'None', 'loss_fn': 'nll'}\ndist_obj = AGNBoost.retrieve_dist(\"ZABeta\", params=params)\n</code></pre></p>"},{"location":"api/#agnboost.model.AGNBoost.custom_objective","title":"custom_objective  <code>staticmethod</code>","text":"<pre><code>custom_objective(trial, hp_dict, dist, d_train, nfold, num_boost_round, early_stopping_rounds, seed)\n</code></pre> <p>Custom objective function for Optuna hyperparameter optimization.</p> <p>Defines the optimization objective for tuning hyperparameters using Optuna. The function constructs hyperparameter combinations based on the provided search space, trains models using cross-validation, and returns the validation loss for optimization. Supports automatic pruning of unpromising trials and handles various parameter types including categorical, float, and integer parameters.</p> <p>Parameters:</p> Name Type Description Default <code>trial</code> <code>Trial</code> <p>Optuna trial object used for suggesting hyperparameter values and managing the optimization process.</p> required <code>hp_dict</code> <code>dict</code> <p>Dictionary defining the hyperparameter search space. Each key is a parameter name, and values are tuples of (param_type, param_constraints) where param_type is one of ['categorical', 'float', 'int', 'none'] and param_constraints define the search range or options.</p> required <code>dist</code> <code>str</code> <p>Name of the probability distribution to use. Must be one of the values in ALLOWED_DISTS ['ZABeta', 'Beta'].</p> required <code>d_train</code> <code>DMatrix</code> <p>Training data in XGBoost DMatrix format for cross-validation.</p> required <code>nfold</code> <code>int</code> <p>Number of folds for cross-validation during hyperparameter optimization.</p> required <code>num_boost_round</code> <code>int</code> <p>Maximum number of boosting rounds to train.</p> required <code>early_stopping_rounds</code> <code>int</code> <p>Number of rounds with no improvement after which training will be stopped early.</p> required <code>seed</code> <code>int</code> <p>Random seed for reproducible results across trials.</p> required <p>Returns:</p> Name Type Description <code>float</code> <p>The minimum cross-validation test score (loss) achieved by the model with the suggested hyperparameters. Lower values indicate better performance.</p> Notes <p>The function automatically handles special XGBoostLSS parameters ('stabilization', 'response_fn', 'loss_fn') separately from standard XGBoost parameters to avoid warnings during model training. It also implements Optuna's XGBoost pruning callback to terminate unpromising trials early, improving optimization efficiency.</p>"},{"location":"api/#agnboost.model.AGNBoost.custom_tune_function","title":"custom_tune_function  <code>staticmethod</code>","text":"<pre><code>custom_tune_function(hp_dict, dist, dtrain, n_trials=None, max_minutes=10, num_boost_round=500, nfold=10, early_stopping_rounds=20, seed=123, metrics=None)\n</code></pre> <p>Perform hyperparameter optimization for AGNBoost models using Optuna. This offers the same  functionality of XGBoostLSS's built-in hyperparameter tuning, but also allows the tuning of  distributional parameters.</p> <p>Automatically selects the optimal hyperparameter tuning strategy based on the search space characteristics. Uses grid search for purely categorical parameters and Bayesian optimization (TPE) for mixed or continuous parameter spaces. Implements pruning to terminate unpromising trials early and provides comprehensive optimization results.</p> <p>Parameters:</p> Name Type Description Default <code>hp_dict</code> <code>dict</code> <p>Dictionary defining the hyperparameter search space. Each key is a parameter name, and values are tuples of (param_type, param_constraints) where param_type is one of ['categorical', 'float', 'int', 'none'].</p> required <code>dist</code> <code>str</code> <p>Name of the probability distribution to use. Must be one of the values in ALLOWED_DISTS ['ZABeta', 'Beta'].</p> required <code>dtrain</code> <code>DMatrix</code> <p>Training data in XGBoost DMatrix format for cross-validation.</p> required <code>n_trials</code> <code>int</code> <p>Maximum number of optimization trials to run. If None and all parameters are categorical, uses grid search to try all combinations. Defaults to None.</p> <code>None</code> <code>max_minutes</code> <code>int</code> <p>Maximum time limit for optimization in minutes.  Optimization will stop after this duration regardless of n_trials. Defaults to 10.</p> <code>10</code> <code>num_boost_round</code> <code>int</code> <p>Maximum number of boosting rounds for each trial. Defaults to 500.</p> <code>500</code> <code>nfold</code> <code>int</code> <p>Number of folds for cross-validation during optimization. Defaults to 10.</p> <code>10</code> <code>early_stopping_rounds</code> <code>int</code> <p>Number of rounds with no improvement after which training will be stopped early. Defaults to 20.</p> <code>20</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible optimization results. Defaults to 123.</p> <code>123</code> <code>metrics</code> <code>optional</code> <p>Additional metrics to track during optimization. Currently unused. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>optuna.Study: Completed Optuna study object containing optimization results, including best parameters, trial history, and performance metrics.</p> Notes <p>The function automatically determines the optimization strategy: if all parameters are categorical, it uses grid search to exhaustively try all combinations. Otherwise, it uses Tree-structured Parzen Estimator (TPE) for Bayesian optimization. Median pruning is applied to terminate unpromising trials early, improving efficiency. The optimal number of boosting rounds is automatically determined and included in the returned results.</p>"},{"location":"api/#agnboost.model.AGNBoost.plot_eval","title":"plot_eval  <code>staticmethod</code>","text":"<pre><code>plot_eval(evals, catalog, best_iter=None)\n</code></pre> <p>Plot the training and validation loss curves.</p> <p>Parameters:</p> Name Type Description Default <code>evals</code> <code>dict</code> <p>Dict of model evaluations.</p> required <code>catalog</code> <code>Catalog instance</code> <p>Catalog object used for training. Used to scale the training/validation curves according to the training/validation sizes.</p> required <p>Returns:</p> Type Description <p>matplotlib.pyplot.axis: The figure axis.</p>"},{"location":"api/#agnboost.model.AGNBoost.train_model","title":"train_model","text":"<pre><code>train_model(model_name, dtrain, dval=None, params=None, split_type='train', num_boost_round=1000, early_stopping_rounds=None, verbose_eval=False, custom_objective=None, custom_metric=None)\n</code></pre> <p>Train an AGNBoost model with the given parameters.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the model to train.  Must be in self.model_names.</p> required <code>dtrain</code> <code>DMatrix or Catalog</code> <p>Training data.  If a Catalog, will create a DMatrix using the specified split.</p> required <code>dval</code> <code>xgboost.DMatrix, bool, default=None</code> <p>Validation data.  If a DMatrix, use directly. If True and dtrain is a Catalog, create a validation DMatrix from the Catalog. If False or None, no validation data is used.</p> <code>None</code> <code>params</code> <code>dict, default=None</code> <p>Dict of model hyperparameters to use If None, uses saved tuned_params from model_info if available, otherwise uses default parameters.</p> <code>None</code> <code>split_type</code> <code>str, default='train'</code> <p>Which data split to use if dtrain is a Catalog.  Options: 'train', 'val', 'test'.</p> <code>'train'</code> <code>num_boost_round</code> <code>int, default=1000</code> <p>Number of boosting rounds.</p> <code>1000</code> <code>early_stopping_rounds</code> <code>int or None, default=50</code> <p>Number of rounds without improvement before early stopping. If None, no early stopping is used.</p> <code>None</code> <code>verbose_eval</code> <code>int or bool, default=100</code> <p>Controls XGBoost's logging frequency.  If True, prints every round. If int, prints every verbose_eval rounds.  If False, no logging.</p> <code>False</code> <code>custom_objective</code> <code>callable, default=None</code> <p>Custom objective function for XGBoost.</p> <code>None</code> <code>custom_metric</code> <code>callable, default=None</code> <p>Custom evaluation metric for XGBoost.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>tuple</code> <p>(trained_model, training_results)</p>"},{"location":"api/#agnboost.model.AGNBoost.get_available_files","title":"get_available_files  <code>staticmethod</code>","text":"<pre><code>get_available_files(path)\n</code></pre> <p>Get all available files in the provided path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path to the directory to check.</p> required <p>Returns:</p> Type Description <p>list or None: list of file paths, or None if the path does not exist.</p>"},{"location":"api/#agnboost.model.AGNBoost.get_available_models","title":"get_available_models","text":"<pre><code>get_available_models(target_name=None)\n</code></pre> <p>Get all available models. Will either get all available models for the provided target, or will return a list of the avilable model types (e.g., fracAGN, redshift, etc.).</p> <p>Parameters:</p> Name Type Description Default <code>target_name</code> <code>str, default=None</code> <p>name of model type to check.</p> <code>None</code> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the directory for the passed target does not yet exist. </p> <p>Returns:</p> Name Type Description <code>list</code> <p>list of available models or model types.</p>"},{"location":"api/#agnboost.model.AGNBoost.load_model","title":"load_model","text":"<pre><code>load_model(model_name, file_name=None, overwrite=False)\n</code></pre> <p>Load a pre-trained model and its metadata from into the AGNBoost instance.</p> <p>Deserializes a previously saved XGBoostLSS model from pickle or gzip format and restores it to the AGNBoost instance. Performs comprehensive validation including feature compatibility checks, model data structure verification, and metadata restoration. Supports automatic selection of the most recent model file if no specific filename is provided.</p> <p>Parameters:</p> Name Type Description Default <code>model_name</code> <code>str</code> <p>Name of the target variable model to load. Must correspond to a valid model type (e.g., 'fagn', 'z_transformed').</p> required <code>file_name</code> <code>str</code> <p>Specific filename of the model to load. If None, automatically selects the most recently modified file in the model subdirectory. Can include or omit the '.pkl' extension. Supports both '.pkl' and '.gz' formats. Defaults to None.</p> <code>None</code> <code>overwrite</code> <code>bool</code> <p>Whether to overwrite an already loaded model in the AGNBoost instance. If False and a model is already loaded, the operation will fail. Defaults to False.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>bool</code> <p>True if the model was loaded successfully, False if an error occurred during loading, validation, or if overwrite constraints were violated.</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the model subdirectory doesn't exist or no model files are found in the expected directory.</p> <code>UnpicklingError</code> <p>If the file is corrupted or not a valid pickle file.</p> <code>Exception</code> <p>For other unexpected errors during the loading process.</p> <p>Examples:</p> <p>Load the most recent model file automatically: <pre><code># Loads the most recently modified model file for 'fagn'\nsuccess = agnboost.load_model('fagn')\n</code></pre></p> <p>Load a specific model file: <pre><code># Load a specific model file\nsuccess = agnboost.load_model('z_transformed', 'my_redshift_model.pkl')\n</code></pre></p> <p>Load with filename without extension: <pre><code># The .pkl extension is automatically added\nsuccess = agnboost.load_model('fagn', 'final_agn_model')\n</code></pre></p> <p>Overwrite an existing loaded model: <pre><code># Replace currently loaded model with a different one\nsuccess = agnboost.load_model('fagn', 'newer_model.pkl', overwrite=True)\n</code></pre></p> <p>Load a compressed model: <pre><code># Automatically handles gzip compressed files\nsuccess = agnboost.load_model('z_transformed', 'compressed_model.pkl.gz')\n</code></pre></p> Notes <p>The method performs several validation steps: - Verifies input parameter types and values - Checks model subdirectory existence - Validates loaded model data structure - Compares feature compatibility between saved and current models - Ensures model names match expected targets</p> <p>Feature compatibility is strictly enforced - the loaded model must have been trained with identical features to the current AGNBoost instance. Any mismatch will result in loading failure with detailed logging of differences.</p> <p>The method supports both pickle (.pkl) and gzip-compressed (.pkl.gz) formats automatically based on file extension. Model metadata including training parameters, performance metrics, and timestamps are restored along with the model object.</p> <p>If no specific filename is provided, the method automatically selects the most recently modified file in the model subdirectory, making it easy to load the latest trained model.</p>"},{"location":"api/#agnboost.model.AGNBoost.dmatrix_validate","title":"dmatrix_validate","text":"<pre><code>dmatrix_validate(data, split_use=None)\n</code></pre> <p>Validate and convert input data to XGBoost DMatrix format.</p> <p>Handles multiple input data formats and converts them to the standardized DMatrix format required by XGBoost models. Supports Catalog objects with optional data split selection, and validates existing DMatrix objects. Performs feature extraction and handles missing value encoding for optimal XGBoost compatibility.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Catalog or DMatrix</code> <p>Input data to validate and convert. If Catalog, extracts features and optionally applies data splitting. If DMatrix, validates and passes through unchanged.</p> required <code>split_use</code> <code>str</code> <p>Specific data split to use when data is a Catalog. Common values include 'train', 'val', 'test', 'trainval'. If None and data is a Catalog, uses the entire feature dataset. Ignored if data is already a DMatrix. Defaults to None.</p> <code>None</code> <p>Returns:</p> Type Description <p>xgboost.DMatrix or None: Validated DMatrix object ready for XGBoost operations, or None if validation fails or an error occurs during conversion.</p>"},{"location":"api/#agnboost.model.AGNBoost.predict","title":"predict","text":"<pre><code>predict(data, model_name, split_use=None, seed=123)\n</code></pre> <p>Generate predictions using an internal trained XGBoostLSS model for the specified target variable.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>pandas.DataFrame, Catalog, or xgboost.DMatrix</code> <p>Input data for prediction. If Catalog, can optionally specify which data split to use. If DataFrame, must contain all required features. If DMatrix, used directly for prediction.</p> required <code>model_name</code> <code>str</code> <p>Name of the specific model to use for predictions. Must correspond to a trained model in the AGNBoost instance (e.g., 'fagn',  'z_transformed').</p> required <code>split_use</code> <code>str</code> <p>Specific data split to use when data is a Catalog object. Common values include 'train', 'val', 'test', 'trainval'. Ignored if data is not a Catalog. Defaults to None.</p> <code>None</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible prediction results. Ensures consistent outputs across multiple prediction runs with identical inputs. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <p>numpy.ndarray or None: Array of prediction expectation values for the target variable, or None if prediction fails due to data validation errors, model unavailability, or feature compatibility issues.</p> <p>Examples:</p> <p>Predict AGN fractions on test data: <pre><code># Using a Catalog with test split\nfagn_predictions = agnboost.predict(catalog, 'fagn', split_use='test')\n</code></pre></p> <p>Predict redshifts on new data: <pre><code># Using a pandas DataFrame\nz_predictions = agnboost.predict(new_data_df, 'z_transformed')\n</code></pre></p> <p>Predict on validation set with custom seed: <pre><code># Reproducible predictions on validation data\npredictions = agnboost.predict(catalog, 'fagn', split_use='val', seed=42)\n</code></pre></p> <p>Handle prediction errors gracefully: <pre><code>predictions = agnboost.predict(data, 'fagn')\nif predictions is not None:\n    print(f\"Generated {len(predictions)} predictions\")\nelse:\n    print(\"Prediction failed - check data and model status\")\n</code></pre></p> Notes <p>Prediction Type: Returns expectation values (means) of the predicted probability distributions rather than raw distribution parameters.</p> <p>When using the pre-trained models:     For AGN models, predictions typically represent fracAGN values (0-1 range).     For redshift models, predictions represent transformed redshift values that     may require inverse transformation to obtain physical redshift units.</p>"},{"location":"api/#agnboost.model.AGNBoost.prediction_uncertainty","title":"prediction_uncertainty","text":"<pre><code>prediction_uncertainty(uncertainty_type, catalog, model_name, split_use=None, seed=123, M=10, num_permutation=100)\n</code></pre> <p>Estimate total predictive uncertainty  for a trained model. Will return the model uncertainty,  uncertainty due to photometric error, or the combination of the two depending on the value of  'uncertainty_type.' This allows for robust uncertainty quantification for AGNBoost estimates, accounting for all types of uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>uncertainty_type</code> <code>str</code> <p>The type of uncertainty to estimate. Allowed values are: 'all', 'photometric', 'model'</p> required <code>catalog</code> <code>Catalog instance</code> <p>the Catalog object to take data from.</p> required <code>model_name</code> <code>str</code> <p>Name of the model to retrieve. Must be a key in the models dictionary and correspond to a valid target variable (e.g., 'fagn',  'z_transformed').</p> required <code>split_use</code> <code>str, default=None</code> <p>Specific data split to use when data is a Catalog object. Common values include 'train', 'val', 'test', 'trainval'. Ignored if data is not a Catalog. Defaults to None.</p> <code>None</code> <code>M</code> <code>int, default=10</code> <p>Number of models in the virtual ensemble. Controls the granularity of epistemic uncertainty estimation. Higher values provide more detailed uncertainty estimates but increase computation time. Defaults to 1.</p> <code>10</code> <code>seed</code> <code>int, default = 123</code> <p>Random seed for reproducible uncertainty estimates. Ensures consistent results across multiple runs with identical inputs. Defaults to 123.</p> <code>123</code> <code>num_permutation</code> <code>int, default=100</code> <p>Number of Monte Carlo iterations to run  if calculating the uncertainty due to photometric uncertainty.</p> <code>100</code> <p>Returns:</p> Type Description <p>np.ndarray or None: Array of total uncertainty estimates for each input sample. For single samples, returns a scalar value. For multiple samples, returns an array with one uncertainty value per input.</p>"},{"location":"api/#agnboost.model.AGNBoost.model_uncertainty","title":"model_uncertainty  <code>staticmethod</code>","text":"<pre><code>model_uncertainty(model: model, data: DMatrix, dist_name: str, best_iter: int, M: int = 1, seed=123) -&gt; np.array\n</code></pre> <p>Estimate total predictive uncertainty by decomposing epistemic and aleatoric components.</p> <p>Implements uncertainty quantification for XGBoostLSS models using a virtual ensemble approach based on boosting iteration truncation. Creates multiple model snapshots at different training stages to approximate epistemic (model) uncertainty, while extracting aleatoric (data) uncertainty from the predicted distributions. Returns the total uncertainty as the combination of both components.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>model</code> <p>Trained XGBoostLSS model object containing the booster, distribution, and starting values needed for prediction.</p> required <code>data</code> <code>DMatrix</code> <p>Input data in XGBoost DMatrix format for which uncertainty estimates will be computed. Can contain single or multiple samples.</p> required <code>dist_name</code> <code>str</code> <p>Name of the probability distribution used by the model. Must match one of the supported distribution types (e.g., 'ZABeta', 'Beta').</p> required <code>best_iter</code> <code>int</code> <p>Best iteration number from model training, typically obtained from early stopping or validation-based selection. Used as the reference point for creating the virtual ensemble.</p> required <code>M</code> <code>int</code> <p>Number of models in the virtual ensemble. Controls the granularity of epistemic uncertainty estimation. Higher values provide more detailed uncertainty estimates but increase computation time. Defaults to 1.</p> <code>1</code> <code>seed</code> <code>int</code> <p>Random seed for reproducible uncertainty estimates. Ensures consistent results across multiple runs with identical inputs. Defaults to 123.</p> <code>123</code> <p>Returns:</p> Type Description <code>array</code> <p>np.ndarray: Array of total uncertainty estimates for each input sample. For single samples, returns a scalar value. For multiple samples, returns an array with one uncertainty value per input.</p>"},{"location":"api/#agnboost.model.AGNBoost.uncertainty_phot","title":"uncertainty_phot  <code>staticmethod</code>","text":"<pre><code>uncertainty_phot(model: model, catalog: Catalog, dist_name: str, num_permutation=100, seed=123) -&gt; np.ndarray\n</code></pre> <p>Estimate the prediction uncertainty due to photometric uncertainty.  Performs monte carlo, randomly sampling the photometric bands  (assuming a normal flux distribution) accoridng to the photometric error bands  stored in the catalog object's data. The standard deviation of the monte carlo preditions is taken to be the prediction uncertainty due to photometric uncertainty.</p> <p>Parameters:</p> Name Type Description Default <code>model</code> <code>model</code> <p>The trained model to use to make predictions.</p> required <code>catalog</code> <code>Catalog instance</code> <p>the Catalog object to take data from.</p> required <code>dist_name</code> <code>str</code> <p>The name of the probability distribution type to make predictions with This should correspond to the distribution stored in self.target_variables[model_name]</p> required <code>seed</code> <code>int, default = 123</code> <p>Random seed for reproducible uncertainty estimates. Ensures consistent results across multiple runs with identical inputs. Defaults to 123.</p> <code>123</code> <code>num_permutation</code> <code>int, default=100</code> <p>Number of Monte Carlo iterations to run  if calculating the uncertainty due to photometric uncertainty.</p> <code>100</code> <p>Returns:</p> Type Description <code>ndarray</code> <p>np.ndarray or None: Array of the uncertainty estimates.</p>"},{"location":"api/#utility-functions","title":"Utility Functions","text":"<p>Helper functions for data processing, feature engineering, and model management.</p>"},{"location":"api/#agnboost.utils.log_message","title":"log_message","text":"<pre><code>log_message(message, level='INFO')\n</code></pre> <p>Log a message with the specified level.</p>"},{"location":"api/#agnboost.utils.log_message--parameters","title":"Parameters:","text":"<p>message : str     Message to log. level : str     Log level (INFO, WARNING, ERROR).</p>"},{"location":"api/#agnboost.utils.validate_param_grid","title":"validate_param_grid","text":"<pre><code>validate_param_grid(param_grid)\n</code></pre> <p>Validate that the parameter grid is correctly formatted.</p> <p>Expected format: {     \"param_name\": [\"type\", type_specific_value],     ... }</p> <p>Where \"type\" is one of: - \"none\": type_specific_value should be a list of specific values - \"int\": type_specific_value should be a dict with \"low\", \"high\", \"log\" keys - \"float\": type_specific_value should be a dict with \"low\", \"high\", \"log\" keys - \"categorical\": type_specific_value should be a list of possible values</p>"},{"location":"api/#agnboost.utils.validate_param_grid--parameters","title":"Parameters:","text":"<p>param_grid : dict     Dictionary of parameter grid specifications</p>"},{"location":"api/#agnboost.utils.validate_param_grid--returns","title":"Returns:","text":"<p>tuple     (is_valid, error_message) - (True, None) if valid, (False, error_msg) if invalid</p>"},{"location":"api/#agnboost.utils.validate_param_dict","title":"validate_param_dict","text":"<pre><code>validate_param_dict(params)\n</code></pre> <p>Validate that the passed params are a dictionary of single values</p> <p>Expected format: {     \"param_name\": value,     ... }</p>"},{"location":"api/#agnboost.utils.validate_param_dict--parameters","title":"Parameters:","text":"<p>params : dict     Dictionary of parameter grid specifications</p>"},{"location":"api/#agnboost.utils.validate_param_dict--returns","title":"Returns:","text":"<p>tuple     (is_valid, error_message) - (True, None) if valid, (False, error_msg) if invalid</p>"},{"location":"tutorials/CustomFeatures%2BTargets/","title":"Custom Features and Transformed Target Variables","text":"In\u00a0[2]: Copied! <pre># Set agnboost folder as root\nimport os\n\n# Navigate to the repository root (parent directory of notebooks/)\nos.chdir('..')\n\n# Verify we're in the right place\nprint(f\"Current directory: {os.getcwd()}\")\nprint(f\"Contents: {os.listdir('.')}\")\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom agnboost import dataset, model\n#from sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\nprint(\"AGNBoost Basic Usage Tutorial\")\nprint(\"=\" * 40)\n</pre> # Set agnboost folder as root import os  # Navigate to the repository root (parent directory of notebooks/) os.chdir('..')  # Verify we're in the right place print(f\"Current directory: {os.getcwd()}\") print(f\"Contents: {os.listdir('.')}\")  # Import necessary libraries import numpy as np import pandas as pd from agnboost import dataset, model #from sklearn.metrics import mean_squared_error  # Set random seed for reproducibility np.random.seed(123)  print(\"AGNBoost Basic Usage Tutorial\") print(\"=\" * 40) <pre>Current directory: /home/kurt/Documents/agnboost\nContents: ['pyproject.toml', 'README.md', 'models', 'notebooks', 'figures', '.gitignore', '.env', 'mkdocs.yml', '.github', 'data', 'models_all', 'docs', 'LICENSE', 'tests', '.git', 'agnboost']\n</pre> <pre>2025-05-31 20:31:50.184 | INFO     | agnboost.config:&lt;module&gt;:11 - PROJ_ROOT path is: /home/kurt/Documents/agnboost\n</pre> <pre>AGNBoost Basic Usage Tutorial\n========================================\n</pre> In\u00a0[3]: Copied! <pre># Load the astronomical data using the Catalog class\ncatalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = True)\n</pre> # Load the astronomical data using the Catalog class catalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = True)  <pre>Current working directory: /home/kurt/Documents/agnboost\nLooking for bands file at: /home/kurt/Documents/agnboost/agnboost/allowed_bands.json\n[INFO] Loaded bands file metadata: This file contains the allowed photometric bands for JWST\n[INFO] Loaded 11 allowed bands from agnboost/allowed_bands.json\n[INFO] Attempting to load file with delimiter: ','\n[INFO] Successfully loaded data with 1000 rows.\n[INFO] Found 11 valid band columns:\n[INFO]   - jwst.nircam.F115W (F115W): 1.154 \u03bcm\n[INFO]   - jwst.nircam.F150W (F150W): 1.501 \u03bcm\n[INFO]   - jwst.nircam.F200W (F200W): 1.988 \u03bcm\n[INFO]   - jwst.nircam.F277W (F277W): 2.776 \u03bcm\n[INFO]   - jwst.nircam.F356W (F356W): 3.565 \u03bcm\n[INFO]   - jwst.nircam.F410M (F410M): 4.083 \u03bcm\n[INFO]   - jwst.nircam.F444W (F444W): 4.402 \u03bcm\n[INFO]   - jwst.miri.F770W (F770W): 7.7 \u03bcm\n[INFO]   - jwst.miri.F1000W (F1000W): 10.0 \u03bcm\n[INFO]   - jwst.miri.F1500W (F1500W): 15.0 \u03bcm\n[INFO]   - jwst.miri.F2100W (F2100W): 21.0 \u03bcm\n\n================================================================================\nDATA SUMMARY: cigale_mock_small.csv\n================================================================================\nDimensions: 1000 rows \u00d7 26 columns\nMemory usage: 0.20 MB\n--------------------------------------------------------------------------------\nValid Band Columns:\n--------------------------------------------------------------------------------\nColumn Name                    Shorthand       Wavelength (\u03bcm)\n--------------------------------------------------------------------------------\njwst.nircam.F115W              F115W           1.154          \njwst.nircam.F150W              F150W           1.501          \njwst.nircam.F200W              F200W           1.988          \njwst.nircam.F277W              F277W           2.776          \njwst.nircam.F356W              F356W           3.565          \njwst.nircam.F410M              F410M           4.083          \njwst.nircam.F444W              F444W           4.402          \njwst.miri.F770W                F770W           7.700          \njwst.miri.F1000W               F1000W          10.000         \njwst.miri.F1500W               F1500W          15.000         \njwst.miri.F2100W               F2100W          21.000         \n--------------------------------------------------------------------------------\nColumn Information:\n--------------------------------------------------------------------------------\nColumn Name                    Type            Non-Null        Null %    \n--------------------------------------------------------------------------------\nIRAC1                          float64         1000/1000            0.00%     \nIRAC2                          float64         1000/1000            0.00%     \nIRAC3                          float64         1000/1000            0.00%     \nIRAC4                          float64         1000/1000            0.00%     \nhst.acs.wfc.F606W              float64         1000/1000            0.00%     \nhst.acs.wfc.F814W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F125W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F140W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F160W              float64         1000/1000            0.00%     \njwst.miri.F1000W               float64         1000/1000            0.00%     \njwst.miri.F1280W               float64         1000/1000            0.00%     \njwst.miri.F1500W               float64         1000/1000            0.00%     \njwst.miri.F1800W               float64         1000/1000            0.00%     \njwst.miri.F2100W               float64         1000/1000            0.00%     \njwst.miri.F770W                float64         1000/1000            0.00%     \njwst.nircam.F115W              float64         1000/1000            0.00%     \njwst.nircam.F150W              float64         1000/1000            0.00%     \njwst.nircam.F200W              float64         1000/1000            0.00%     \njwst.nircam.F277W              float64         1000/1000            0.00%     \njwst.nircam.F356W              float64         1000/1000            0.00%     \njwst.nircam.F410M              float64         1000/1000            0.00%     \njwst.nircam.F444W              float64         1000/1000            0.00%     \nsfh.sfr100Myrs                 float64         1000/1000            0.00%     \nstellar.m_star                 float64         1000/1000            0.00%     \nagn.fracAGN                    float64         1000/1000            0.00%     \nuniverse.redshift              float64         1000/1000            0.00%     \n--------------------------------------------------------------------------------\n\nNumeric Column Statistics:\n--------------------------------------------------------------------------------\nColumn               Mean         Std          Min          Max         \n--------------------------------------------------------------------------------\nIRAC1                57.9         1308         2.413e-06    4.098e+04   \nIRAC2                22.97        509.9        8.821e-07    1.596e+04   \nIRAC3                39.96        918          1.646e-06    2.879e+04   \nIRAC4                57.92        1309         2.413e-06    4.099e+04   \nhst.acs.wfc.F606W    0.311        5.52         0            169         \nhst.acs.wfc.F814W    0.3093       5.099        5.455e-13    155.8       \nhst.wfc3.ir.F125W    0.5148       6.576        1.614e-09    192.2       \nhst.wfc3.ir.F140W    0.6132       7.125        2.611e-09    196.7       \nhst.wfc3.ir.F160W    0.7412       7.991        4.119e-09    200.2       \njwst.miri.F1000W     57.54        1356         3.049e-06    4.257e+04   \njwst.miri.F1280W     71.4         1587         4.006e-06    4.97e+04    \njwst.miri.F1500W     74.16        1638         4.475e-06    5.129e+04   \njwst.miri.F1800W     82.2         1710         4.232e-06    5.339e+04   \njwst.miri.F2100W     87.79        1773         4.001e-06    5.527e+04   \njwst.miri.F770W      58.58        1315         2.288e-06    4.117e+04   \njwst.nircam.F115W    0.461        6.317        1.192e-09    188.6       \njwst.nircam.F150W    0.706        7.721        3.693e-09    198.7       \njwst.nircam.F200W    1.482        15.31        1.959e-08    280.6       \njwst.nircam.F277W    4.441        68.46        1.332e-07    2009        \njwst.nircam.F356W    11.62        225.3        4.48e-07     6973        \njwst.nircam.F410M    17.51        373          6.242e-07    1.164e+04   \njwst.nircam.F444W    21.65        477          8.29e-07     1.492e+04   \nsfh.sfr100Myrs       4.765        4.403        4.765e-27    15.79       \nstellar.m_star       3.51e+09     2.551e+09    3.367e+07    7.388e+09   \nagn.fracAGN          0.4993       0.3164       0            0.99        \nuniverse.redshift    1.765        1.811        0.01         7.999       \n================================================================================\n\n</pre> In\u00a0[4]: Copied! <pre># Get the list of the valid photometric bands\nVALID_BANDS = catalog.get_valid_bands_list()\nprint(f\"Valid bands: {VALID_BANDS# Navigate to the repository root (parent directory of notebooks/)\nos.chdir('..')\n\n# Verify we're in the right place\nprint(f\"Current directory: {os.getcwd()}\")\nprint(f\"Contents: {os.listdir('.')}\")}\")\n\ndef custom_feature_func(data: pd.DataFrame) -&gt; pd.DataFrame:\n    # Create all the features we want\n    log_phot_df = data[VALID_BANDS].apply( np.log10 )\n\n    f770w_f444w_color = np.log10( data['jwst.miri.F770W']/data['jwst.nircam.F444W'] ).rename(\"F770W/F444W\")\n    f2100W_f770W_color = np.log10( data['jwst.miri.F2100W']/data['jwst.miri.F770W'] ).rename(\"F2100W/F770W\")\n\n    z_df = data['universe.redshift']\n\n    # Combine them together so that they have shape (N_data, N_features)    \n    feature_df = pd.concat( [log_phot_df, f770w_f444w_color, f2100W_f770W_color, z_df], axis=1, join = 'outer')\n    return feature_df\n\n# Now, let's test this.\ntest_feature_df = custom_feature_func(data = catalog.get_data() )\nprint(f\"\\ntest_feature_df has shape {test_feature_df.shape} and original data has {len( catalog.get_data() )} rows.\")\nprint(f\"test_feature_df consists of {test_feature_df.shape[1]} features: {list(test_feature_df.columns)}\\n\")\n</pre> # Get the list of the valid photometric bands VALID_BANDS = catalog.get_valid_bands_list() print(f\"Valid bands: {VALID_BANDS# Navigate to the repository root (parent directory of notebooks/) os.chdir('..')  # Verify we're in the right place print(f\"Current directory: {os.getcwd()}\") print(f\"Contents: {os.listdir('.')}\")}\")  def custom_feature_func(data: pd.DataFrame) -&gt; pd.DataFrame:     # Create all the features we want     log_phot_df = data[VALID_BANDS].apply( np.log10 )      f770w_f444w_color = np.log10( data['jwst.miri.F770W']/data['jwst.nircam.F444W'] ).rename(\"F770W/F444W\")     f2100W_f770W_color = np.log10( data['jwst.miri.F2100W']/data['jwst.miri.F770W'] ).rename(\"F2100W/F770W\")      z_df = data['universe.redshift']      # Combine them together so that they have shape (N_data, N_features)         feature_df = pd.concat( [log_phot_df, f770w_f444w_color, f2100W_f770W_color, z_df], axis=1, join = 'outer')     return feature_df  # Now, let's test this. test_feature_df = custom_feature_func(data = catalog.get_data() ) print(f\"\\ntest_feature_df has shape {test_feature_df.shape} and original data has {len( catalog.get_data() )} rows.\") print(f\"test_feature_df consists of {test_feature_df.shape[1]} features: {list(test_feature_df.columns)}\\n\")  <pre>\n  Cell In[4], line 3\n    print(f\"Valid bands: {VALID_BANDS# Navigate to the repository root (parent directory of notebooks/)\n          ^\nSyntaxError: unterminated string literal (detected at line 3)\n</pre> <p>This custom feature dataframe is saved into our <code>catalog</code> instance, so we are good to continue from here as usual (i.e., following the same method in the <code>basic-usage.ipynb</code> example).</p> In\u00a0[\u00a0]: Copied! <pre>def mod_sigmoid_trans(z, a = 0.4):\n    trans_z =  2/ (1 + np.exp(-a*z)) - 1\n    return trans_z\n    \n# The 1e-8 is included in the log to avoid issues of log(0)\ndef inverse_mod_sigmoid_trans(trans_z, a = 0.4):\n    z = -(1/a)*np.log( 2/(1+trans_z) -1 + 1e-8)\n    return z\n</pre> def mod_sigmoid_trans(z, a = 0.4):     trans_z =  2/ (1 + np.exp(-a*z)) - 1     return trans_z      # The 1e-8 is included in the log to avoid issues of log(0) def inverse_mod_sigmoid_trans(trans_z, a = 0.4):     z = -(1/a)*np.log( 2/(1+trans_z) -1 + 1e-8)     return z   <p>We can now use this transformation to add a transformed redshift column to the saved data in our <code>catalog</code> instance. We will perform a sanity check to ensure that applying the inverse of our transformation to the transformed data matches the original data.</p> In\u00a0[\u00a0]: Copied! <pre># Let's create a name for the new column. \n# Note that this will also become the name of the model used to predict this transformed variable \n#     (and consequently the name of the directory the models will be saved in)\ntransformed_col_name = \"mod_sigmoid_redshift\"\n\ntransformed_redshift = catalog.transform( column_name = 'universe.redshift', \n                                         transform_func = mod_sigmoid_trans, \n                                         new_column_name = transformed_col_name\n                                        )\n\nprint(f\"Created transformed redshift column {transformed_redshift.name}\")\nprint(f\"Stored data now has columns: {catalog.get_data().columns}\")\n\n\n#--------------------\n\n# Let's ensure that taking the inverse transformation of this returns the redshifts to their original state\noriginal_z = catalog.get_data()['universe.redshift']\n\n# Apply the inverse transformation to our transformed redshift\ninverse_trans_z = inverse_mod_sigmoid_trans( transformed_redshift )\nmismatch_z = 0\n\n# Iterate through the arrays\nfor i in range(len(original_z)):  \n    if np.around(original_z.iloc[i], decimals = 3) != np.around(inverse_trans_z.iloc[i], decimals = 3):\n        mismatch_z += 1\n        print(f\"redshift mismatch. orig z: {original_z.iloc[i]:.3f}, recovered transformed z: {inverse_trans_z.iloc[i]:.3f}\")\n\nif mismatch_z == 0:\n    print(f\"\\nNo redshift mismatches after transorming and transforming back!\\n\")\n</pre> # Let's create a name for the new column.  # Note that this will also become the name of the model used to predict this transformed variable  #     (and consequently the name of the directory the models will be saved in) transformed_col_name = \"mod_sigmoid_redshift\"  transformed_redshift = catalog.transform( column_name = 'universe.redshift',                                           transform_func = mod_sigmoid_trans,                                           new_column_name = transformed_col_name                                         )  print(f\"Created transformed redshift column {transformed_redshift.name}\") print(f\"Stored data now has columns: {catalog.get_data().columns}\")   #--------------------  # Let's ensure that taking the inverse transformation of this returns the redshifts to their original state original_z = catalog.get_data()['universe.redshift']  # Apply the inverse transformation to our transformed redshift inverse_trans_z = inverse_mod_sigmoid_trans( transformed_redshift ) mismatch_z = 0  # Iterate through the arrays for i in range(len(original_z)):       if np.around(original_z.iloc[i], decimals = 3) != np.around(inverse_trans_z.iloc[i], decimals = 3):         mismatch_z += 1         print(f\"redshift mismatch. orig z: {original_z.iloc[i]:.3f}, recovered transformed z: {inverse_trans_z.iloc[i]:.3f}\")  if mismatch_z == 0:     print(f\"\\nNo redshift mismatches after transorming and transforming back!\\n\")  <p>We could then create an AGNBoost model to perform regression on the transformed redshift:</p> In\u00a0[\u00a0]: Copied! <pre># First, we will create the feature dataframe\ncatalog.create_feature_dataframe(silent = True)\n\nagnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),\n                              target_variables = {'mod_sigmoid_redshift' : 'Beta'}\n                           )\nprint(f\"AGNBoost object made with target varible name {list(agnboost_m.get_models().keys())[0]}. A {agnboost_m.get_models()['mod_sigmoid_redshift']} distribution is predicted for this target variable.\\n\")\n\n        \n</pre> # First, we will create the feature dataframe catalog.create_feature_dataframe(silent = True)  agnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),                               target_variables = {'mod_sigmoid_redshift' : 'Beta'}                            ) print(f\"AGNBoost object made with target varible name {list(agnboost_m.get_models().keys())[0]}. A {agnboost_m.get_models()['mod_sigmoid_redshift']} distribution is predicted for this target variable.\\n\")"},{"location":"tutorials/CustomFeatures%2BTargets/#custom-features-and-transformed-target-variables","title":"Custom Features and Transformed Target Variables\u00b6","text":"<p>This notebook demonstrates how to customize AGNBoost to your data needs through:</p> <ol> <li>Custom feature creation.</li> <li>Applying transformations to your target variables.</li> </ol> <p>Let's start by importing the necessary libraries and loading our data.</p>"},{"location":"tutorials/CustomFeatures%2BTargets/#loading-the-data","title":"Loading the Data\u00b6","text":"<p>We'll use the Catalog class to load our astronomical dataset. The <code>models-block-0.fits</code> file contains photometric measurements and AGN fraction labels for our analysis. We will load it and print out the data summary so we can easily see all the columns in the data.</p>"},{"location":"tutorials/CustomFeatures%2BTargets/#creating-custom-features","title":"Creating Custom Features\u00b6","text":"<p>By default, AGNBoost will create a feature dataframe that includes all of the photometric bands matching valid bands (in the bands.json), all non-reciprocal colors derived from those, and the squares of those colors. However, it is also simple to create a feature dataframe to only include features you want, including features not included by default.</p> <p>Let's say we want to create a feature dataframe that consists of:</p> <ol> <li>All the valid photometric bands. We will take the log10 of the fluxes.</li> <li>ONLY the F770W/F444W and F21000/F770W colors</li> <li>the redshift</li> </ol> <p>To do this, we must first create our custom function to create this dataframe from the data stored in our <code>catalog</code> object. Note that this function is expected to have only one input (the data), and needs to return a pandas Dataframe. We will create this function and then test it to ensure that it is working.</p>"},{"location":"tutorials/CustomFeatures%2BTargets/#applying-transformations-to-the-target-variable-eg-redshift","title":"Applying transformations to the target variable (e.g. redshift)\u00b6","text":"<p>You may wish to apply some form of transformation to your target variable (i.e., that which you are performing regression to predict). For example, in the AGNBoost paper (Hamblin+2025), we applied a modified sigmoid transformation to redshift, in order to transform redshift from the (theoretical) [0,inf) range to (0,1). This allowed us to predict a beta distribution with AGNBoost.</p> <p>This transformation functionality is built-in to AGNBoost. All we need to do is define the function for the transformation. Let's create the python function for the modified sigmoid transformation above. Note that this modified sigmoid transforamtion has an optional parameter a which is used to tune the effects of the transformation.</p> <p>We also need to define the inverse of the transformation, in order to transform the predictions from AGNBoost back to the untransformed space of interest (i.e., transformed-redshift space -&gt; redhisft space)</p>"},{"location":"tutorials/basic-usage/","title":"AGNBoost Basic Usage Tutorial","text":"In\u00a0[1]: Copied! <pre>%load_ext autoreload\n%autoreload 2\n</pre> %load_ext autoreload %autoreload 2 In\u00a0[2]: Copied! <pre># Set agnboost folder as root\nimport os\n\n# Navigate to the repository root (parent directory of notebooks/)\nos.chdir('..')\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom agnboost import dataset, model\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\nprint(\"AGNBoost Basic Usage Tutorial\")\nprint(\"=\" * 40)\n</pre> # Set agnboost folder as root import os  # Navigate to the repository root (parent directory of notebooks/) os.chdir('..')  # Import necessary libraries import numpy as np import pandas as pd from agnboost import dataset, model  # Set random seed for reproducibility np.random.seed(123)  print(\"AGNBoost Basic Usage Tutorial\") print(\"=\" * 40) <pre>2025-05-31 20:10:51.393 | INFO     | agnboost.config:&lt;module&gt;:11 - PROJ_ROOT path is: /home/kurt/Documents/agnboost\n</pre> <pre>AGNBoost Basic Usage Tutorial\n========================================\n</pre> In\u00a0[3]: Copied! <pre># Load the astronomical data using the Catalog class\ncatalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)\n</pre> # Load the astronomical data using the Catalog class catalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)  <pre>Current working directory: /home/kurt/Documents/agnboost\nLooking for bands file at: /home/kurt/Documents/agnboost/allowed_bands.json\n[INFO] Loaded bands file metadata: This file contains the allowed photometric bands for JWST\n[INFO] Loaded 11 allowed bands from agnboost/allowed_bands.json\n[INFO] Attempting to load file with delimiter: ','\n[INFO] Successfully loaded data with 1000 rows.\n[INFO] Found 11 valid band columns:\n[INFO]   - jwst.nircam.F115W (F115W): 1.154 \u03bcm\n[INFO]   - jwst.nircam.F150W (F150W): 1.501 \u03bcm\n[INFO]   - jwst.nircam.F200W (F200W): 1.988 \u03bcm\n[INFO]   - jwst.nircam.F277W (F277W): 2.776 \u03bcm\n[INFO]   - jwst.nircam.F356W (F356W): 3.565 \u03bcm\n[INFO]   - jwst.nircam.F410M (F410M): 4.083 \u03bcm\n[INFO]   - jwst.nircam.F444W (F444W): 4.402 \u03bcm\n[INFO]   - jwst.miri.F770W (F770W): 7.7 \u03bcm\n[INFO]   - jwst.miri.F1000W (F1000W): 10.0 \u03bcm\n[INFO]   - jwst.miri.F1500W (F1500W): 15.0 \u03bcm\n[INFO]   - jwst.miri.F2100W (F2100W): 21.0 \u03bcm\n</pre> In\u00a0[11]: Copied! <pre>catalog.allowed_bands\n</pre> catalog.allowed_bands Out[11]: <pre>{'jwst.nircam.F115W': {'shorthand': 'F115W', 'wavelength': 1.154},\n 'jwst.nircam.F150W': {'shorthand': 'F150W', 'wavelength': 1.501},\n 'jwst.nircam.F200W': {'shorthand': 'F200W', 'wavelength': 1.988},\n 'jwst.nircam.F277W': {'shorthand': 'F277W', 'wavelength': 2.776},\n 'jwst.nircam.F356W': {'shorthand': 'F356W', 'wavelength': 3.565},\n 'jwst.nircam.F410M': {'shorthand': 'F410M', 'wavelength': 4.083},\n 'jwst.nircam.F444W': {'shorthand': 'F444W', 'wavelength': 4.402},\n 'jwst.miri.F770W': {'shorthand': 'F770W', 'wavelength': 7.7},\n 'jwst.miri.F1000W': {'shorthand': 'F1000W', 'wavelength': 10.0},\n 'jwst.miri.F1500W': {'shorthand': 'F1500W', 'wavelength': 15.0},\n 'jwst.miri.F2100W': {'shorthand': 'F2100W', 'wavelength': 21.0}}</pre> In\u00a0[4]: Copied! <pre># Display comprehensive data summary\ncatalog.print_data_summary()\n\n# Check which photometric bands were validated\nvalid_bands = catalog.get_valid_bands()\nprint(f\"\\nValid photometric bands found: {len(valid_bands)}\")\nfor band_name, info in valid_bands.items():\n    print(f\"  {band_name}: {info['shorthand']} ({info['wavelength']} \u03bcm)\")\n\n# Check if our target variable exists\ntarget_column = 'agn.fracAGN'\nif target_column in catalog.get_data().columns:\n    print(f\"\\nTarget variable '{target_column}' found in dataset\")\n    target_stats = catalog.get_data()[target_column].describe()\n    print(\"Target variable statistics:\")\n    print(target_stats)\nelse:\n    print(f\"Warning: Target variable '{target_column}' not found in dataset\")\n    print(\"Available columns:\", list(catalog.get_data().columns))\n</pre> # Display comprehensive data summary catalog.print_data_summary()  # Check which photometric bands were validated valid_bands = catalog.get_valid_bands() print(f\"\\nValid photometric bands found: {len(valid_bands)}\") for band_name, info in valid_bands.items():     print(f\"  {band_name}: {info['shorthand']} ({info['wavelength']} \u03bcm)\")  # Check if our target variable exists target_column = 'agn.fracAGN' if target_column in catalog.get_data().columns:     print(f\"\\nTarget variable '{target_column}' found in dataset\")     target_stats = catalog.get_data()[target_column].describe()     print(\"Target variable statistics:\")     print(target_stats) else:     print(f\"Warning: Target variable '{target_column}' not found in dataset\")     print(\"Available columns:\", list(catalog.get_data().columns)) <pre>\n================================================================================\nDATA SUMMARY: cigale_mock_small.csv\n================================================================================\nDimensions: 1000 rows \u00d7 26 columns\nMemory usage: 0.20 MB\n--------------------------------------------------------------------------------\nValid Band Columns:\n--------------------------------------------------------------------------------\nColumn Name                    Shorthand       Wavelength (\u03bcm)\n--------------------------------------------------------------------------------\njwst.nircam.F115W              F115W           1.154          \njwst.nircam.F150W              F150W           1.501          \njwst.nircam.F200W              F200W           1.988          \njwst.nircam.F277W              F277W           2.776          \njwst.nircam.F356W              F356W           3.565          \njwst.nircam.F410M              F410M           4.083          \njwst.nircam.F444W              F444W           4.402          \njwst.miri.F770W                F770W           7.700          \njwst.miri.F1000W               F1000W          10.000         \njwst.miri.F1500W               F1500W          15.000         \njwst.miri.F2100W               F2100W          21.000         \n--------------------------------------------------------------------------------\nColumn Information:\n--------------------------------------------------------------------------------\nColumn Name                    Type            Non-Null        Null %    \n--------------------------------------------------------------------------------\nIRAC1                          float64         1000/1000            0.00%     \nIRAC2                          float64         1000/1000            0.00%     \nIRAC3                          float64         1000/1000            0.00%     \nIRAC4                          float64         1000/1000            0.00%     \nhst.acs.wfc.F606W              float64         1000/1000            0.00%     \nhst.acs.wfc.F814W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F125W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F140W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F160W              float64         1000/1000            0.00%     \njwst.miri.F1000W               float64         1000/1000            0.00%     \njwst.miri.F1280W               float64         1000/1000            0.00%     \njwst.miri.F1500W               float64         1000/1000            0.00%     \njwst.miri.F1800W               float64         1000/1000            0.00%     \njwst.miri.F2100W               float64         1000/1000            0.00%     \njwst.miri.F770W                float64         1000/1000            0.00%     \njwst.nircam.F115W              float64         1000/1000            0.00%     \njwst.nircam.F150W              float64         1000/1000            0.00%     \njwst.nircam.F200W              float64         1000/1000            0.00%     \njwst.nircam.F277W              float64         1000/1000            0.00%     \njwst.nircam.F356W              float64         1000/1000            0.00%     \njwst.nircam.F410M              float64         1000/1000            0.00%     \njwst.nircam.F444W              float64         1000/1000            0.00%     \nsfh.sfr100Myrs                 float64         1000/1000            0.00%     \nstellar.m_star                 float64         1000/1000            0.00%     \nagn.fracAGN                    float64         1000/1000            0.00%     \nuniverse.redshift              float64         1000/1000            0.00%     \n--------------------------------------------------------------------------------\n\nNumeric Column Statistics:\n--------------------------------------------------------------------------------\nColumn               Mean         Std          Min          Max         \n--------------------------------------------------------------------------------\nIRAC1                57.9         1308         2.413e-06    4.098e+04   \nIRAC2                22.97        509.9        8.821e-07    1.596e+04   \nIRAC3                39.96        918          1.646e-06    2.879e+04   \nIRAC4                57.92        1309         2.413e-06    4.099e+04   \nhst.acs.wfc.F606W    0.311        5.52         0            169         \nhst.acs.wfc.F814W    0.3093       5.099        5.455e-13    155.8       \nhst.wfc3.ir.F125W    0.5148       6.576        1.614e-09    192.2       \nhst.wfc3.ir.F140W    0.6132       7.125        2.611e-09    196.7       \nhst.wfc3.ir.F160W    0.7412       7.991        4.119e-09    200.2       \njwst.miri.F1000W     57.54        1356         3.049e-06    4.257e+04   \njwst.miri.F1280W     71.4         1587         4.006e-06    4.97e+04    \njwst.miri.F1500W     74.16        1638         4.475e-06    5.129e+04   \njwst.miri.F1800W     82.2         1710         4.232e-06    5.339e+04   \njwst.miri.F2100W     87.79        1773         4.001e-06    5.527e+04   \njwst.miri.F770W      58.58        1315         2.288e-06    4.117e+04   \njwst.nircam.F115W    0.461        6.317        1.192e-09    188.6       \njwst.nircam.F150W    0.706        7.721        3.693e-09    198.7       \njwst.nircam.F200W    1.482        15.31        1.959e-08    280.6       \njwst.nircam.F277W    4.441        68.46        1.332e-07    2009        \njwst.nircam.F356W    11.62        225.3        4.48e-07     6973        \njwst.nircam.F410M    17.51        373          6.242e-07    1.164e+04   \njwst.nircam.F444W    21.65        477          8.29e-07     1.492e+04   \nsfh.sfr100Myrs       4.765        4.403        4.765e-27    15.79       \nstellar.m_star       3.51e+09     2.551e+09    3.367e+07    7.388e+09   \nagn.fracAGN          0.4993       0.3164       0            0.99        \nuniverse.redshift    1.765        1.811        0.01         7.999       \n================================================================================\n\n\nValid photometric bands found: 11\n  jwst.nircam.F115W: F115W (1.154 \u03bcm)\n  jwst.nircam.F150W: F150W (1.501 \u03bcm)\n  jwst.nircam.F200W: F200W (1.988 \u03bcm)\n  jwst.nircam.F277W: F277W (2.776 \u03bcm)\n  jwst.nircam.F356W: F356W (3.565 \u03bcm)\n  jwst.nircam.F410M: F410M (4.083 \u03bcm)\n  jwst.nircam.F444W: F444W (4.402 \u03bcm)\n  jwst.miri.F770W: F770W (7.7 \u03bcm)\n  jwst.miri.F1000W: F1000W (10.0 \u03bcm)\n  jwst.miri.F1500W: F1500W (15.0 \u03bcm)\n  jwst.miri.F2100W: F2100W (21.0 \u03bcm)\n\nTarget variable 'agn.fracAGN' found in dataset\nTarget variable statistics:\ncount    1000.000000\nmean        0.499330\nstd         0.316352\nmin         0.000000\n25%         0.200000\n50%         0.500000\n75%         0.800000\nmax         0.990000\nName: agn.fracAGN, dtype: float64\n</pre> In\u00a0[5]: Copied! <pre># Create train/validation/test splitsget_train_val_test_sizes\ncatalog.split_data(test_size=0.2, val_size=0.2, random_state=42)\n\n# Get split information\nsplit_info = catalog.get_train_val_test_sizes()\nprint(\"Data split summary:\")\nprint(f\"  Total samples: {split_info['total']}\")\nprint(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\")\nprint(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\")\nprint(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\")\n</pre> # Create train/validation/test splitsget_train_val_test_sizes catalog.split_data(test_size=0.2, val_size=0.2, random_state=42)  # Get split information split_info = catalog.get_train_val_test_sizes() print(\"Data split summary:\") print(f\"  Total samples: {split_info['total']}\") print(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\") print(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\") print(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\") <pre>Data split summary:\n  Total samples: 1000\n  Training: 600 (60.0%)\n  Validation: 200 (20.0%)\n  Test: 200 (20.0%)\n</pre> In\u00a0[6]: Copied! <pre># Drop rows with NaN values in the validated columns\ncatalog.drop_nan(inplace=True)\n</pre> # Drop rows with NaN values in the validated columns catalog.drop_nan(inplace=True)  <pre>[INFO] No rows with NaN values found in the specified columns.\n</pre> <p>There are no-nan rows to remove since the CIGALE mock data we loaded has none, but your real data might.</p> In\u00a0[7]: Copied! <pre># Create features for modeling\ncatalog.create_feature_dataframe()\n\n# Get information about created features\nfeatures = catalog.get_features()\nprint(f\"Feature engineering complete:\")\nprint(f\"  Feature dataframe shape: {features.shape}\")\n</pre> # Create features for modeling catalog.create_feature_dataframe()  # Get information about created features features = catalog.get_features() print(f\"Feature engineering complete:\") print(f\"  Feature dataframe shape: {features.shape}\")  <pre>[INFO] Created feature dataframe with 121 columns and 1000 rows.\n[INFO] Created features are: ['jwst.nircam.F115W', 'jwst.nircam.F150W', 'jwst.nircam.F200W', 'jwst.nircam.F277W', 'jwst.nircam.F356W', 'jwst.nircam.F410M', 'jwst.nircam.F444W', 'jwst.miri.F770W', 'jwst.miri.F1000W', 'jwst.miri.F1500W', 'jwst.miri.F2100W', 'F2100W/F1500W', 'F2100W/F1000W', 'F2100W/F770W', 'F2100W/F444W', 'F2100W/F410M', 'F2100W/F356W', 'F2100W/F277W', 'F2100W/F200W', 'F2100W/F150W', 'F2100W/F115W', 'F1500W/F1000W', 'F1500W/F770W', 'F1500W/F444W', 'F1500W/F410M', 'F1500W/F356W', 'F1500W/F277W', 'F1500W/F200W', 'F1500W/F150W', 'F1500W/F115W', 'F1000W/F770W', 'F1000W/F444W', 'F1000W/F410M', 'F1000W/F356W', 'F1000W/F277W', 'F1000W/F200W', 'F1000W/F150W', 'F1000W/F115W', 'F770W/F444W', 'F770W/F410M', 'F770W/F356W', 'F770W/F277W', 'F770W/F200W', 'F770W/F150W', 'F770W/F115W', 'F444W/F410M', 'F444W/F356W', 'F444W/F277W', 'F444W/F200W', 'F444W/F150W', 'F444W/F115W', 'F410M/F356W', 'F410M/F277W', 'F410M/F200W', 'F410M/F150W', 'F410M/F115W', 'F356W/F277W', 'F356W/F200W', 'F356W/F150W', 'F356W/F115W', 'F277W/F200W', 'F277W/F150W', 'F277W/F115W', 'F200W/F150W', 'F200W/F115W', 'F150W/F115W', 'F2100W/F1500W^2', 'F2100W/F1000W^2', 'F2100W/F770W^2', 'F2100W/F444W^2', 'F2100W/F410M^2', 'F2100W/F356W^2', 'F2100W/F277W^2', 'F2100W/F200W^2', 'F2100W/F150W^2', 'F2100W/F115W^2', 'F1500W/F1000W^2', 'F1500W/F770W^2', 'F1500W/F444W^2', 'F1500W/F410M^2', 'F1500W/F356W^2', 'F1500W/F277W^2', 'F1500W/F200W^2', 'F1500W/F150W^2', 'F1500W/F115W^2', 'F1000W/F770W^2', 'F1000W/F444W^2', 'F1000W/F410M^2', 'F1000W/F356W^2', 'F1000W/F277W^2', 'F1000W/F200W^2', 'F1000W/F150W^2', 'F1000W/F115W^2', 'F770W/F444W^2', 'F770W/F410M^2', 'F770W/F356W^2', 'F770W/F277W^2', 'F770W/F200W^2', 'F770W/F150W^2', 'F770W/F115W^2', 'F444W/F410M^2', 'F444W/F356W^2', 'F444W/F277W^2', 'F444W/F200W^2', 'F444W/F150W^2', 'F444W/F115W^2', 'F410M/F356W^2', 'F410M/F277W^2', 'F410M/F200W^2', 'F410M/F150W^2', 'F410M/F115W^2', 'F356W/F277W^2', 'F356W/F200W^2', 'F356W/F150W^2', 'F356W/F115W^2', 'F277W/F200W^2', 'F277W/F150W^2', 'F277W/F115W^2', 'F200W/F150W^2', 'F200W/F115W^2', 'F150W/F115W^2']\nFeature engineering complete:\n  Feature dataframe shape: (1000, 121)\n</pre> In\u00a0[8]: Copied! <pre># Initialize an AGNBoost model. The target variable is the name of the target variable column, and its value in the passed dictionary is the distribution used to model it.\nagnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),\n                          target_variables = {'agn.fracAGN' : 'ZABeta'},\n                         )\n\n# Load pre-trained models. We will not pass a filename to load, and will simply the the most recent fracAGN model.\nagnboost_m.load_model(model_name = 'fracAGN', overwrite = True)\n\nif agnboost_m.models['agn.fracAGN'] is not None:\n    print(\"\u2705 Pre-trained model loaded successfully!\")\n    \n    # Display model information\n    model_info = agnboost_m.model_info.get('agn.fracAGN', {})\n    if model_info:\n        print(\"\\nModel information:\")\n        if 'training_timestamp' in model_info:\n            print(f\"  Trained: {model_info['training_timestamp']}\")\n        if 'best_score' in model_info:\n            print(f\"  Best validation score: {model_info['best_score']:.6f}\")\n        if 'features' in model_info:\n            print(f\"  Number of features: {len(model_info['features'])}\")\nelse:\n    print(\"\u274c No pre-trained models found!\")\n    print(\"You may need to train a new model or check the models directory.\")\n</pre> # Initialize an AGNBoost model. The target variable is the name of the target variable column, and its value in the passed dictionary is the distribution used to model it. agnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),                           target_variables = {'agn.fracAGN' : 'ZABeta'},                          )  # Load pre-trained models. We will not pass a filename to load, and will simply the the most recent fracAGN model. agnboost_m.load_model(model_name = 'fracAGN', overwrite = True)  if agnboost_m.models['agn.fracAGN'] is not None:     print(\"\u2705 Pre-trained model loaded successfully!\")          # Display model information     model_info = agnboost_m.model_info.get('agn.fracAGN', {})     if model_info:         print(\"\\nModel information:\")         if 'training_timestamp' in model_info:             print(f\"  Trained: {model_info['training_timestamp']}\")         if 'best_score' in model_info:             print(f\"  Best validation score: {model_info['best_score']:.6f}\")         if 'features' in model_info:             print(f\"  Number of features: {len(model_info['features'])}\") else:     print(\"\u274c No pre-trained models found!\")     print(\"You may need to train a new model or check the models directory.\") <pre>2025-05-31 20:10:53,543 - AGNBoost.AGNBoost - WARNING - No file_name passed. Using the most recently modified one instead: 2025_05_22-PM06_59_58_agn.fracAGN_model.pkl.gz.\n</pre> <pre>\u2705 Pre-trained model loaded successfully!\n\nModel information:\n  Best validation score: -649218.125000\n  Number of features: 121\n</pre> In\u00a0[9]: Copied! <pre># Make predictions on the test set\n#agnboost_m.models['agn.fracAGN'].booster.set_param( {'device': 'cpu'})\npreds = agnboost_m.predict( data = catalog, split_use = 'test', model_name = 'agn.fracAGN')\n\nprint(f\"  Mean: {np.mean(preds):.6f}\")\nprint(f\"  Std: {np.std(preds):.6f}\")\nprint(f\"  Min: {np.min(preds):.6f}\")\nprint(f\"  Max: {np.max(preds):.6f}\")\n</pre> # Make predictions on the test set #agnboost_m.models['agn.fracAGN'].booster.set_param( {'device': 'cpu'}) preds = agnboost_m.predict( data = catalog, split_use = 'test', model_name = 'agn.fracAGN')  print(f\"  Mean: {np.mean(preds):.6f}\") print(f\"  Std: {np.std(preds):.6f}\") print(f\"  Min: {np.min(preds):.6f}\") print(f\"  Max: {np.max(preds):.6f}\")  <pre>2025-05-31 20:10:55,651 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the test set stored in the passed Catalog.\n</pre> <pre>  Mean: 0.504962\n  Std: 0.325251\n  Min: 0.000308\n  Max: 0.989859\n</pre> In\u00a0[10]: Copied! <pre>model_uncertainty = agnboost_m.prediction_uncertainty( uncertainty_type = 'model', model_name = 'agn.fracAGN', catalog = catalog)\n\nprint(f\"\u2705 Uncertainty estimates generated\")\nprint(f\"Uncertainty statistics:\")\nprint(f\"  Mean uncertainty: {np.mean(model_uncertainty):.6f}\")\nprint(f\"  Std uncertainty: {np.std(model_uncertainty):.6f}\")\nprint(f\"  Min uncertainty: {np.min(model_uncertainty):.6f}\")\nprint(f\"  Max uncertainty: {np.max(model_uncertainty):.6f}\")\n</pre> model_uncertainty = agnboost_m.prediction_uncertainty( uncertainty_type = 'model', model_name = 'agn.fracAGN', catalog = catalog)  print(f\"\u2705 Uncertainty estimates generated\") print(f\"Uncertainty statistics:\") print(f\"  Mean uncertainty: {np.mean(model_uncertainty):.6f}\") print(f\"  Std uncertainty: {np.std(model_uncertainty):.6f}\") print(f\"  Min uncertainty: {np.min(model_uncertainty):.6f}\") print(f\"  Max uncertainty: {np.max(model_uncertainty):.6f}\") <pre>2025-05-31 20:10:55,838 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the None set stored in the passed Catalog.\nProcessing truncated model uncertainty:  38%|\u258d| 376/1000 [03:06&lt;05:09,  2.02it/s\n</pre> <pre>\n---------------------------------------------------------------------------\nKeyboardInterrupt                         Traceback (most recent call last)\nCell In[10], line 1\n----&gt; 1 model_uncertainty = agnboost_m.prediction_uncertainty( uncertainty_type = 'model', model_name = 'agn.fracAGN', catalog = catalog)\n      3 print(f\"\u2705 Uncertainty estimates generated\")\n      4 print(f\"Uncertainty statistics:\")\n\nFile ~/Documents/agnboost/agnboost/model.py:975, in AGNBoost.prediction_uncertainty(self, uncertainty_type, catalog, model_name, split_use, seed, M, num_permutation)\n    970 # Load the saved model + validate the features\n    971 xgblss_m = self.get_model(model_name)\n    973 match uncertainty_type:\n    974     case 'model':\n--&gt; 975         model_uncertainty = self.model_uncertainty( model = xgblss_m,\n    976                                                      data = dmatrix, \n    977                                                      dist_name = self.target_variables[model_name],\n    978                                                      best_iter = xgblss_m.booster.best_iteration, \n    979                                                      M = M\n    980                                                      )\n    981         return model_uncertainty\n    982 \n    983     case 'photometric':\n    984         uncertainty_from_phot_err = self.uncertainty_phot( model =xgblss_m,\n    985                                                             catalog = catalog,\n    986                                                             dist_name = self.target_variables[model_name],\n    987                                                             num_permutation = num_permutation,\n    988                                                             seed = seed\n    989                                                             )\n    990         return uncertainty_from_phot_err\n    991 \n    992     case 'all':\n    993         model_uncertainty = self.model_uncertainty( model = xgblss_m,\n    994                                                      data = dmatrix, \n    995                                                      dist_name = self.target_variables[model_name],\n    996                                                      best_iter = xgblss_m.booster.best_iteration, \n    997                                                      M = M\n    998                                                      )\n    999 \n   1000         uncertainty_from_phot_err = self.uncertainty_phot( model =xgblss_m,\n   1001                                                             catalog = catalog,\n   1002                                                             dist_name = self.target_variables[model_name],\n   1003                                                             num_permutation = num_permutation,\n   1004                                                             seed = seed\n   1005                                                             )\n   1006 \n   1007         return np.array( [np.sqrt(model_uncertainty[i]**2 + uncertainty_from_phot_err[i]**2) for i in range( catalog.get_length() )] )\n   1009 return None\n\nFile ~/Documents/agnboost/agnboost/model.py:1100, in AGNBoost.model_uncertainty(model, data, dist_name, best_iter, M, seed)\n   1095     for j in range(len(iteration_grid)):\n   1096         # Need to add 1 to iteration grid value because booster.preeict evaluates on [a, b)\n   1097         pred_params = predict_dist_trunc(dist = model.dist, booster = model.booster, start_values = model.start_values, data = data.slice([i]), iteration_range = (0, int(iteration_grid[j]) + 1) )\n-&gt; 1100         pred_mus[i, j] = predict_mean( trained_model = model, \n   1101                                     dist_name = dist_name,\n   1102                                     data = data.slice([i]),\n   1103                                     seed = seed\n   1104                                      )\n   1106         pred_stds[i, j] = predict_std( trained_model = model, \n   1107                     dist_name = dist_name,\n   1108                     data = data.slice([i]),\n   1109                     seed = seed\n   1110                      )\n   1112 epistemic_uncert = pred_mus.std(axis = 1)\n\nFile ~/Documents/agnboost/agnboost/utils.py:219, in predict_mean(trained_model, dist_name, data, seed)\n    213 def predict_mean( trained_model: xgboostlss_model,\n    214                 dist_name : str,\n    215                 data : DMatrix,\n    216                 seed : int = 123\n    217                     ):\n--&gt; 219     pred_params = trained_model.predict(data = data, pred_type = \"parameters\", seed = seed)\n    221     match dist_name:\n    222         case \"ZABeta\":\n    223             mu_array = predict_mean_fromparams_ZABeta( params = pred_params,  seed = seed)\n    225     return mu_array\n\nFile ~/anaconda3/envs/python311_clone/lib/python3.11/site-packages/xgboostlss/model.py:499, in XGBoostLSS.predict(self, data, pred_type, n_samples, quantiles, seed)\n    472 \"\"\"\n    473 Function that predicts from the trained model.\n    474 \n   (...)\n    495     Predictions.\n    496 \"\"\"\n    498 # Predict\n--&gt; 499 predt_df = self.dist.predict_dist(booster=self.booster,\n    500                                   start_values=self.start_values,\n    501                                   data=data,\n    502                                   pred_type=pred_type,\n    503                                   n_samples=n_samples,\n    504                                   quantiles=quantiles,\n    505                                   seed=seed)\n    507 return predt_df\n\nFile ~/anaconda3/envs/python311_clone/lib/python3.11/site-packages/xgboostlss/distributions/distribution_utils.py:468, in DistributionClass.predict_dist(self, booster, start_values, data, pred_type, n_samples, quantiles, seed)\n    465 base_margin_test = (np.ones(shape=(data.num_row(), 1))) * start_values\n    466 data.set_base_margin(base_margin_test.flatten())\n--&gt; 468 predt = np.array(booster.predict(data, output_margin=True)).reshape(-1, self.n_dist_param)\n    469 predt = torch.tensor(predt, dtype=torch.float32)\n    471 # Transform predicted parameters to response scale\n\nFile ~/anaconda3/envs/python311_clone/lib/python3.11/site-packages/xgboost/core.py:2298, in Booster.predict(self, data, output_margin, pred_leaf, pred_contribs, approx_contribs, pred_interactions, validate_features, training, iteration_range, strict_shape)\n   2295 shape = ctypes.POINTER(c_bst_ulong)()\n   2296 dims = c_bst_ulong()\n   2297 _check_call(\n-&gt; 2298     _LIB.XGBoosterPredictFromDMatrix(\n   2299         self.handle,\n   2300         data.handle,\n   2301         from_pystr_to_cstr(json.dumps(args)),\n   2302         ctypes.byref(shape),\n   2303         ctypes.byref(dims),\n   2304         ctypes.byref(preds),\n   2305     )\n   2306 )\n   2307 return _prediction_output(shape, dims, preds, False)\n\nKeyboardInterrupt: </pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/basic-usage/#agnboost-basic-usage-tutorial","title":"AGNBoost Basic Usage Tutorial\u00b6","text":"<p>This notebook demonstrates the basic workflow for using AGNBoost to predict AGN fractions from photometric data. We'll walk through:</p> <ol> <li>Loading astronomical data with the Catalog class</li> <li>Exploring the dataset structure and properties</li> <li>Splitting data into training, validation, and test sets</li> <li>Cleaning the data by removing rows with missing values</li> <li>Loading a pre-trained AGN fraction model</li> <li>Making predictions with uncertainty quantification</li> <li>Evaluating model performance</li> </ol> <p>Let's start by importing the necessary libraries and loading our data.</p>"},{"location":"tutorials/basic-usage/#loading-the-data","title":"Loading the Data\u00b6","text":"<p>We'll use the Catalog class to load our astronomical dataset. The <code>cigale_mock_small.csv</code> file contains is a small set of mock NIRCam+MIRI CIGALE galaxies for demonstration purposes.</p>"},{"location":"tutorials/basic-usage/#exploring-the-dataset","title":"Exploring the Dataset\u00b6","text":"<p>Let's examine the structure of our data to understand what photometric bands are available and get basic statistics about our dataset. The <code>print_data_summary()</code> method provides comprehensive information about:</p> <ul> <li>Dataset dimensions and memory usage</li> <li>Photometric band validation and metadata</li> <li>Column-by-column statistics including missing values</li> <li>Summary statistics for numerical columns</li> </ul> <p>This information helps us understand data quality and identify any potential issues before modeling.</p>"},{"location":"tutorials/basic-usage/#creating-traintestvalidation-splits","title":"Creating Train/Test/Validation Splits\u00b6","text":"<p>Before any modeling, we need to split our data into separate sets for training, validation, and testing. AGNBoost provides intelligent data splitting with optional stratification to ensure representative samples across all splits.</p> <p>We'll use the default split ratios:</p> <ul> <li>60% for training</li> <li>20% for validation</li> <li>20% for testing</li> </ul> <p>The random state ensures reproducible results. This step is not strictly necessary, as AGNBoost will internally perform the split if it has not explicitly been done.</p>"},{"location":"tutorials/basic-usage/#cleaning-the-data","title":"Cleaning the Data\u00b6","text":"<p>Real astronomical datasets often contain missing values due to various observational limitations. Before training or making predictions, we will remove rows that have NaN values in critical columns.</p> <p>The <code>drop_nan()</code> method removes rows with missing values in the validated photometric band columns, ensuring our model receives complete data for all features.</p>"},{"location":"tutorials/basic-usage/#creating-features","title":"Creating Features\u00b6","text":"<p>AGNBoost automatically engineers features from photometric data, including colors and transformations. Let's create the feature dataframe that will be used for modeling.</p> <p>By default, AGNBoost will create a features consisting of the photometric bands + derived colors + the squares of those derived colors</p>"},{"location":"tutorials/basic-usage/#loading-the-pre-trained-model","title":"Loading the Pre-trained Model\u00b6","text":"<p>AGNBoost comes with pre-trained models for common astronomical tasks. We'll load the model specifically trained for AGN fraction estimation (<code>agn.fracAGN</code>).</p> <p>The <code>load_models()</code> method automatically:</p> <ul> <li>Checks for compatible pre-trained models</li> <li>Validates feature compatibility between the model and our data</li> <li>Loads model metadata including training parameters and performance metrics</li> </ul>"},{"location":"tutorials/basic-usage/#making-predictions","title":"Making Predictions\u00b6","text":"<p>Now we'll use our loaded model to predict AGN fractions for the test set. AGNBoost seamlessly handles the conversion of our catalog data into the format required by the underlying XGBoost model.</p> <p>The prediction process uses the engineered features (colors, log magnitudes, etc.) that were automatically created from our photometric band data.</p>"},{"location":"tutorials/basic-usage/#quantifying-prediction-uncertainty","title":"Quantifying Prediction Uncertainty\u00b6","text":"<p>One of AGNBoost's key advantages is its ability to provide robust uncertainty estimates through XGBoostLSS distributional modeling. Rather than just point estimates, we get full uncertainty quantification for each prediction.</p> <p>The <code>prediction_uncertainty()</code> method returns uncertainty estimates that account for both model uncertainty and the inherent variability in the data. This is crucial for astronomical applications where understanding prediction confidence is essential for scientific interpretation.</p> <p>Since the loaded data is a CIGALE mock catalog with no photometric uncertainty, we will only estimate the model (aleatoric + epistemic) uncertainty for each source.</p>"},{"location":"tutorials/basic-usage_old/","title":"AGNBoost Basic Usage Tutorial","text":"In\u00a0[2]: Copied! <pre># Set agnbioost folder as root\nimport os\nos.chdir(os.path.expanduser(\"/home/kurt/Documents/agnboost/\"))\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom agnboost import dataset, model\n#from sklearn.metrics import mean_squared_error\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\nprint(\"AGNBoost Basic Usage Tutorial\")\nprint(\"=\" * 40)\n</pre> # Set agnbioost folder as root import os os.chdir(os.path.expanduser(\"/home/kurt/Documents/agnboost/\"))  # Import necessary libraries import numpy as np import pandas as pd from agnboost import dataset, model #from sklearn.metrics import mean_squared_error  # Set random seed for reproducibility np.random.seed(123)  print(\"AGNBoost Basic Usage Tutorial\") print(\"=\" * 40) <pre>2025-05-25 16:36:50.131 | INFO     | agnboost.config:&lt;module&gt;:11 - PROJ_ROOT path is: /home/kurt/Documents/agnboost\n</pre> <pre>AGNBoost Basic Usage Tutorial\n========================================\n</pre> In\u00a0[25]: Copied! <pre># Load the astronomical data using the Catalog class\ncatalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)\n</pre> # Load the astronomical data using the Catalog class catalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)  <pre>Current working directory: /home/kurt/Documents/agnboost\nLooking for bands file at: /home/kurt/Documents/agnboost/allowed_bands.json\n[INFO] Loaded bands file metadata: This file contains the allowed photometric bands for JWST\n[INFO] Loaded 11 allowed bands from agnboost/allowed_bands.json\n[INFO] Attempting to load file with delimiter: ','\n[INFO] Successfully loaded data with 1000 rows.\n[INFO] Found 11 valid band columns:\n[INFO]   - jwst.nircam.F115W (F115W): 1.154 \u03bcm\n[INFO]   - jwst.nircam.F150W (F150W): 1.501 \u03bcm\n[INFO]   - jwst.nircam.F200W (F200W): 1.988 \u03bcm\n[INFO]   - jwst.nircam.F277W (F277W): 2.776 \u03bcm\n[INFO]   - jwst.nircam.F356W (F356W): 3.565 \u03bcm\n[INFO]   - jwst.nircam.F410M (F410M): 4.083 \u03bcm\n[INFO]   - jwst.nircam.F444W (F444W): 4.402 \u03bcm\n[INFO]   - jwst.miri.F770W (F770W): 7.7 \u03bcm\n[INFO]   - jwst.miri.F1000W (F1000W): 10.0 \u03bcm\n[INFO]   - jwst.miri.F1500W (F1500W): 15.0 \u03bcm\n[INFO]   - jwst.miri.F2100W (F2100W): 21.0 \u03bcm\n</pre> In\u00a0[26]: Copied! <pre># Display comprehensive data summary\ncatalog.print_data_summary()\n\n# Check which photometric bands were validated\nvalid_bands = catalog.get_valid_bands()\nprint(f\"\\nValid photometric bands found: {len(valid_bands)}\")\nfor band_name, info in valid_bands.items():\n    print(f\"  {band_name}: {info['shorthand']} ({info['wavelength']} \u03bcm)\")\n\n# Check if our target variable exists\ntarget_column = 'agn.fracAGN'\nif target_column in catalog.get_data().columns:\n    print(f\"\\nTarget variable '{target_column}' found in dataset\")\n    target_stats = catalog.get_data()[target_column].describe()\n    print(\"Target variable statistics:\")\n    print(target_stats)\nelse:\n    print(f\"Warning: Target variable '{target_column}' not found in dataset\")\n    print(\"Available columns:\", list(catalog.get_data().columns))\n</pre> # Display comprehensive data summary catalog.print_data_summary()  # Check which photometric bands were validated valid_bands = catalog.get_valid_bands() print(f\"\\nValid photometric bands found: {len(valid_bands)}\") for band_name, info in valid_bands.items():     print(f\"  {band_name}: {info['shorthand']} ({info['wavelength']} \u03bcm)\")  # Check if our target variable exists target_column = 'agn.fracAGN' if target_column in catalog.get_data().columns:     print(f\"\\nTarget variable '{target_column}' found in dataset\")     target_stats = catalog.get_data()[target_column].describe()     print(\"Target variable statistics:\")     print(target_stats) else:     print(f\"Warning: Target variable '{target_column}' not found in dataset\")     print(\"Available columns:\", list(catalog.get_data().columns)) <pre>\n================================================================================\nDATA SUMMARY: cigale_mock_small.csv\n================================================================================\nDimensions: 1000 rows \u00d7 26 columns\nMemory usage: 0.20 MB\n--------------------------------------------------------------------------------\nValid Band Columns:\n--------------------------------------------------------------------------------\nColumn Name                    Shorthand       Wavelength (\u03bcm)\n--------------------------------------------------------------------------------\njwst.nircam.F115W              F115W           1.154          \njwst.nircam.F150W              F150W           1.501          \njwst.nircam.F200W              F200W           1.988          \njwst.nircam.F277W              F277W           2.776          \njwst.nircam.F356W              F356W           3.565          \njwst.nircam.F410M              F410M           4.083          \njwst.nircam.F444W              F444W           4.402          \njwst.miri.F770W                F770W           7.700          \njwst.miri.F1000W               F1000W          10.000         \njwst.miri.F1500W               F1500W          15.000         \njwst.miri.F2100W               F2100W          21.000         \n--------------------------------------------------------------------------------\nColumn Information:\n--------------------------------------------------------------------------------\nColumn Name                    Type            Non-Null        Null %    \n--------------------------------------------------------------------------------\nIRAC1                          float64         1000/1000            0.00%     \nIRAC2                          float64         1000/1000            0.00%     \nIRAC3                          float64         1000/1000            0.00%     \nIRAC4                          float64         1000/1000            0.00%     \nhst.acs.wfc.F606W              float64         1000/1000            0.00%     \nhst.acs.wfc.F814W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F125W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F140W              float64         1000/1000            0.00%     \nhst.wfc3.ir.F160W              float64         1000/1000            0.00%     \njwst.miri.F1000W               float64         1000/1000            0.00%     \njwst.miri.F1280W               float64         1000/1000            0.00%     \njwst.miri.F1500W               float64         1000/1000            0.00%     \njwst.miri.F1800W               float64         1000/1000            0.00%     \njwst.miri.F2100W               float64         1000/1000            0.00%     \njwst.miri.F770W                float64         1000/1000            0.00%     \njwst.nircam.F115W              float64         1000/1000            0.00%     \njwst.nircam.F150W              float64         1000/1000            0.00%     \njwst.nircam.F200W              float64         1000/1000            0.00%     \njwst.nircam.F277W              float64         1000/1000            0.00%     \njwst.nircam.F356W              float64         1000/1000            0.00%     \njwst.nircam.F410M              float64         1000/1000            0.00%     \njwst.nircam.F444W              float64         1000/1000            0.00%     \nsfh.sfr100Myrs                 float64         1000/1000            0.00%     \nstellar.m_star                 float64         1000/1000            0.00%     \nagn.fracAGN                    float64         1000/1000            0.00%     \nuniverse.redshift              float64         1000/1000            0.00%     \n--------------------------------------------------------------------------------\n\nNumeric Column Statistics:\n--------------------------------------------------------------------------------\nColumn                         Mean         Std          Min          Max         \n--------------------------------------------------------------------------------\nIRAC1                57.9         1308         2.413e-06    4.098e+04   \nIRAC2                22.97        509.9        8.821e-07    1.596e+04   \nIRAC3                39.96        918          1.646e-06    2.879e+04   \nIRAC4                57.92        1309         2.413e-06    4.099e+04   \nhst.acs.wfc.F606W    0.311        5.52         0            169         \nhst.acs.wfc.F814W    0.3093       5.099        5.455e-13    155.8       \nhst.wfc3.ir.F125W    0.5148       6.576        1.614e-09    192.2       \nhst.wfc3.ir.F140W    0.6132       7.125        2.611e-09    196.7       \nhst.wfc3.ir.F160W    0.7412       7.991        4.119e-09    200.2       \njwst.miri.F1000W     57.54        1356         3.049e-06    4.257e+04   \njwst.miri.F1280W     71.4         1587         4.006e-06    4.97e+04    \njwst.miri.F1500W     74.16        1638         4.475e-06    5.129e+04   \njwst.miri.F1800W     82.2         1710         4.232e-06    5.339e+04   \njwst.miri.F2100W     87.79        1773         4.001e-06    5.527e+04   \njwst.miri.F770W      58.58        1315         2.288e-06    4.117e+04   \njwst.nircam.F115W    0.461        6.317        1.192e-09    188.6       \njwst.nircam.F150W    0.706        7.721        3.693e-09    198.7       \njwst.nircam.F200W    1.482        15.31        1.959e-08    280.6       \njwst.nircam.F277W    4.441        68.46        1.332e-07    2009        \njwst.nircam.F356W    11.62        225.3        4.48e-07     6973        \njwst.nircam.F410M    17.51        373          6.242e-07    1.164e+04   \njwst.nircam.F444W    21.65        477          8.29e-07     1.492e+04   \nsfh.sfr100Myrs       4.765        4.403        4.765e-27    15.79       \nstellar.m_star       3.51e+09     2.551e+09    3.367e+07    7.388e+09   \nagn.fracAGN          0.4993       0.3164       0            0.99        \nuniverse.redshift    1.765        1.811        0.01         7.999       \n================================================================================\n\n\nValid photometric bands found: 11\n  jwst.nircam.F115W: F115W (1.154 \u03bcm)\n  jwst.nircam.F150W: F150W (1.501 \u03bcm)\n  jwst.nircam.F200W: F200W (1.988 \u03bcm)\n  jwst.nircam.F277W: F277W (2.776 \u03bcm)\n  jwst.nircam.F356W: F356W (3.565 \u03bcm)\n  jwst.nircam.F410M: F410M (4.083 \u03bcm)\n  jwst.nircam.F444W: F444W (4.402 \u03bcm)\n  jwst.miri.F770W: F770W (7.7 \u03bcm)\n  jwst.miri.F1000W: F1000W (10.0 \u03bcm)\n  jwst.miri.F1500W: F1500W (15.0 \u03bcm)\n  jwst.miri.F2100W: F2100W (21.0 \u03bcm)\n\nTarget variable 'agn.fracAGN' found in dataset\nTarget variable statistics:\ncount    1000.000000\nmean        0.499330\nstd         0.316352\nmin         0.000000\n25%         0.200000\n50%         0.500000\n75%         0.800000\nmax         0.990000\nName: agn.fracAGN, dtype: float64\n</pre> In\u00a0[27]: Copied! <pre># Create train/validation/test splitsget_train_val_test_sizes\ncatalog.split_data(test_size=0.2, val_size=0.2, random_state=42)\n\n# Get split information\nsplit_info = catalog.get_train_val_test_sizes()\nprint(\"Data split summary:\")\nprint(f\"  Total samples: {split_info['total']}\")\nprint(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\")\nprint(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\")\nprint(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\")\n</pre> # Create train/validation/test splitsget_train_val_test_sizes catalog.split_data(test_size=0.2, val_size=0.2, random_state=42)  # Get split information split_info = catalog.get_train_val_test_sizes() print(\"Data split summary:\") print(f\"  Total samples: {split_info['total']}\") print(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\") print(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\") print(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\") <pre>Data split summary:\n  Total samples: 1000\n  Training: 600 (60.0%)\n  Validation: 200 (20.0%)\n  Test: 200 (20.0%)\n</pre> In\u00a0[28]: Copied! <pre># Drop rows with NaN values in the validated columns\ncatalog.drop_nan(inplace=True)\n</pre> # Drop rows with NaN values in the validated columns catalog.drop_nan(inplace=True)  <pre>[INFO] No rows with NaN values found in the specified columns.\n</pre> In\u00a0[29]: Copied! <pre># Create features for modeling\ncatalog.create_feature_dataframe()\n\n# Get information about created features\nfeatures = catalog.get_features()\nprint(f\"Feature engineering complete:\")\nprint(f\"  Feature dataframe shape: {features.shape}\")\n</pre> # Create features for modeling catalog.create_feature_dataframe()  # Get information about created features features = catalog.get_features() print(f\"Feature engineering complete:\") print(f\"  Feature dataframe shape: {features.shape}\")  <pre>[INFO] Created feature dataframe with 121 columns and 1000 rows.\nFeature engineering complete:\n  Feature dataframe shape: (1000, 121)\n</pre> In\u00a0[30]: Copied! <pre># Initialize AGNBoost with the target model\nagnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),\n                          target_variables = {'agn.fracAGN' : 'ZABeta'},\n                         )\n\n# Load pre-trained models\nfilename = '2025_05_22-PM06_59_58_agn.fracAGN_model.pkl.gz'\nagnboost_m.load_model(file_name = filename, overwrite = True)\nprint(agnboost_m.models)\n\nif agnboost_m.models['agn.fracAGN'] is not None:\n    print(\"\u2705 Pre-trained model loaded successfully!\")\n    \n    # Display model information\n    model_info = agnboost_m.model_info.get('agn.fracAGN', {})\n    if model_info:\n        print(\"\\nModel information:\")\n        if 'training_timestamp' in model_info:\n            print(f\"  Trained: {model_info['training_timestamp']}\")\n        if 'best_score' in model_info:\n            print(f\"  Best validation score: {model_info['best_score']:.6f}\")\n        if 'features' in model_info:\n            print(f\"  Number of features: {len(model_info['features'])}\")\nelse:\n    print(\"\u274c No pre-trained models found!\")\n    print(\"You may need to train a new model or check the models directory.\")\n</pre> # Initialize AGNBoost with the target model agnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),                           target_variables = {'agn.fracAGN' : 'ZABeta'},                          )  # Load pre-trained models filename = '2025_05_22-PM06_59_58_agn.fracAGN_model.pkl.gz' agnboost_m.load_model(file_name = filename, overwrite = True) print(agnboost_m.models)  if agnboost_m.models['agn.fracAGN'] is not None:     print(\"\u2705 Pre-trained model loaded successfully!\")          # Display model information     model_info = agnboost_m.model_info.get('agn.fracAGN', {})     if model_info:         print(\"\\nModel information:\")         if 'training_timestamp' in model_info:             print(f\"  Trained: {model_info['training_timestamp']}\")         if 'best_score' in model_info:             print(f\"  Best validation score: {model_info['best_score']:.6f}\")         if 'features' in model_info:             print(f\"  Number of features: {len(model_info['features'])}\") else:     print(\"\u274c No pre-trained models found!\")     print(\"You may need to train a new model or check the models directory.\") <pre>{'agn.fracAGN': &lt;xgboostlss.model.XGBoostLSS object at 0x7905e74b1090&gt;}\n\u2705 Pre-trained model loaded successfully!\n\nModel information:\n  Best validation score: -649218.125000\n  Number of features: 121\n</pre> In\u00a0[31]: Copied! <pre># Make predictions on the test set\n#agnboost_m.models['agn.fracAGN'].booster.set_param( {'device': 'cpu'})\npreds = agnboost_m.predict( data = catalog, split_use = 'test', model_name = 'agn.fracAGN')\n\nprint(f\"  Mean: {np.mean(preds):.6f}\")\nprint(f\"  Std: {np.std(preds):.6f}\")\nprint(f\"  Min: {np.min(preds):.6f}\")\nprint(f\"  Max: {np.max(preds):.6f}\")\n</pre> # Make predictions on the test set #agnboost_m.models['agn.fracAGN'].booster.set_param( {'device': 'cpu'}) preds = agnboost_m.predict( data = catalog, split_use = 'test', model_name = 'agn.fracAGN')  print(f\"  Mean: {np.mean(preds):.6f}\") print(f\"  Std: {np.std(preds):.6f}\") print(f\"  Min: {np.min(preds):.6f}\") print(f\"  Max: {np.max(preds):.6f}\")  <pre>2025-05-25 17:01:36,708 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the test set stored in the passed Catalog.\n</pre> <pre>  Mean: 0.504962\n  Std: 0.325251\n  Min: 0.000308\n  Max: 0.989859\n</pre> In\u00a0[32]: Copied! <pre>model_uncertainty = agnboost_m.prediction_uncertainty( uncertainty_type = 'model', model_name = 'agn.fracAGN', catalog = catalog)\n\nprint(f\"\u2705 Uncertainty estimates generated\")\nprint(f\"Uncertainty statistics:\")\nprint(f\"  Mean uncertainty: {np.mean(model_uncertainty):.6f}\")\nprint(f\"  Std uncertainty: {np.std(model_uncertainty):.6f}\")\nprint(f\"  Min uncertainty: {np.min(model_uncertainty):.6f}\")\nprint(f\"  Max uncertainty: {np.max(model_uncertainty):.6f}\")\n</pre> model_uncertainty = agnboost_m.prediction_uncertainty( uncertainty_type = 'model', model_name = 'agn.fracAGN', catalog = catalog)  print(f\"\u2705 Uncertainty estimates generated\") print(f\"Uncertainty statistics:\") print(f\"  Mean uncertainty: {np.mean(model_uncertainty):.6f}\") print(f\"  Std uncertainty: {np.std(model_uncertainty):.6f}\") print(f\"  Min uncertainty: {np.min(model_uncertainty):.6f}\") print(f\"  Max uncertainty: {np.max(model_uncertainty):.6f}\") <pre>2025-05-25 17:01:39,333 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the None set stored in the passed Catalog.\nProcessing truncated model uncertainty: 100%|\u2588| 1000/1000 [07:09&lt;00:00,  2.33it/</pre> <pre>\u2705 Uncertainty estimates generated\nUncertainty statistics:\n  Mean uncertainty: 0.033900\n  Std uncertainty: 0.013166\n  Min uncertainty: 0.000940\n  Max uncertainty: 0.071419\n</pre> <pre>\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/basic-usage_old/#agnboost-basic-usage-tutorial","title":"AGNBoost Basic Usage Tutorial\u00b6","text":"<p>This notebook demonstrates the basic workflow for using AGNBoost to predict AGN fractions from photometric data. We'll walk through:</p> <ol> <li>Loading astronomical data with the Catalog class</li> <li>Exploring the dataset structure and properties</li> <li>Splitting data into training, validation, and test sets</li> <li>Cleaning the data by removing rows with missing values</li> <li>Loading a pre-trained AGN fraction model</li> <li>Making predictions with uncertainty quantification</li> <li>Evaluating model performance</li> </ol> <p>Let's start by importing the necessary libraries and loading our data.</p>"},{"location":"tutorials/basic-usage_old/#loading-the-data","title":"Loading the Data\u00b6","text":"<p>We'll use the Catalog class to load our astronomical dataset. The <code>models-block-0.fits</code> file contains photometric measurements and AGN fraction labels for our analysis.</p>"},{"location":"tutorials/basic-usage_old/#exploring-the-dataset","title":"Exploring the Dataset\u00b6","text":"<p>Let's examine the structure of our data to understand what photometric bands are available and get basic statistics about our dataset. The <code>print_data_summary()</code> method provides comprehensive information about:</p> <ul> <li>Dataset dimensions and memory usage</li> <li>Photometric band validation and metadata</li> <li>Column-by-column statistics including missing values</li> <li>Summary statistics for numerical columns</li> </ul> <p>This information helps us understand data quality and identify any potential issues before modeling.</p>"},{"location":"tutorials/basic-usage_old/#creating-traintestvalidation-splits","title":"Creating Train/Test/Validation Splits\u00b6","text":"<p>Before any modeling, we need to split our data into separate sets for training, validation, and testing. AGNBoost provides intelligent data splitting with optional stratification to ensure representative samples across all splits.</p> <p>We'll use the default split ratios:</p> <ul> <li>60% for training</li> <li>20% for validation</li> <li>20% for testing</li> </ul> <p>The random state ensures reproducible results.</p>"},{"location":"tutorials/basic-usage_old/#cleaning-the-data","title":"Cleaning the Data\u00b6","text":"<p>Real astronomical datasets often contain missing values due to various observational limitations. Before training or making predictions, we will remove rows that have NaN values in critical columns.</p> <p>The <code>drop_nan()</code> method removes rows with missing values in the validated photometric band columns, ensuring our model receives complete data for all features.</p>"},{"location":"tutorials/basic-usage_old/#creating-features","title":"Creating Features\u00b6","text":"<p>AGNBoost automatically engineers features from photometric data, including colors and transformations. Let's create the feature dataframe that will be used for modeling.</p> <p>By default, AGNBoost will create a features consisting of the photometric bands + derived colors + the squares of those derived colors</p>"},{"location":"tutorials/basic-usage_old/#loading-the-pre-trained-model","title":"Loading the Pre-trained Model\u00b6","text":"<p>AGNBoost comes with pre-trained models for common astronomical tasks. We'll load the model specifically trained for AGN fraction estimation (<code>agn.fracAGN</code>).</p> <p>The <code>load_models()</code> method automatically:</p> <ul> <li>Checks for compatible pre-trained models</li> <li>Validates feature compatibility between the model and our data</li> <li>Loads model metadata including training parameters and performance metrics</li> </ul>"},{"location":"tutorials/basic-usage_old/#making-predictions","title":"Making Predictions\u00b6","text":"<p>Now we'll use our loaded model to predict AGN fractions for the test set. AGNBoost seamlessly handles the conversion of our catalog data into the format required by the underlying XGBoost model.</p> <p>The prediction process uses the engineered features (colors, log magnitudes, etc.) that were automatically created from our photometric band data.</p>"},{"location":"tutorials/basic-usage_old/#quantifying-prediction-uncertainty","title":"Quantifying Prediction Uncertainty\u00b6","text":"<p>One of AGNBoost's key advantages is its ability to provide robust uncertainty estimates through XGBoostLSS distributional modeling. Rather than just point estimates, we get full uncertainty quantification for each prediction.</p> <p>The <code>prediction_uncertainty()</code> method returns uncertainty estimates that account for both model uncertainty and the inherent variability in the data. This is crucial for astronomical applications where understanding prediction confidence is essential for scientific interpretation.</p> <p>Since the loaded data is a CIGALE mock catalog with no photometric uncertainty, we will only estimate the model (aleatoric + epistemic) uncertainty for each source.</p>"},{"location":"tutorials/custom_bands/","title":"Using AGNBoost with Custom (non-default) Photometric Bands","text":"In\u00a0[1]: Copied! <pre># Set agnboost folder as root\nimport os\n\n# Navigate to the repository root (parent directory of notebooks/)\nos.chdir('..')\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom agnboost import dataset, model\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\nprint(\"AGNBoost Basic Usage Tutorial\")\nprint(\"=\" * 40)\n</pre> # Set agnboost folder as root import os  # Navigate to the repository root (parent directory of notebooks/) os.chdir('..')  # Import necessary libraries import numpy as np import pandas as pd from agnboost import dataset, model  # Set random seed for reproducibility np.random.seed(123)  print(\"AGNBoost Basic Usage Tutorial\") print(\"=\" * 40) <pre>2025-05-31 20:52:53.834 | INFO     | agnboost.config:&lt;module&gt;:11 - PROJ_ROOT path is: /home/kurt/Documents/agnboost\n</pre> <pre>AGNBoost Basic Usage Tutorial\n========================================\n</pre> <p>To create a <code>Catalog</code> object with custom bands, you need only create a python dictionary containing the information of the bands. Note that this dictionary has very specific formatting requirements! For example, here is the default dictionary used by AGNBoost, including 7 NIRCam bands and 4 MIRI bands:</p> <pre><code>band_dict =   {   \"jwst.nircam.F115W\": {\"shorthand\": \"F115W\", \"wavelength\": 1.154},\n                  \"jwst.nircam.F150W\": {\"shorthand\": \"F150W\", \"wavelength\": 1.501},\n                  \"jwst.nircam.F200W\": {\"shorthand\": \"F200W\", \"wavelength\": 1.988},\n                  \"jwst.nircam.F277W\": {\"shorthand\": \"F277W\", \"wavelength\": 2.776},\n                  \"jwst.nircam.F356W\": {\"shorthand\": \"F356W\", \"wavelength\": 3.565},\n                  \"jwst.nircam.F410M\": {\"shorthand\": \"F410M\", \"wavelength\": 4.083},\n                  \"jwst.nircam.F444W\": {\"shorthand\": \"F444W\", \"wavelength\": 4.402},\n                  \"jwst.miri.F770W\": {\"shorthand\": \"F770W\", \"wavelength\": 7.7},\n                  \"jwst.miri.F1000W\": {\"shorthand\": \"F1000W\", \"wavelength\": 10.0},\n                  \"jwst.miri.F1500W\": {\"shorthand\": \"F1500W\", \"wavelength\": 15.0},\n                  \"jwst.miri.F2100W\": {\"shorthand\": \"F2100W\", \"wavelength\": 21.0}\n            }\n</code></pre> <p>Each key in the dictionary is the name of the photometric band in your dataset. So, if you are using mock CIGALE data to train and test models, the formatting should follow the CIGALE band naming conventions (i.e., \"jwst.nircam.F115W\", \"jwst.miri.F2100W\", etc.). The values corresponding to these keys are also python dictionaries, each with a \"shorthand\" key and a \"wavelength\" key. The \"shorthand\" key is the desired shortname of the photometric band (for example, with CIGALE, it is cleaner to be able to name a band \"F115W\" instead of the full CIGALE column name \"jwst.nircam.F115W\"). The \"wavelength\" is the pivot wavelength of the photomtric band (in microns).</p> <p>So, to create a AGNBoost model that uses ONLY the NIRCam bands, we can create the following dictionary:</p> In\u00a0[3]: Copied! <pre>NIRCam_band_dict =   {    \"jwst.nircam.F115W\": {\"shorthand\": \"F115W\", \"wavelength\": 1.154},\n                          \"jwst.nircam.F150W\": {\"shorthand\": \"F150W\", \"wavelength\": 1.501},\n                          \"jwst.nircam.F200W\": {\"shorthand\": \"F200W\", \"wavelength\": 1.988},\n                          \"jwst.nircam.F277W\": {\"shorthand\": \"F277W\", \"wavelength\": 2.776},\n                          \"jwst.nircam.F356W\": {\"shorthand\": \"F356W\", \"wavelength\": 3.565},\n                          \"jwst.nircam.F410M\": {\"shorthand\": \"F410M\", \"wavelength\": 4.083},\n                          \"jwst.nircam.F444W\": {\"shorthand\": \"F444W\", \"wavelength\": 4.402},\n                }\n</pre> NIRCam_band_dict =   {    \"jwst.nircam.F115W\": {\"shorthand\": \"F115W\", \"wavelength\": 1.154},                           \"jwst.nircam.F150W\": {\"shorthand\": \"F150W\", \"wavelength\": 1.501},                           \"jwst.nircam.F200W\": {\"shorthand\": \"F200W\", \"wavelength\": 1.988},                           \"jwst.nircam.F277W\": {\"shorthand\": \"F277W\", \"wavelength\": 2.776},                           \"jwst.nircam.F356W\": {\"shorthand\": \"F356W\", \"wavelength\": 3.565},                           \"jwst.nircam.F410M\": {\"shorthand\": \"F410M\", \"wavelength\": 4.083},                           \"jwst.nircam.F444W\": {\"shorthand\": \"F444W\", \"wavelength\": 4.402},                 } <p>Then, we can pass this to an instantiation of the <code>Catalog</code> class as the <code>band_dict</code> parameter. We will use the small provided set of CIGALE data to demonstrate this:</p> In\u00a0[4]: Copied! <pre># Load the astronomical data using the Catalog class\ncatalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\", \n                          band_dict = NIRCam_band_dict, \n                          summarize = False)\n</pre> # Load the astronomical data using the Catalog class catalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",                            band_dict = NIRCam_band_dict,                            summarize = False) <pre>[INFO] Attempting to load file with delimiter: ','\n[INFO] Successfully loaded data with 1000 rows.\n[INFO] Found 7 valid band columns:\n[INFO]   - jwst.nircam.F115W (F115W): 1.154 \u03bcm\n[INFO]   - jwst.nircam.F150W (F150W): 1.501 \u03bcm\n[INFO]   - jwst.nircam.F200W (F200W): 1.988 \u03bcm\n[INFO]   - jwst.nircam.F277W (F277W): 2.776 \u03bcm\n[INFO]   - jwst.nircam.F356W (F356W): 3.565 \u03bcm\n[INFO]   - jwst.nircam.F410M (F410M): 4.083 \u03bcm\n[INFO]   - jwst.nircam.F444W (F444W): 4.402 \u03bcm\n</pre> <p>We can easily see that our catalog object now only uses the 7 NIRCam bands. If we were to then create the default feature dataframe (phots + colors + colors^2), we should only see quantities involving these valid bands. Let's check:</p> In\u00a0[5]: Copied! <pre># Create features for modeling\nfeature_df = catalog.create_feature_dataframe()\n</pre> # Create features for modeling feature_df = catalog.create_feature_dataframe()  <pre>[INFO] Created feature dataframe with 49 columns and 1000 rows.\n[INFO] Created features are: ['jwst.nircam.F115W', 'jwst.nircam.F150W', 'jwst.nircam.F200W', 'jwst.nircam.F277W', 'jwst.nircam.F356W', 'jwst.nircam.F410M', 'jwst.nircam.F444W', 'F444W/F410M', 'F444W/F356W', 'F444W/F277W', 'F444W/F200W', 'F444W/F150W', 'F444W/F115W', 'F410M/F356W', 'F410M/F277W', 'F410M/F200W', 'F410M/F150W', 'F410M/F115W', 'F356W/F277W', 'F356W/F200W', 'F356W/F150W', 'F356W/F115W', 'F277W/F200W', 'F277W/F150W', 'F277W/F115W', 'F200W/F150W', 'F200W/F115W', 'F150W/F115W', 'F444W/F410M^2', 'F444W/F356W^2', 'F444W/F277W^2', 'F444W/F200W^2', 'F444W/F150W^2', 'F444W/F115W^2', 'F410M/F356W^2', 'F410M/F277W^2', 'F410M/F200W^2', 'F410M/F150W^2', 'F410M/F115W^2', 'F356W/F277W^2', 'F356W/F200W^2', 'F356W/F150W^2', 'F356W/F115W^2', 'F277W/F200W^2', 'F277W/F150W^2', 'F277W/F115W^2', 'F200W/F150W^2', 'F200W/F115W^2', 'F150W/F115W^2']\n</pre> <p>Good to go! Just remember that the loaded data in the catalog object needs to have the bands you specify!</p> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/custom_bands/#using-agnboost-with-custom-non-default-photometric-bands","title":"Using AGNBoost with Custom (non-default) Photometric Bands\u00b6","text":"<p>This notebook demonstrates how to use AGNBoost with photometric bands that are not included in the default models of AGNBoost. Thankfully, this is very straightforward to do with AGNBoost, and is entiely relegated to the <code>Catalog</code> class. Frist, we import the necessary modules.</p>"},{"location":"tutorials/tuning%2Btraining/","title":"Training and Tuning AGNBoost Models from Scratch","text":"In\u00a0[2]: Copied! <pre># Set agnboost folder as root\nimport os\n\n# Navigate to the repository root (parent directory of notebooks/)\nos.chdir('..')\n\n# Import necessary libraries\nimport numpy as np\nimport pandas as pd\nfrom agnboost import dataset, model\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set random seed for reproducibility\nnp.random.seed(123)\n\nprint(\"AGNBoost Basic Usage Tutorial\")\nprint(\"=\" * 40)\n</pre> # Set agnboost folder as root import os  # Navigate to the repository root (parent directory of notebooks/) os.chdir('..')  # Import necessary libraries import numpy as np import pandas as pd from agnboost import dataset, model import seaborn as sns import matplotlib.pyplot as plt  # Set random seed for reproducibility np.random.seed(123)  print(\"AGNBoost Basic Usage Tutorial\") print(\"=\" * 40) <pre>2025-05-31 21:17:14.384 | INFO     | agnboost.config:&lt;module&gt;:11 - PROJ_ROOT path is: /home/kurt/Documents/agnboost\n</pre> <pre>AGNBoost Basic Usage Tutorial\n========================================\n</pre> In\u00a0[23]: Copied! <pre># Load the astronomical data using the Catalog class with all NIRCam+MIRI bands\ncatalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)\n\n# Create train/validation/test splitsget_train_val_test_sizes\ncatalog.split_data(test_size=0.2, val_size=0.2, random_state=42)\n\n# Get split information\nsplit_info = catalog.get_train_val_test_sizes()\nprint(\"Data split summary:\")\nprint(f\"  Total samples: {split_info['total']}\")\nprint(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\")\nprint(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\")\nprint(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\")\n</pre> # Load the astronomical data using the Catalog class with all NIRCam+MIRI bands catalog = dataset.Catalog(path=\"data/cigale_mock_small.csv\",summarize = False)  # Create train/validation/test splitsget_train_val_test_sizes catalog.split_data(test_size=0.2, val_size=0.2, random_state=42)  # Get split information split_info = catalog.get_train_val_test_sizes() print(\"Data split summary:\") print(f\"  Total samples: {split_info['total']}\") print(f\"  Training: {split_info['train']['size']} ({split_info['train']['percentage']:.1f}%)\") print(f\"  Validation: {split_info['validation']['size']} ({split_info['validation']['percentage']:.1f}%)\") print(f\"  Test: {split_info['test']['size']} ({split_info['test']['percentage']:.1f}%)\") <pre>Current working directory: /home/kurt/Documents/agnboost\nLooking for bands file at: /home/kurt/Documents/agnboost/agnboost/allowed_bands.json\n[INFO] Loaded bands file metadata: This file contains the allowed photometric bands for JWST\n[INFO] Loaded 11 allowed bands from agnboost/allowed_bands.json\n[INFO] Attempting to load file with delimiter: ','\n[INFO] Successfully loaded data with 1000 rows.\n[INFO] Found 11 valid band columns:\n[INFO]   - jwst.nircam.F115W (F115W): 1.154 \u03bcm\n[INFO]   - jwst.nircam.F150W (F150W): 1.501 \u03bcm\n[INFO]   - jwst.nircam.F200W (F200W): 1.988 \u03bcm\n[INFO]   - jwst.nircam.F277W (F277W): 2.776 \u03bcm\n[INFO]   - jwst.nircam.F356W (F356W): 3.565 \u03bcm\n[INFO]   - jwst.nircam.F410M (F410M): 4.083 \u03bcm\n[INFO]   - jwst.nircam.F444W (F444W): 4.402 \u03bcm\n[INFO]   - jwst.miri.F770W (F770W): 7.7 \u03bcm\n[INFO]   - jwst.miri.F1000W (F1000W): 10.0 \u03bcm\n[INFO]   - jwst.miri.F1500W (F1500W): 15.0 \u03bcm\n[INFO]   - jwst.miri.F2100W (F2100W): 21.0 \u03bcm\nData split summary:\n  Total samples: 1000\n  Training: 600 (60.0%)\n  Validation: 200 (20.0%)\n  Test: 200 (20.0%)\n</pre> In\u00a0[4]: Copied! <pre># Create features for modeling\ncatalog.create_feature_dataframe()\n\n# Get information about created features\nfeatures = catalog.get_features()\nprint(f\"Feature engineering complete:\")\nprint(f\"  Feature dataframe shape: {features.shape}\")\n</pre> # Create features for modeling catalog.create_feature_dataframe()  # Get information about created features features = catalog.get_features() print(f\"Feature engineering complete:\") print(f\"  Feature dataframe shape: {features.shape}\")  <pre>[INFO] Created feature dataframe with 121 columns and 1000 rows.\n[INFO] Created features are: ['jwst.nircam.F115W', 'jwst.nircam.F150W', 'jwst.nircam.F200W', 'jwst.nircam.F277W', 'jwst.nircam.F356W', 'jwst.nircam.F410M', 'jwst.nircam.F444W', 'jwst.miri.F770W', 'jwst.miri.F1000W', 'jwst.miri.F1500W', 'jwst.miri.F2100W', 'F2100W/F1500W', 'F2100W/F1000W', 'F2100W/F770W', 'F2100W/F444W', 'F2100W/F410M', 'F2100W/F356W', 'F2100W/F277W', 'F2100W/F200W', 'F2100W/F150W', 'F2100W/F115W', 'F1500W/F1000W', 'F1500W/F770W', 'F1500W/F444W', 'F1500W/F410M', 'F1500W/F356W', 'F1500W/F277W', 'F1500W/F200W', 'F1500W/F150W', 'F1500W/F115W', 'F1000W/F770W', 'F1000W/F444W', 'F1000W/F410M', 'F1000W/F356W', 'F1000W/F277W', 'F1000W/F200W', 'F1000W/F150W', 'F1000W/F115W', 'F770W/F444W', 'F770W/F410M', 'F770W/F356W', 'F770W/F277W', 'F770W/F200W', 'F770W/F150W', 'F770W/F115W', 'F444W/F410M', 'F444W/F356W', 'F444W/F277W', 'F444W/F200W', 'F444W/F150W', 'F444W/F115W', 'F410M/F356W', 'F410M/F277W', 'F410M/F200W', 'F410M/F150W', 'F410M/F115W', 'F356W/F277W', 'F356W/F200W', 'F356W/F150W', 'F356W/F115W', 'F277W/F200W', 'F277W/F150W', 'F277W/F115W', 'F200W/F150W', 'F200W/F115W', 'F150W/F115W', 'F2100W/F1500W^2', 'F2100W/F1000W^2', 'F2100W/F770W^2', 'F2100W/F444W^2', 'F2100W/F410M^2', 'F2100W/F356W^2', 'F2100W/F277W^2', 'F2100W/F200W^2', 'F2100W/F150W^2', 'F2100W/F115W^2', 'F1500W/F1000W^2', 'F1500W/F770W^2', 'F1500W/F444W^2', 'F1500W/F410M^2', 'F1500W/F356W^2', 'F1500W/F277W^2', 'F1500W/F200W^2', 'F1500W/F150W^2', 'F1500W/F115W^2', 'F1000W/F770W^2', 'F1000W/F444W^2', 'F1000W/F410M^2', 'F1000W/F356W^2', 'F1000W/F277W^2', 'F1000W/F200W^2', 'F1000W/F150W^2', 'F1000W/F115W^2', 'F770W/F444W^2', 'F770W/F410M^2', 'F770W/F356W^2', 'F770W/F277W^2', 'F770W/F200W^2', 'F770W/F150W^2', 'F770W/F115W^2', 'F444W/F410M^2', 'F444W/F356W^2', 'F444W/F277W^2', 'F444W/F200W^2', 'F444W/F150W^2', 'F444W/F115W^2', 'F410M/F356W^2', 'F410M/F277W^2', 'F410M/F200W^2', 'F410M/F150W^2', 'F410M/F115W^2', 'F356W/F277W^2', 'F356W/F200W^2', 'F356W/F150W^2', 'F356W/F115W^2', 'F277W/F200W^2', 'F277W/F150W^2', 'F277W/F115W^2', 'F200W/F150W^2', 'F200W/F115W^2', 'F150W/F115W^2']\nFeature engineering complete:\n  Feature dataframe shape: (1000, 121)\n</pre> In\u00a0[5]: Copied! <pre># Print columns in data\nprint( catalog.get_data().columns )\n</pre> # Print columns in data print( catalog.get_data().columns ) <pre>Index(['IRAC1', 'IRAC2', 'IRAC3', 'IRAC4', 'hst.acs.wfc.F606W',\n       'hst.acs.wfc.F814W', 'hst.wfc3.ir.F125W', 'hst.wfc3.ir.F140W',\n       'hst.wfc3.ir.F160W', 'jwst.miri.F1000W', 'jwst.miri.F1280W',\n       'jwst.miri.F1500W', 'jwst.miri.F1800W', 'jwst.miri.F2100W',\n       'jwst.miri.F770W', 'jwst.nircam.F115W', 'jwst.nircam.F150W',\n       'jwst.nircam.F200W', 'jwst.nircam.F277W', 'jwst.nircam.F356W',\n       'jwst.nircam.F410M', 'jwst.nircam.F444W', 'sfh.sfr100Myrs',\n       'stellar.m_star', 'agn.fracAGN', 'universe.redshift'],\n      dtype='object')\n</pre> <p>Let's say we wanted to create our own model to estimate the <code>agn.fracAGN</code> (i.e. frac$_\\text{AGN}$), parameter (recall that this is the fraction of 3-30 micron light attributable to an AGN power law). We first need to instantiate an AGNBoost model with <code>agn.fracAGN</code>  as the target variable. We already know that frac$_\\text{AGN}$ is bounded [0,1), but let's first quickly visualize the distribution of <code>agn.fracAGN</code> to decide what type of probability distribution to model.</p> In\u00a0[10]: Copied! <pre>fig, ax = plt.subplots()\n\nbins_width = 0.25\nbins = np.arange(0, 1 + bins_width, bins_width)\nfagn_data = catalog.get_data()['agn.fracAGN']\nsns.histplot( x = catalog.get_data()['agn.fracAGN'], ax = ax, bins = bins, color = 'xkcd:cobalt blue',  element = 'step', fill = True, alpha = 0.3)\n\nprint(f\"min fracAGN: {np.min(fagn_data )}\")\nprint(f\"max fracAGN: {np.max(fagn_data )}\")\n</pre> fig, ax = plt.subplots()  bins_width = 0.25 bins = np.arange(0, 1 + bins_width, bins_width) fagn_data = catalog.get_data()['agn.fracAGN'] sns.histplot( x = catalog.get_data()['agn.fracAGN'], ax = ax, bins = bins, color = 'xkcd:cobalt blue',  element = 'step', fill = True, alpha = 0.3)  print(f\"min fracAGN: {np.min(fagn_data )}\") print(f\"max fracAGN: {np.max(fagn_data )}\") <pre>min fracAGN: 0.0\nmax fracAGN: 0.99\n</pre> <p>As expected, the <code>agn.fracAGN</code> variable is bounded [0, 1), so we will model it with a zero-inflated beta distribution (ZABeta). Let us instantiate a AGNBoost model with <code>agn.fracAGN</code> as the target variable, and using ZABeta as the modeled probabiltiy distribution:</p> In\u00a0[12]: Copied! <pre># Initialize an AGNBoost model. The target variable is the name of the target variable column, and its value in the passed dictionary is the distribution used to model it.\nagnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),\n                          target_variables = {'agn.fracAGN' : 'ZABeta'},\n                         )\n</pre> # Initialize an AGNBoost model. The target variable is the name of the target variable column, and its value in the passed dictionary is the distribution used to model it. agnboost_m = model.AGNBoost( feature_names = catalog.get_feature_names(),                           target_variables = {'agn.fracAGN' : 'ZABeta'},                          ) In\u00a0[\u00a0]: Copied! <pre>\n</pre> In\u00a0[24]: Copied! <pre>param_dict = {\n    \"stabilization\":          [\"categorical\", [\"None\", \"MAD\", \"L2\"]],\n    \"response_fn\":          [\"categorical\", [\"softplus\", \"exp\"]],\n\n    \"eta\":              [\"none\", [.5]],\n    \"tree_method\":    [\"categorical\", [\"hist\",\"approx\"] ],\n    \"max_depth\":        [\"int\",   {\"low\": 3,      \"high\": 10,    \"log\": False}],\n    \"gamma\":            [\"float\", {\"low\": 1e-8,   \"high\": 40,    \"log\": True}], \n    \"min_child_weight\": [\"int\", {\"low\": 1,   \"high\": 250,  \"log\": False}],\n    \"lambda\":            [\"float\", {\"low\": 1,   \"high\": 150,    \"log\": True}], \n\n    \"device\" : [\"categorical\", [\"cpu\"] ]\n}\n</pre> param_dict = {     \"stabilization\":          [\"categorical\", [\"None\", \"MAD\", \"L2\"]],     \"response_fn\":          [\"categorical\", [\"softplus\", \"exp\"]],      \"eta\":              [\"none\", [.5]],     \"tree_method\":    [\"categorical\", [\"hist\",\"approx\"] ],     \"max_depth\":        [\"int\",   {\"low\": 3,      \"high\": 10,    \"log\": False}],     \"gamma\":            [\"float\", {\"low\": 1e-8,   \"high\": 40,    \"log\": True}],      \"min_child_weight\": [\"int\", {\"low\": 1,   \"high\": 250,  \"log\": False}],     \"lambda\":            [\"float\", {\"low\": 1,   \"high\": 150,    \"log\": True}],       \"device\" : [\"categorical\", [\"cpu\"] ] }  In\u00a0[25]: Copied! <pre>best_params_1stpass = agnboost_m.tune_model( model_name = 'agn.fracAGN',\n                                             param_grid = param_dict,\n                                             dtune = catalog,\n                                            max_minutes = 10, #We will tune for 10 minutes. You will probably want to tune for much longer!\n                                             split_type = 'trainval'  #Options: train, val, test, trainval. trainval uses the combined train+val test split\n                                           )\n</pre> best_params_1stpass = agnboost_m.tune_model( model_name = 'agn.fracAGN',                                              param_grid = param_dict,                                              dtune = catalog,                                             max_minutes = 10, #We will tune for 10 minutes. You will probably want to tune for much longer!                                              split_type = 'trainval'  #Options: train, val, test, trainval. trainval uses the combined train+val test split                                            ) <pre>2025-05-31 21:59:39,310 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the trainval set stored in the passed Catalog.\n``multivariate`` option is an experimental feature. The interface can change in the future.\n[I 2025-05-31 21:59:39,337] A new study created in memory with name: no-name-15235329-2d3f-4858-ab06-f5bb2604f0d8\n</pre> <pre>[INFO] Created feature dataframe with 121 columns and 1000 rows.\n[INFO] Created features are: ['jwst.nircam.F115W', 'jwst.nircam.F150W', 'jwst.nircam.F200W', 'jwst.nircam.F277W', 'jwst.nircam.F356W', 'jwst.nircam.F410M', 'jwst.nircam.F444W', 'jwst.miri.F770W', 'jwst.miri.F1000W', 'jwst.miri.F1500W', 'jwst.miri.F2100W', 'F2100W/F1500W', 'F2100W/F1000W', 'F2100W/F770W', 'F2100W/F444W', 'F2100W/F410M', 'F2100W/F356W', 'F2100W/F277W', 'F2100W/F200W', 'F2100W/F150W', 'F2100W/F115W', 'F1500W/F1000W', 'F1500W/F770W', 'F1500W/F444W', 'F1500W/F410M', 'F1500W/F356W', 'F1500W/F277W', 'F1500W/F200W', 'F1500W/F150W', 'F1500W/F115W', 'F1000W/F770W', 'F1000W/F444W', 'F1000W/F410M', 'F1000W/F356W', 'F1000W/F277W', 'F1000W/F200W', 'F1000W/F150W', 'F1000W/F115W', 'F770W/F444W', 'F770W/F410M', 'F770W/F356W', 'F770W/F277W', 'F770W/F200W', 'F770W/F150W', 'F770W/F115W', 'F444W/F410M', 'F444W/F356W', 'F444W/F277W', 'F444W/F200W', 'F444W/F150W', 'F444W/F115W', 'F410M/F356W', 'F410M/F277W', 'F410M/F200W', 'F410M/F150W', 'F410M/F115W', 'F356W/F277W', 'F356W/F200W', 'F356W/F150W', 'F356W/F115W', 'F277W/F200W', 'F277W/F150W', 'F277W/F115W', 'F200W/F150W', 'F200W/F115W', 'F150W/F115W', 'F2100W/F1500W^2', 'F2100W/F1000W^2', 'F2100W/F770W^2', 'F2100W/F444W^2', 'F2100W/F410M^2', 'F2100W/F356W^2', 'F2100W/F277W^2', 'F2100W/F200W^2', 'F2100W/F150W^2', 'F2100W/F115W^2', 'F1500W/F1000W^2', 'F1500W/F770W^2', 'F1500W/F444W^2', 'F1500W/F410M^2', 'F1500W/F356W^2', 'F1500W/F277W^2', 'F1500W/F200W^2', 'F1500W/F150W^2', 'F1500W/F115W^2', 'F1000W/F770W^2', 'F1000W/F444W^2', 'F1000W/F410M^2', 'F1000W/F356W^2', 'F1000W/F277W^2', 'F1000W/F200W^2', 'F1000W/F150W^2', 'F1000W/F115W^2', 'F770W/F444W^2', 'F770W/F410M^2', 'F770W/F356W^2', 'F770W/F277W^2', 'F770W/F200W^2', 'F770W/F150W^2', 'F770W/F115W^2', 'F444W/F410M^2', 'F444W/F356W^2', 'F444W/F277W^2', 'F444W/F200W^2', 'F444W/F150W^2', 'F444W/F115W^2', 'F410M/F356W^2', 'F410M/F277W^2', 'F410M/F200W^2', 'F410M/F150W^2', 'F410M/F115W^2', 'F356W/F277W^2', 'F356W/F200W^2', 'F356W/F150W^2', 'F356W/F115W^2', 'F277W/F200W^2', 'F277W/F150W^2', 'F277W/F115W^2', 'F200W/F150W^2', 'F200W/F115W^2', 'F150W/F115W^2']\n</pre> <pre>   0%|          | 00:00/10:00</pre> <pre>4.3 seconds\n[I 2025-05-31 21:59:43,603] Trial 0 finished with value: -538.3346859999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.5396329035358637e-05, 'min_child_weight': 17, 'lambda': 25.752571083027494, 'device': 'cpu'}. Best is trial 0 with value: -538.3346859999999.\n1.4 seconds\n[I 2025-05-31 21:59:45,029] Trial 1 finished with value: -233.0358885 and parameters: {'stabilization': 'L2', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 0.009805474860380612, 'min_child_weight': 222, 'lambda': 36.97821636577757, 'device': 'cpu'}. Best is trial 0 with value: -538.3346859999999.\n2.0 seconds\n[I 2025-05-31 21:59:47,073] Trial 2 finished with value: -568.1241150000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 1.4233405719313036e-08, 'min_child_weight': 156, 'lambda': 5.785645476080961, 'device': 'cpu'}. Best is trial 2 with value: -568.1241150000001.\n1.7 seconds\n[I 2025-05-31 21:59:48,775] Trial 3 finished with value: -528.4065095000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.41164506099767684, 'min_child_weight': 37, 'lambda': 4.999444210250469, 'device': 'cpu'}. Best is trial 2 with value: -568.1241150000001.\n2.0 seconds\n[I 2025-05-31 21:59:50,748] Trial 4 finished with value: -539.612793 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 5, 'gamma': 0.029989033512431876, 'min_child_weight': 72, 'lambda': 4.801093482603916, 'device': 'cpu'}. Best is trial 2 with value: -568.1241150000001.\n</pre> <pre>invalid value encountered in subtract\n</pre> <pre>1.9 seconds\n[I 2025-05-31 21:59:52,643] Trial 5 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 4.7025417911805535e-05, 'min_child_weight': 80, 'lambda': 3.4752834971002446, 'device': 'cpu'}. Best is trial 2 with value: -568.1241150000001.\n1.9 seconds\n[I 2025-05-31 21:59:54,506] Trial 6 finished with value: -440.689758 and parameters: {'stabilization': 'L2', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 1.7553267516359528e-05, 'min_child_weight': 80, 'lambda': 50.83870744181027, 'device': 'cpu'}. Best is trial 2 with value: -568.1241150000001.\n2.1 seconds\n[I 2025-05-31 21:59:56,626] Trial 7 finished with value: -571.186035 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 1.5543187142694343e-07, 'min_child_weight': 134, 'lambda': 7.636122743857147, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.7 seconds\n[I 2025-05-31 21:59:58,351] Trial 8 finished with value: -414.29258749999997 and parameters: {'stabilization': 'L2', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 5, 'gamma': 3.7930128620486294, 'min_child_weight': 84, 'lambda': 9.221570011904374, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.9 seconds\n[I 2025-05-31 22:00:00,224] Trial 9 finished with value: -550.360321 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 0.016226422714380257, 'min_child_weight': 22, 'lambda': 5.7225183929734404, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.2 seconds\n[I 2025-05-31 22:00:02,403] Trial 10 finished with value: -537.8868104999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 4, 'gamma': 1.7940320408573984e-06, 'min_child_weight': 219, 'lambda': 5.496936537996518, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.2 seconds\n[I 2025-05-31 22:00:04,642] Trial 11 finished with value: -511.49543750000004 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 5, 'gamma': 1.5299822988853193e-08, 'min_child_weight': 58, 'lambda': 22.918740299170256, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.8 seconds\n[I 2025-05-31 22:00:06,451] Trial 12 finished with value: -536.672455 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 1.0310454091770936e-08, 'min_child_weight': 218, 'lambda': 1.2753821787593513, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n[I 2025-05-31 22:00:07,085] Trial 13 pruned. Trial was pruned at iteration 20.\n1.7 seconds\n[I 2025-05-31 22:00:08,845] Trial 14 finished with value: -549.9922489999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 5.203454941578109e-08, 'min_child_weight': 179, 'lambda': 28.40801714583442, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.1 seconds\n[I 2025-05-31 22:00:10,978] Trial 15 finished with value: -552.591797 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.5029511921947606e-06, 'min_child_weight': 136, 'lambda': 8.546611186771843, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n[I 2025-05-31 22:00:11,651] Trial 16 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:12,350] Trial 17 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:13,140] Trial 18 pruned. Trial was pruned at iteration 20.\n1.8 seconds\n[I 2025-05-31 22:00:14,967] Trial 19 finished with value: -506.10517849999997 and parameters: {'stabilization': 'L2', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 2.712873962951033e-07, 'min_child_weight': 53, 'lambda': 2.6021551800638707, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.9 seconds\n[I 2025-05-31 22:00:16,880] Trial 20 finished with value: -536.210846 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 7, 'gamma': 1.1551116608310091e-06, 'min_child_weight': 114, 'lambda': 2.192434050611229, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.1 seconds\n[I 2025-05-31 22:00:19,026] Trial 21 finished with value: -532.755478 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 6.531527613555491e-05, 'min_child_weight': 124, 'lambda': 5.228658688245536, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.1 seconds\n[I 2025-05-31 22:00:21,112] Trial 22 finished with value: -552.2903745000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 2.206230461601861e-08, 'min_child_weight': 132, 'lambda': 10.879565933655973, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.8 seconds\n[I 2025-05-31 22:00:22,958] Trial 23 finished with value: -528.7008364999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 4.636555608485337e-06, 'min_child_weight': 158, 'lambda': 70.11447167065849, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n[I 2025-05-31 22:00:23,565] Trial 24 pruned. Trial was pruned at iteration 20.\n2.3 seconds\n[I 2025-05-31 22:00:25,880] Trial 25 finished with value: -565.2005005 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 3.3097555524427615e-08, 'min_child_weight': 103, 'lambda': 7.4590062241080135, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n3.4 seconds\n[I 2025-05-31 22:00:29,298] Trial 26 finished with value: -536.43573 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 4.725100012205021e-08, 'min_child_weight': 28, 'lambda': 16.02532558151139, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.5 seconds\n[I 2025-05-31 22:00:31,767] Trial 27 finished with value: -557.6807859999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 4.1196652381410564e-06, 'min_child_weight': 133, 'lambda': 6.635485580564186, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n2.7 seconds\n[I 2025-05-31 22:00:34,476] Trial 28 finished with value: -526.4791565 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 2.1382968788984822e-07, 'min_child_weight': 87, 'lambda': 20.566345108533405, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n[I 2025-05-31 22:00:35,195] Trial 29 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:00:37,120] Trial 30 finished with value: -559.1357725 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 1.7756430613316088e-08, 'min_child_weight': 149, 'lambda': 8.275129902685977, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.8 seconds\n[I 2025-05-31 22:00:38,904] Trial 31 finished with value: -566.6150514999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 1.2463050681972017e-08, 'min_child_weight': 146, 'lambda': 9.775925995982428, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n1.9 seconds\n[I 2025-05-31 22:00:40,806] Trial 32 finished with value: -560.3184205 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 1.4046435953988977e-08, 'min_child_weight': 129, 'lambda': 9.162166516737194, 'device': 'cpu'}. Best is trial 7 with value: -571.186035.\n[I 2025-05-31 22:00:41,397] Trial 33 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:41,965] Trial 34 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:42,633] Trial 35 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:43,235] Trial 36 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:43,886] Trial 37 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:44,462] Trial 38 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:45,106] Trial 39 pruned. Trial was pruned at iteration 20.\n2.2 seconds\n[I 2025-05-31 22:00:47,300] Trial 40 finished with value: -571.6891175000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 7.860624643290146e-06, 'min_child_weight': 108, 'lambda': 9.032580896099716, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n</pre> <pre>invalid value encountered in add\n</pre> <pre>[I 2025-05-31 22:00:49,287] Trial 41 pruned. Trial was pruned at iteration 125.\n[I 2025-05-31 22:00:50,026] Trial 42 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:50,761] Trial 43 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:00:52,781] Trial 44 pruned. Trial was pruned at iteration 125.\n[I 2025-05-31 22:00:53,367] Trial 45 pruned. Trial was pruned at iteration 20.\n1.8 seconds\n[I 2025-05-31 22:00:55,195] Trial 46 finished with value: -565.3455505 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 4, 'gamma': 1.8679492218445252e-07, 'min_child_weight': 126, 'lambda': 3.4176836684664607, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n2.1 seconds\n[I 2025-05-31 22:00:57,349] Trial 47 finished with value: -550.0536195 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 5, 'gamma': 5.007938682813857e-08, 'min_child_weight': 122, 'lambda': 5.789844346969788, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.6 seconds\n[I 2025-05-31 22:00:59,003] Trial 48 finished with value: -551.1981505 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 2.9976062365699335e-05, 'min_child_weight': 116, 'lambda': 1.8864465740640275, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.9 seconds\n[I 2025-05-31 22:01:00,896] Trial 49 finished with value: -559.3542175 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 4, 'gamma': 1.1595978789306633e-06, 'min_child_weight': 157, 'lambda': 1.7606221450790216, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n[I 2025-05-31 22:01:01,477] Trial 50 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:02,069] Trial 51 pruned. Trial was pruned at iteration 20.\n2.0 seconds\n[I 2025-05-31 22:01:04,052] Trial 52 finished with value: -544.5730590000001 and parameters: {'stabilization': 'L2', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 5, 'gamma': 0.013184669771200148, 'min_child_weight': 13, 'lambda': 11.86539041879886, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.7 seconds\n[I 2025-05-31 22:01:05,747] Trial 53 finished with value: -551.305145 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 9.953853208798318e-08, 'min_child_weight': 227, 'lambda': 12.166813817434917, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.8 seconds\n[I 2025-05-31 22:01:07,510] Trial 54 finished with value: -537.8863220000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 4, 'gamma': 5.0711814335032386e-05, 'min_child_weight': 104, 'lambda': 12.41428108353893, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.6 seconds\n[I 2025-05-31 22:01:09,080] Trial 55 finished with value: -563.784027 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.0015880752905682901, 'min_child_weight': 79, 'lambda': 119.42356346215409, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n[I 2025-05-31 22:01:09,720] Trial 56 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:10,380] Trial 57 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:11,014] Trial 58 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:11,629] Trial 59 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:12,209] Trial 60 pruned. Trial was pruned at iteration 20.\n1.6 seconds\n[I 2025-05-31 22:01:13,805] Trial 61 finished with value: -538.716034 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.0012966855940974174, 'min_child_weight': 65, 'lambda': 146.2106582631281, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.6 seconds\n[I 2025-05-31 22:01:15,374] Trial 62 finished with value: -529.4952695 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 0.03248268866167507, 'min_child_weight': 169, 'lambda': 86.1387905861123, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n</pre> <pre>invalid value encountered in subtract\ninvalid value encountered in add\n</pre> <pre>2.1 seconds\n[I 2025-05-31 22:01:17,491] Trial 63 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 4.82218843841422, 'min_child_weight': 97, 'lambda': 46.86953830724628, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n[I 2025-05-31 22:01:18,141] Trial 64 pruned. Trial was pruned at iteration 20.\n</pre> <pre>invalid value encountered in subtract\n</pre> <pre>2.1 seconds\n[I 2025-05-31 22:01:20,215] Trial 65 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.000131061786050065, 'min_child_weight': 68, 'lambda': 21.07070990752435, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n1.9 seconds\n[I 2025-05-31 22:01:22,081] Trial 66 finished with value: -554.254608 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 5, 'gamma': 7.936866283744524e-07, 'min_child_weight': 70, 'lambda': 24.188763934320402, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n</pre> <pre>invalid value encountered in subtract\n</pre> <pre>2.2 seconds\n[I 2025-05-31 22:01:24,331] Trial 67 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 3, 'gamma': 0.02637272250292243, 'min_child_weight': 75, 'lambda': 70.54541585403223, 'device': 'cpu'}. Best is trial 40 with value: -571.6891175000001.\n[I 2025-05-31 22:01:24,915] Trial 68 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:25,619] Trial 69 pruned. Trial was pruned at iteration 20.\n1.8 seconds\n[I 2025-05-31 22:01:27,436] Trial 70 finished with value: -572.7865294999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.007191774416874417, 'min_child_weight': 96, 'lambda': 122.73669366406183, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.8 seconds\n[I 2025-05-31 22:01:29,235] Trial 71 finished with value: -534.8459625 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 0.00034089888889644523, 'min_child_weight': 124, 'lambda': 41.1675794109943, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.7 seconds\n[I 2025-05-31 22:01:30,930] Trial 72 finished with value: -530.1312869999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.11517957680592521, 'min_child_weight': 51, 'lambda': 105.83644005065398, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.8 seconds\n[I 2025-05-31 22:01:32,695] Trial 73 finished with value: -545.870453 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 3, 'gamma': 1.1231274710787972e-07, 'min_child_weight': 58, 'lambda': 2.042495528112113, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:33,405] Trial 74 pruned. Trial was pruned at iteration 20.\n2.0 seconds\n[I 2025-05-31 22:01:35,384] Trial 75 finished with value: -544.8724669999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 0.00041443077655468804, 'min_child_weight': 49, 'lambda': 141.06296878866524, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:36,010] Trial 76 pruned. Trial was pruned at iteration 20.\n1.7 seconds\n[I 2025-05-31 22:01:37,768] Trial 77 finished with value: -545.0274655000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 1.3723014280978176e-08, 'min_child_weight': 170, 'lambda': 4.881785497786435, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n2.0 seconds\n[I 2025-05-31 22:01:39,808] Trial 78 finished with value: -535.615631 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 1.2973099002718484e-06, 'min_child_weight': 80, 'lambda': 16.370492551210816, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:40,523] Trial 79 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:01:41,151] Trial 80 pruned. Trial was pruned at iteration 20.\n2.0 seconds\n[I 2025-05-31 22:01:43,166] Trial 81 finished with value: -555.6707155 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 1.3541146314592335e-08, 'min_child_weight': 98, 'lambda': 12.916317472553699, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:43,820] Trial 82 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:01:45,756] Trial 83 finished with value: -549.8399045 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 7, 'gamma': 3.7625956186465914e-08, 'min_child_weight': 118, 'lambda': 7.829757116547688, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.7 seconds\n[I 2025-05-31 22:01:47,439] Trial 84 finished with value: -535.5687865 and parameters: {'stabilization': 'L2', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 5, 'gamma': 0.0056184079927858325, 'min_child_weight': 43, 'lambda': 1.2266838607730863, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:48,106] Trial 85 pruned. Trial was pruned at iteration 20.\n1.8 seconds\n[I 2025-05-31 22:01:49,947] Trial 86 finished with value: -556.758148 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.021367393522320883, 'min_child_weight': 161, 'lambda': 134.5369432225549, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.8 seconds\n[I 2025-05-31 22:01:51,782] Trial 87 finished with value: -538.9547729999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 5.573561560266356e-07, 'min_child_weight': 135, 'lambda': 52.149646051217054, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n[I 2025-05-31 22:01:52,519] Trial 88 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:01:54,465] Trial 89 finished with value: -543.941101 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 1.6879980509145527e-07, 'min_child_weight': 202, 'lambda': 4.764289713123308, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n2.1 seconds\n[I 2025-05-31 22:01:56,600] Trial 90 finished with value: -561.8856505 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 7, 'gamma': 1.282766479899665, 'min_child_weight': 91, 'lambda': 95.1504140205716, 'device': 'cpu'}. Best is trial 70 with value: -572.7865294999999.\n1.9 seconds\n[I 2025-05-31 22:01:58,544] Trial 91 finished with value: -577.146088 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.15521317103010426, 'min_child_weight': 108, 'lambda': 113.6533104326656, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n[I 2025-05-31 22:01:59,187] Trial 92 pruned. Trial was pruned at iteration 20.\n1.8 seconds\n[I 2025-05-31 22:02:01,038] Trial 93 finished with value: -569.622406 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 5.054061246489338, 'min_child_weight': 140, 'lambda': 83.43365168986566, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n2.1 seconds\n[I 2025-05-31 22:02:03,107] Trial 94 finished with value: -557.7799685 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.05953051769941643, 'min_child_weight': 81, 'lambda': 66.51962804228228, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n[I 2025-05-31 22:02:03,719] Trial 95 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:02:05,590] Trial 96 finished with value: -550.1198119999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 5.916949492530223, 'min_child_weight': 168, 'lambda': 96.54305369005682, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n1.9 seconds\n[I 2025-05-31 22:02:07,479] Trial 97 finished with value: -569.898254 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.7867385174584295, 'min_child_weight': 141, 'lambda': 88.73247867666245, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n2.0 seconds\n[I 2025-05-31 22:02:09,506] Trial 98 finished with value: -560.8153995 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 2.013476162679143, 'min_child_weight': 174, 'lambda': 73.87906293203815, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n[I 2025-05-31 22:02:10,100] Trial 99 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:02:10,793] Trial 100 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:02:12,662] Trial 101 finished with value: -576.359009 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.03939823179896599, 'min_child_weight': 129, 'lambda': 98.55138888202369, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n[I 2025-05-31 22:02:13,280] Trial 102 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:02:15,154] Trial 103 finished with value: -558.8230590000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 7, 'gamma': 0.00710884873690168, 'min_child_weight': 172, 'lambda': 126.3949915037119, 'device': 'cpu'}. Best is trial 91 with value: -577.146088.\n1.9 seconds\n[I 2025-05-31 22:02:17,081] Trial 104 finished with value: -585.4270630000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.0019938022688501832, 'min_child_weight': 105, 'lambda': 70.07508991962831, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.2 seconds\n[I 2025-05-31 22:02:19,262] Trial 105 finished with value: -565.6244815 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.0014696837690568924, 'min_child_weight': 90, 'lambda': 112.37693943301231, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.1 seconds\n[I 2025-05-31 22:02:21,339] Trial 106 finished with value: -566.5224915 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 8.068991626456967e-05, 'min_child_weight': 86, 'lambda': 120.32719277321299, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.9 seconds\n[I 2025-05-31 22:02:23,233] Trial 107 finished with value: -565.115906 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.09390706538319665, 'min_child_weight': 141, 'lambda': 133.79327025180092, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:23,854] Trial 108 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:02:25,722] Trial 109 finished with value: -543.6245115 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.0021618337111951273, 'min_child_weight': 164, 'lambda': 24.877169306110012, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.6 seconds\n[I 2025-05-31 22:02:27,348] Trial 110 finished with value: -556.3331605000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 8.023579739007078, 'min_child_weight': 98, 'lambda': 26.055687660008193, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.0 seconds\n[I 2025-05-31 22:02:29,323] Trial 111 finished with value: -568.5903625 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.020213745529973536, 'min_child_weight': 102, 'lambda': 70.49304036914943, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.3 seconds\n[I 2025-05-31 22:02:31,681] Trial 112 finished with value: -571.2257079999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.00046499685552308344, 'min_child_weight': 94, 'lambda': 149.3303661556515, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.7 seconds\n[I 2025-05-31 22:02:34,369] Trial 113 finished with value: -553.1926269999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.00021643442379436624, 'min_child_weight': 69, 'lambda': 71.98310710317897, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:34,994] Trial 114 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:02:35,660] Trial 115 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:02:36,342] Trial 116 pruned. Trial was pruned at iteration 20.\n2.3 seconds\n[I 2025-05-31 22:02:38,641] Trial 117 finished with value: -570.8308105 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 0.0035137973294394166, 'min_child_weight': 134, 'lambda': 127.40266355787772, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.4 seconds\n[I 2025-05-31 22:02:41,013] Trial 118 finished with value: -560.286987 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.00783175568676248, 'min_child_weight': 79, 'lambda': 111.96176947563438, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.0 seconds\n[I 2025-05-31 22:02:43,028] Trial 119 finished with value: -564.5676880000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.0007596093189488856, 'min_child_weight': 157, 'lambda': 131.70317411761158, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.2 seconds\n[I 2025-05-31 22:02:45,220] Trial 120 finished with value: -570.505127 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 3.3569837959173813e-07, 'min_child_weight': 107, 'lambda': 116.93883747624722, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:45,811] Trial 121 pruned. Trial was pruned at iteration 20.\n1.6 seconds\n[I 2025-05-31 22:02:47,465] Trial 122 finished with value: -567.2421265 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 7.5564354921252495, 'min_child_weight': 121, 'lambda': 106.18723481429146, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.1 seconds\n[I 2025-05-31 22:02:49,574] Trial 123 finished with value: -555.3678285000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 5.390663400151784e-08, 'min_child_weight': 106, 'lambda': 72.9932561448359, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:50,173] Trial 124 pruned. Trial was pruned at iteration 20.\n2.4 seconds\n[I 2025-05-31 22:02:52,623] Trial 125 finished with value: -566.4755555 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 0.04263672929306466, 'min_child_weight': 88, 'lambda': 117.40530076550712, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.1 seconds\n[I 2025-05-31 22:02:54,762] Trial 126 finished with value: -554.646027 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 5, 'gamma': 0.0007435140429590045, 'min_child_weight': 89, 'lambda': 147.96834496006034, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:55,358] Trial 127 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:02:57,861] Trial 128 finished with value: -558.957367 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 1.3912651924774933e-07, 'min_child_weight': 106, 'lambda': 9.976358215091079, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:02:58,505] Trial 129 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:02:59,109] Trial 130 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:02:59,729] Trial 131 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:00,342] Trial 132 pruned. Trial was pruned at iteration 20.\n</pre> <pre>invalid value encountered in subtract\n</pre> <pre>2.5 seconds\n[I 2025-05-31 22:03:02,860] Trial 133 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 8.52159439565328e-08, 'min_child_weight': 53, 'lambda': 97.20698026222445, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:03,501] Trial 134 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:03:05,455] Trial 135 finished with value: -564.3602295 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.983718521423068, 'min_child_weight': 115, 'lambda': 126.82858131454422, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:06,076] Trial 136 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:06,775] Trial 137 pruned. Trial was pruned at iteration 20.\n1.5 seconds\n[I 2025-05-31 22:03:08,281] Trial 138 finished with value: -562.680664 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 15.019922925552814, 'min_child_weight': 127, 'lambda': 25.365996026225513, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.7 seconds\n[I 2025-05-31 22:03:09,949] Trial 139 finished with value: -569.9113769999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 9.501344390986599, 'min_child_weight': 90, 'lambda': 65.7399190186448, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:10,623] Trial 140 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:11,252] Trial 141 pruned. Trial was pruned at iteration 20.\n2.0 seconds\n[I 2025-05-31 22:03:13,230] Trial 142 finished with value: -574.0078430000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 2.3132838969836595, 'min_child_weight': 132, 'lambda': 130.66472371122643, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.0 seconds\n[I 2025-05-31 22:03:15,248] Trial 143 finished with value: -579.5551755 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.016202155007680775, 'min_child_weight': 96, 'lambda': 38.935427269526215, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.3 seconds\n[I 2025-05-31 22:03:17,537] Trial 144 finished with value: -562.923828 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 2.770545064265404e-06, 'min_child_weight': 94, 'lambda': 108.30594390272606, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.9 seconds\n[I 2025-05-31 22:03:19,451] Trial 145 finished with value: -557.0128784999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.07305821559996975, 'min_child_weight': 99, 'lambda': 36.7362950282458, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.7 seconds\n[I 2025-05-31 22:03:21,189] Trial 146 finished with value: -548.016266 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.0004368849395024274, 'min_child_weight': 171, 'lambda': 102.35060705640052, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.8 seconds\n[I 2025-05-31 22:03:23,965] Trial 147 finished with value: -555.720581 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 8.157899770713593e-08, 'min_child_weight': 29, 'lambda': 144.48042547089233, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.2 seconds\n[I 2025-05-31 22:03:26,146] Trial 148 finished with value: -583.9998475 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 7.025135466448473e-05, 'min_child_weight': 72, 'lambda': 13.639521462455042, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:26,852] Trial 149 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:27,454] Trial 150 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:28,306] Trial 151 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:28,898] Trial 152 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:29,566] Trial 153 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:30,272] Trial 154 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:30,936] Trial 155 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:03:31,518] Trial 156 pruned. Trial was pruned at iteration 20.\n2.1 seconds\n[I 2025-05-31 22:03:33,668] Trial 157 finished with value: -562.604645 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 8.263882864616723e-05, 'min_child_weight': 72, 'lambda': 39.75121615604367, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.3 seconds\n[I 2025-05-31 22:03:35,956] Trial 158 finished with value: -551.5425415 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.000401529310553666, 'min_child_weight': 78, 'lambda': 12.51160913599006, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.7 seconds\n[I 2025-05-31 22:03:37,631] Trial 159 finished with value: -571.4435424999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.001999028217939728, 'min_child_weight': 96, 'lambda': 52.55218079393602, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.9 seconds\n[I 2025-05-31 22:03:39,533] Trial 160 finished with value: -557.6372985 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 5.575523471135619e-07, 'min_child_weight': 133, 'lambda': 8.353739646631562, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.7 seconds\n[I 2025-05-31 22:03:41,267] Trial 161 finished with value: -561.934113 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 0.00905510004166771, 'min_child_weight': 132, 'lambda': 35.76373336616059, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.9 seconds\n[I 2025-05-31 22:03:43,209] Trial 162 finished with value: -557.813263 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.0065216525836809145, 'min_child_weight': 108, 'lambda': 93.22615130610858, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:43,792] Trial 163 pruned. Trial was pruned at iteration 20.\n2.3 seconds\n[I 2025-05-31 22:03:46,090] Trial 164 finished with value: -562.4273985 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.03436998540917756, 'min_child_weight': 81, 'lambda': 78.55235813813921, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:46,670] Trial 165 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:03:48,596] Trial 166 finished with value: -566.3221435 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 3, 'gamma': 0.6086420759927776, 'min_child_weight': 93, 'lambda': 94.82010597823499, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.7 seconds\n[I 2025-05-31 22:03:50,337] Trial 167 finished with value: -546.8621215000001 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 3, 'gamma': 0.0004712016445965929, 'min_child_weight': 53, 'lambda': 39.902940772667364, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.1 seconds\n[I 2025-05-31 22:03:52,472] Trial 168 finished with value: -577.0916745 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 0.002911621446984091, 'min_child_weight': 121, 'lambda': 144.34064380260932, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n1.8 seconds\n[I 2025-05-31 22:03:54,325] Trial 169 finished with value: -554.0499875 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.8925027982061278, 'min_child_weight': 160, 'lambda': 138.05090325644838, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.0 seconds\n[I 2025-05-31 22:03:56,383] Trial 170 finished with value: -562.6233215 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 0.01799333571184205, 'min_child_weight': 135, 'lambda': 145.35636465334727, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n2.0 seconds\n[I 2025-05-31 22:03:58,412] Trial 171 finished with value: -574.9880674999999 and parameters: {'stabilization': 'MAD', 'response_fn': 'softplus', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.12754188516895057, 'min_child_weight': 127, 'lambda': 117.71054455322371, 'device': 'cpu'}. Best is trial 104 with value: -585.4270630000001.\n[I 2025-05-31 22:03:59,055] Trial 172 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:04:01,590] Trial 173 finished with value: -593.6697995 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.005434830633400749, 'min_child_weight': 33, 'lambda': 119.55250590983898, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n2.4 seconds\n[I 2025-05-31 22:04:04,040] Trial 174 finished with value: -588.3483275 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 0.006448513670224484, 'min_child_weight': 25, 'lambda': 59.98751721071317, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n2.4 seconds\n[I 2025-05-31 22:04:06,437] Trial 175 finished with value: -565.9078065 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 0.006973605271577416, 'min_child_weight': 30, 'lambda': 25.26985876788745, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n2.7 seconds\n[I 2025-05-31 22:04:09,165] Trial 176 finished with value: -587.1781315000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.0014749625770416434, 'min_child_weight': 10, 'lambda': 30.063040255345115, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n1.8 seconds\n[I 2025-05-31 22:04:10,967] Trial 177 finished with value: -586.3135374999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.8595646128529425, 'min_child_weight': 32, 'lambda': 122.31996900370613, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n2.3 seconds\n[I 2025-05-31 22:04:13,241] Trial 178 finished with value: -580.9576415 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 0.020901799267839614, 'min_child_weight': 35, 'lambda': 73.6979159455691, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n1.7 seconds\n[I 2025-05-31 22:04:15,005] Trial 179 finished with value: -585.6549685 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.1299263110621307, 'min_child_weight': 20, 'lambda': 129.79762970243408, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n[I 2025-05-31 22:04:15,633] Trial 180 pruned. Trial was pruned at iteration 20.\n2.4 seconds\n[I 2025-05-31 22:04:18,011] Trial 181 finished with value: -565.0000915 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 0.03946945825129624, 'min_child_weight': 4, 'lambda': 90.25280271421961, 'device': 'cpu'}. Best is trial 173 with value: -593.6697995.\n[I 2025-05-31 22:04:18,643] Trial 182 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:04:19,522] Trial 183 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:04:22,078] Trial 184 finished with value: -597.4772945 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 9.596151705181202e-05, 'min_child_weight': 31, 'lambda': 98.78838252818872, 'device': 'cpu'}. Best is trial 184 with value: -597.4772945.\n2.3 seconds\n[I 2025-05-31 22:04:24,407] Trial 185 finished with value: -575.258911 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 2.909575521235058e-06, 'min_child_weight': 48, 'lambda': 44.8010066255139, 'device': 'cpu'}. Best is trial 184 with value: -597.4772945.\n[I 2025-05-31 22:04:25,072] Trial 186 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:04:27,548] Trial 187 finished with value: -599.183777 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.9111290329601907e-06, 'min_child_weight': 38, 'lambda': 142.4996020188469, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.5 seconds\n[I 2025-05-31 22:04:30,052] Trial 188 finished with value: -581.9074705 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 4.1465277784778914e-05, 'min_child_weight': 36, 'lambda': 77.28320288251074, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.8 seconds\n[I 2025-05-31 22:04:32,830] Trial 189 finished with value: -591.335327 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.309232502330243e-06, 'min_child_weight': 16, 'lambda': 110.55214791189547, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.9 seconds\n[I 2025-05-31 22:04:35,741] Trial 190 finished with value: -592.5753784999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 7.591166070282343e-07, 'min_child_weight': 16, 'lambda': 108.35646225989339, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.8 seconds\n[I 2025-05-31 22:04:38,544] Trial 191 finished with value: -582.3792725000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 7.631319873667642e-05, 'min_child_weight': 17, 'lambda': 128.64667335152433, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:04:39,241] Trial 192 pruned. Trial was pruned at iteration 20.\n2.9 seconds\n[I 2025-05-31 22:04:42,145] Trial 193 finished with value: -587.864807 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.643805723692164e-05, 'min_child_weight': 8, 'lambda': 72.13446750665545, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.7 seconds\n[I 2025-05-31 22:04:44,889] Trial 194 finished with value: -587.7263794999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 5.4181461931134994e-05, 'min_child_weight': 15, 'lambda': 79.05088236833085, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.3 seconds\n[I 2025-05-31 22:04:47,195] Trial 195 finished with value: -587.3322145 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 3.49591650400422e-05, 'min_child_weight': 12, 'lambda': 43.20545390995926, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.1 seconds\n[I 2025-05-31 22:04:49,335] Trial 196 finished with value: -597.4509885 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 0.0009592064618035505, 'min_child_weight': 36, 'lambda': 110.58807516276524, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.2 seconds\n[I 2025-05-31 22:04:51,527] Trial 197 finished with value: -589.43396 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.0024079239940583276, 'min_child_weight': 15, 'lambda': 61.72865332470975, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.4 seconds\n[I 2025-05-31 22:04:53,934] Trial 198 finished with value: -577.5836795 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 7.169945701767782e-05, 'min_child_weight': 3, 'lambda': 93.66505999871147, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n1.8 seconds\n[I 2025-05-31 22:04:55,721] Trial 199 finished with value: -574.5710449999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.16071475285654765, 'min_child_weight': 48, 'lambda': 54.13010926380267, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:04:56,402] Trial 200 pruned. Trial was pruned at iteration 20.\n2.1 seconds\n[I 2025-05-31 22:04:58,546] Trial 201 finished with value: -569.057373 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 3.447688058528674e-05, 'min_child_weight': 38, 'lambda': 42.669568027841, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:05:01,193] Trial 202 finished with value: -582.401276 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 6.714145151965375e-05, 'min_child_weight': 24, 'lambda': 70.69393315516336, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n3.3 seconds\n[I 2025-05-31 22:05:04,539] Trial 203 finished with value: -554.918579 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 7.89591472212701e-05, 'min_child_weight': 1, 'lambda': 120.43713376942627, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n</pre> <pre>invalid value encountered in subtract\n</pre> <pre>2.5 seconds\n[I 2025-05-31 22:05:07,073] Trial 204 finished with value: 100000000.0 and parameters: {'stabilization': 'MAD', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.3144912311448094, 'min_child_weight': 64, 'lambda': 71.25598865752376, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.7 seconds\n[I 2025-05-31 22:05:09,837] Trial 205 finished with value: -591.6864925 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.0389562372846309e-07, 'min_child_weight': 16, 'lambda': 92.9559870292283, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:10,498] Trial 206 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:05:13,247] Trial 207 finished with value: -584.461151 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.085923081816322e-05, 'min_child_weight': 15, 'lambda': 22.586236104800115, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:14,115] Trial 208 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:05:16,847] Trial 209 finished with value: -574.6616825000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.264390389517532e-05, 'min_child_weight': 11, 'lambda': 29.252036610466583, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:17,517] Trial 210 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:05:20,008] Trial 211 finished with value: -589.223419 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 2.2988261763199818e-08, 'min_child_weight': 31, 'lambda': 55.98258178202411, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:05:22,657] Trial 212 finished with value: -591.512787 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 2.5245260125483864e-08, 'min_child_weight': 17, 'lambda': 58.518558530503384, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.5 seconds\n[I 2025-05-31 22:05:25,126] Trial 213 finished with value: -565.959747 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 6, 'gamma': 1.6656091854178262e-08, 'min_child_weight': 4, 'lambda': 58.34355172862271, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n1.9 seconds\n[I 2025-05-31 22:05:27,084] Trial 214 finished with value: -578.0919495 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 8, 'gamma': 0.00989673944193604, 'min_child_weight': 43, 'lambda': 92.80902615703727, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.8 seconds\n[I 2025-05-31 22:05:29,948] Trial 215 finished with value: -578.4188839999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 9.688911844423959e-08, 'min_child_weight': 7, 'lambda': 104.56682995875201, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:30,640] Trial 216 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:05:31,588] Trial 217 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:05:32,363] Trial 218 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:05:35,075] Trial 219 finished with value: -565.917633 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 5.362068500764623e-08, 'min_child_weight': 11, 'lambda': 26.88680742571276, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.7 seconds\n[I 2025-05-31 22:05:37,766] Trial 220 finished with value: -589.1559145 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.0066212276628484e-08, 'min_child_weight': 30, 'lambda': 103.85920387144012, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:05:40,343] Trial 221 finished with value: -599.1741939999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.2415044674414208e-08, 'min_child_weight': 36, 'lambda': 125.28368131545064, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:41,007] Trial 222 pruned. Trial was pruned at iteration 20.\n2.4 seconds\n[I 2025-05-31 22:05:43,379] Trial 223 finished with value: -562.871185 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 1.792701201817649e-05, 'min_child_weight': 4, 'lambda': 27.600013003315084, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.5 seconds\n[I 2025-05-31 22:05:45,937] Trial 224 finished with value: -594.3693845 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.078892702151498e-07, 'min_child_weight': 39, 'lambda': 122.3906141198032, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:05:46,789] Trial 225 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:05:49,509] Trial 226 finished with value: -594.8168335 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.2977971021710195e-07, 'min_child_weight': 20, 'lambda': 99.4385979398534, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.5 seconds\n[I 2025-05-31 22:05:52,044] Trial 227 finished with value: -596.0321045000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.0766867782109426e-08, 'min_child_weight': 32, 'lambda': 140.19857433378183, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:05:54,706] Trial 228 finished with value: -591.4422605 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.686245727621205e-08, 'min_child_weight': 25, 'lambda': 82.70904973770058, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.5 seconds\n[I 2025-05-31 22:05:57,277] Trial 229 finished with value: -586.1632385 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.483411017104337e-07, 'min_child_weight': 37, 'lambda': 122.18411339671609, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.2 seconds\n[I 2025-05-31 22:05:59,477] Trial 230 finished with value: -584.6388549999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 2.425725480886774e-08, 'min_child_weight': 6, 'lambda': 58.03576091356325, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:06:02,061] Trial 231 finished with value: -590.565796 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 2.9553420346842775e-08, 'min_child_weight': 37, 'lambda': 124.0858881061765, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:03,071] Trial 232 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:03,727] Trial 233 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:04,313] Trial 234 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:06:06,798] Trial 235 finished with value: -575.8400575000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.828715276630296e-08, 'min_child_weight': 47, 'lambda': 105.62314836376376, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:07,456] Trial 236 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:08,083] Trial 237 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:09,049] Trial 238 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:06:11,733] Trial 239 finished with value: -594.649414 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.5972716106758723e-07, 'min_child_weight': 19, 'lambda': 74.7947699790986, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:12,356] Trial 240 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:06:14,905] Trial 241 finished with value: -579.7512815 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 7.970004695507159e-08, 'min_child_weight': 37, 'lambda': 62.56125928054887, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:06:17,537] Trial 242 finished with value: -587.650177 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 5.281936028948404e-08, 'min_child_weight': 21, 'lambda': 47.20155076283943, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:06:20,167] Trial 243 finished with value: -588.892548 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.5187484120465705e-08, 'min_child_weight': 29, 'lambda': 84.81796306896985, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.9 seconds\n[I 2025-05-31 22:06:23,139] Trial 244 finished with value: -586.701111 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.5415379025975205e-07, 'min_child_weight': 9, 'lambda': 91.7623263209023, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.7 seconds\n[I 2025-05-31 22:06:25,894] Trial 245 finished with value: -582.4947205 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.0257265459958567e-07, 'min_child_weight': 20, 'lambda': 34.10167957772191, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.6 seconds\n[I 2025-05-31 22:06:28,542] Trial 246 finished with value: -594.1955565000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.0002344225196907e-08, 'min_child_weight': 37, 'lambda': 135.22226431952606, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.8 seconds\n[I 2025-05-31 22:06:31,344] Trial 247 finished with value: -580.6051635 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.315365526973836e-08, 'min_child_weight': 10, 'lambda': 44.118699708844076, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:32,042] Trial 248 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:32,695] Trial 249 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:33,374] Trial 250 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:06:35,926] Trial 251 finished with value: -567.3463135 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 0.016852493034729323, 'min_child_weight': 1, 'lambda': 101.61440582642247, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n3.0 seconds\n[I 2025-05-31 22:06:38,893] Trial 252 finished with value: -577.5386655 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 8.116682373883527e-07, 'min_child_weight': 10, 'lambda': 146.75721962166006, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:39,958] Trial 253 pruned. Trial was pruned at iteration 20.\n2.3 seconds\n[I 2025-05-31 22:06:42,247] Trial 254 finished with value: -590.5600585 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 1.850608772457e-07, 'min_child_weight': 32, 'lambda': 135.608562817217, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n[I 2025-05-31 22:06:42,957] Trial 255 pruned. Trial was pruned at iteration 20.\n2.6 seconds\n[I 2025-05-31 22:06:45,585] Trial 256 finished with value: -589.964172 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.245589535927039e-08, 'min_child_weight': 30, 'lambda': 79.483470922049, 'device': 'cpu'}. Best is trial 187 with value: -599.183777.\n2.7 seconds\n[I 2025-05-31 22:06:48,309] Trial 257 finished with value: -599.8393245 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.1778167700003365e-07, 'min_child_weight': 34, 'lambda': 138.2915405612491, 'device': 'cpu'}. Best is trial 257 with value: -599.8393245.\n[I 2025-05-31 22:06:49,003] Trial 258 pruned. Trial was pruned at iteration 20.\n2.6 seconds\n[I 2025-05-31 22:06:51,647] Trial 259 finished with value: -601.2568665 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 2.484356935538712e-08, 'min_child_weight': 42, 'lambda': 128.45728606916686, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:06:52,340] Trial 260 pruned. Trial was pruned at iteration 20.\n2.3 seconds\n[I 2025-05-31 22:06:54,669] Trial 261 finished with value: -600.418274 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 3.40766898995286e-07, 'min_child_weight': 16, 'lambda': 120.93817589525166, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:06:55,339] Trial 262 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:55,991] Trial 263 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:06:58,543] Trial 264 finished with value: -577.715637 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 10, 'gamma': 7.93622747665867e-07, 'min_child_weight': 8, 'lambda': 133.98108160182596, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:06:59,165] Trial 265 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:06:59,921] Trial 266 pruned. Trial was pruned at iteration 20.\n2.6 seconds\n[I 2025-05-31 22:07:02,517] Trial 267 finished with value: -586.0458985 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.1779090937338665e-08, 'min_child_weight': 44, 'lambda': 83.90074577077645, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n3.0 seconds\n[I 2025-05-31 22:07:05,547] Trial 268 finished with value: -578.8237 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 4.685781555966457e-08, 'min_child_weight': 8, 'lambda': 147.47492285154524, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:06,186] Trial 269 pruned. Trial was pruned at iteration 20.\n2.6 seconds\n[I 2025-05-31 22:07:08,848] Trial 270 finished with value: -590.4955445 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.9992419984399783e-08, 'min_child_weight': 26, 'lambda': 139.1161729496995, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.6 seconds\n[I 2025-05-31 22:07:11,507] Trial 271 finished with value: -583.5397645 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 4.146504974654282e-08, 'min_child_weight': 11, 'lambda': 70.22806166828269, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:12,140] Trial 272 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:12,813] Trial 273 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:13,483] Trial 274 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:07:16,154] Trial 275 finished with value: -595.073761 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.05183859418107e-08, 'min_child_weight': 30, 'lambda': 149.38307761644515, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:16,741] Trial 276 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:17,396] Trial 277 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:18,041] Trial 278 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:18,721] Trial 279 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:19,395] Trial 280 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:07:22,155] Trial 281 finished with value: -598.1211545 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.6064478854224275e-08, 'min_child_weight': 27, 'lambda': 148.19527444763497, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:22,810] Trial 282 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:23,526] Trial 283 pruned. Trial was pruned at iteration 20.\n1.7 seconds\n[I 2025-05-31 22:07:25,271] Trial 284 finished with value: -578.2234195 and parameters: {'stabilization': 'L2', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 2.1804432730529276e-08, 'min_child_weight': 24, 'lambda': 73.82695113896183, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:26,118] Trial 285 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:26,777] Trial 286 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:27,499] Trial 287 pruned. Trial was pruned at iteration 20.\n2.6 seconds\n[I 2025-05-31 22:07:30,163] Trial 288 finished with value: -599.8133545000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.2879836163321456e-08, 'min_child_weight': 31, 'lambda': 135.28244098710138, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.6 seconds\n[I 2025-05-31 22:07:32,804] Trial 289 finished with value: -580.6707765 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 9, 'gamma': 6.123553663097115e-08, 'min_child_weight': 6, 'lambda': 141.68950138682072, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.9 seconds\n[I 2025-05-31 22:07:35,742] Trial 290 finished with value: -593.4555355 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.7887710291476416e-06, 'min_child_weight': 25, 'lambda': 148.21265902499152, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:36,466] Trial 291 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:37,486] Trial 292 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:38,254] Trial 293 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:07:41,016] Trial 294 finished with value: -601.03244 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 9.927857644127e-07, 'min_child_weight': 20, 'lambda': 149.73001916878425, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:41,767] Trial 295 pruned. Trial was pruned at iteration 20.\n3.0 seconds\n[I 2025-05-31 22:07:44,814] Trial 296 finished with value: -593.3292845000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 5.163741280677277e-07, 'min_child_weight': 21, 'lambda': 143.26665545872834, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n3.1 seconds\n[I 2025-05-31 22:07:47,920] Trial 297 finished with value: -574.4964904999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 1.530174009981062e-05, 'min_child_weight': 8, 'lambda': 82.78390707661082, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.8 seconds\n[I 2025-05-31 22:07:50,760] Trial 298 finished with value: -590.5969545 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 1.4833979383801682e-05, 'min_child_weight': 19, 'lambda': 147.89898730785384, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:07:51,566] Trial 299 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:07:52,289] Trial 300 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:07:55,020] Trial 301 finished with value: -581.532959 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 6.097552892419677e-07, 'min_child_weight': 7, 'lambda': 123.084781251413, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n3.0 seconds\n[I 2025-05-31 22:07:58,006] Trial 302 finished with value: -577.7270510000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 2.9522629235947133e-05, 'min_child_weight': 8, 'lambda': 132.027773687951, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.9 seconds\n[I 2025-05-31 22:08:00,962] Trial 303 finished with value: -595.3218079999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.1200373778907926e-06, 'min_child_weight': 35, 'lambda': 128.35845793331518, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:01,674] Trial 304 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:02,384] Trial 305 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:03,040] Trial 306 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:03,686] Trial 307 pruned. Trial was pruned at iteration 20.\n3.2 seconds\n[I 2025-05-31 22:08:06,957] Trial 308 finished with value: -584.5502625 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.103955393656e-07, 'min_child_weight': 9, 'lambda': 140.04559113424372, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:07,705] Trial 309 pruned. Trial was pruned at iteration 20.\n3.2 seconds\n[I 2025-05-31 22:08:10,884] Trial 310 finished with value: -590.869873 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 1.3169065943220413e-06, 'min_child_weight': 25, 'lambda': 141.09975241296178, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:11,593] Trial 311 pruned. Trial was pruned at iteration 20.\n3.0 seconds\n[I 2025-05-31 22:08:14,582] Trial 312 finished with value: -585.5322575 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 8.48831764817078e-07, 'min_child_weight': 21, 'lambda': 141.69335248673582, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:15,295] Trial 313 pruned. Trial was pruned at iteration 20.\n3.4 seconds\n[I 2025-05-31 22:08:18,680] Trial 314 finished with value: -585.3191225 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 2.1299380446707428e-07, 'min_child_weight': 7, 'lambda': 56.399728670769235, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:19,391] Trial 315 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:20,039] Trial 316 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:20,675] Trial 317 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:21,394] Trial 318 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:22,114] Trial 319 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:08:24,843] Trial 320 finished with value: -596.6705320000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 1.979502058044974e-06, 'min_child_weight': 34, 'lambda': 136.30208706641133, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.9 seconds\n[I 2025-05-31 22:08:27,770] Trial 321 finished with value: -590.3934325 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 3.8576995871370396e-06, 'min_child_weight': 21, 'lambda': 146.07312706661165, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.8 seconds\n[I 2025-05-31 22:08:30,572] Trial 322 finished with value: -601.0820925 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 8.482319002406128e-07, 'min_child_weight': 26, 'lambda': 137.64263963352425, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n3.3 seconds\n[I 2025-05-31 22:08:33,927] Trial 323 finished with value: -572.7724304999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 2.140104287538964e-08, 'min_child_weight': 2, 'lambda': 130.57280610745346, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.5 seconds\n[I 2025-05-31 22:08:36,440] Trial 324 finished with value: -584.5784914999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 8.703545718421913e-06, 'min_child_weight': 51, 'lambda': 74.90293344038497, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n[I 2025-05-31 22:08:37,183] Trial 325 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:37,865] Trial 326 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:38,564] Trial 327 pruned. Trial was pruned at iteration 20.\n2.8 seconds\n[I 2025-05-31 22:08:41,416] Trial 328 finished with value: -580.2828675000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 9, 'gamma': 6.112702558674326e-07, 'min_child_weight': 12, 'lambda': 119.17254387580842, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.1 seconds\n[I 2025-05-31 22:08:43,546] Trial 329 finished with value: -592.4882505 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 2.1310682877225877e-05, 'min_child_weight': 11, 'lambda': 44.371533861121705, 'device': 'cpu'}. Best is trial 259 with value: -601.2568665.\n2.6 seconds\n[I 2025-05-31 22:08:46,182] Trial 330 finished with value: -604.271698 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 1.1232379176730886e-07, 'min_child_weight': 39, 'lambda': 137.23900983865695, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n2.0 seconds\n[I 2025-05-31 22:08:48,254] Trial 331 finished with value: -598.9309695 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 5, 'gamma': 3.695886342690696e-08, 'min_child_weight': 21, 'lambda': 85.06102079346542, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:08:48,982] Trial 332 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:49,643] Trial 333 pruned. Trial was pruned at iteration 20.\n2.8 seconds\n[I 2025-05-31 22:08:52,494] Trial 334 finished with value: -582.6651305 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 7.1431361852683e-07, 'min_child_weight': 12, 'lambda': 122.0100787047589, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n1.9 seconds\n[I 2025-05-31 22:08:54,454] Trial 335 finished with value: -587.1540225 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 5.551209154082488e-08, 'min_child_weight': 35, 'lambda': 67.56774173764688, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:08:55,189] Trial 336 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:08:55,861] Trial 337 pruned. Trial was pruned at iteration 20.\n3.0 seconds\n[I 2025-05-31 22:08:58,920] Trial 338 finished with value: -595.250183 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 4.4973334398682754e-08, 'min_child_weight': 6, 'lambda': 144.17501991657653, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n2.1 seconds\n[I 2025-05-31 22:09:01,088] Trial 339 finished with value: -590.0383605 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 6.730954119158129e-08, 'min_child_weight': 30, 'lambda': 77.32880141433776, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:01,829] Trial 340 pruned. Trial was pruned at iteration 20.\n2.2 seconds\n[I 2025-05-31 22:09:04,025] Trial 341 finished with value: -585.855011 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 6, 'gamma': 2.071911108729702e-08, 'min_child_weight': 7, 'lambda': 102.50620923720858, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:04,772] Trial 342 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:05,394] Trial 343 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:09:08,121] Trial 344 finished with value: -596.4279175 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 3.195613146365003e-08, 'min_child_weight': 22, 'lambda': 125.95327754417856, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:08,843] Trial 345 pruned. Trial was pruned at iteration 20.\n1.9 seconds\n[I 2025-05-31 22:09:10,759] Trial 346 finished with value: -571.1947325 and parameters: {'stabilization': 'L2', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 1.874713140922482e-07, 'min_child_weight': 15, 'lambda': 141.35966042376964, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:11,508] Trial 347 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:12,184] Trial 348 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:12,855] Trial 349 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:13,535] Trial 350 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:14,250] Trial 351 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:14,889] Trial 352 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:09:17,610] Trial 353 finished with value: -600.6770325 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 3.905926756821402e-08, 'min_child_weight': 19, 'lambda': 84.46157687717987, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:18,329] Trial 354 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:19,011] Trial 355 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:19,723] Trial 356 pruned. Trial was pruned at iteration 20.\n2.9 seconds\n[I 2025-05-31 22:09:22,672] Trial 357 finished with value: -585.9804690000001 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 1.9445567469897204e-08, 'min_child_weight': 9, 'lambda': 148.50848226274172, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:23,407] Trial 358 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:24,013] Trial 359 pruned. Trial was pruned at iteration 20.\n2.5 seconds\n[I 2025-05-31 22:09:26,515] Trial 360 finished with value: -589.624054 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 8, 'gamma': 8.006094627689205e-08, 'min_child_weight': 43, 'lambda': 119.59372058859712, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:27,341] Trial 361 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:28,033] Trial 362 pruned. Trial was pruned at iteration 20.\n2.0 seconds\n[I 2025-05-31 22:09:30,031] Trial 363 finished with value: -586.2982179999999 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'approx', 'max_depth': 4, 'gamma': 2.046189509021907e-07, 'min_child_weight': 21, 'lambda': 114.07050633904194, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n[I 2025-05-31 22:09:30,706] Trial 364 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:31,357] Trial 365 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:32,123] Trial 366 pruned. Trial was pruned at iteration 21.\n[I 2025-05-31 22:09:32,779] Trial 367 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:33,515] Trial 368 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:34,214] Trial 369 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:34,842] Trial 370 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:35,465] Trial 371 pruned. Trial was pruned at iteration 20.\n[I 2025-05-31 22:09:36,214] Trial 372 pruned. Trial was pruned at iteration 20.\n2.7 seconds\n[I 2025-05-31 22:09:38,941] Trial 373 finished with value: -574.2764895 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 7, 'gamma': 3.127980740432347e-08, 'min_child_weight': 10, 'lambda': 116.8959020481343, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n2.6 seconds\n[I 2025-05-31 22:09:41,562] Trial 374 finished with value: -581.2537845 and parameters: {'stabilization': 'None', 'response_fn': 'exp', 'eta': 0.5, 'tree_method': 'hist', 'max_depth': 10, 'gamma': 0.0063497907083119925, 'min_child_weight': 51, 'lambda': 80.59123209789261, 'device': 'cpu'}. Best is trial 330 with value: -604.271698.\n\nHyper-Parameter Optimization successfully finished.\nNumber of finished trials:  375\n\tBest trial:\n\t\tValue: -604.271698\n\t\tParams:\n\t\t\tstabilization: None\n\t\t\tresponse_fn: exp\n\t\t\teta: 0.5\n\t\t\ttree_method: hist\n\t\t\tmax_depth: 8\n\t\t\tgamma: 1.1232379176730886e-07\n\t\t\tmin_child_weight: 39\n\t\t\tlambda: 137.23900983865695\n\t\t\tdevice: cpu\n\t\t\topt_rounds: 39\n</pre> <p>Now, with this optimized set of parameters, we can start training models! First, let's reduce the training rate (eta) to a lower value so that we can find the optimal number of boosting rounds. We will train a model with the training set and evaluate it with the validation set at each iteration. The iteration set after which the loss on the valdation set begins to increase will be the optimal number of rounds (this is known as early stopping, see the docs here for more information).</p> In\u00a0[100]: Copied! <pre># Set learning rate in best params to a smaller value\nbest_params_1stpass['eta'] = 0.01\n\n# Alternatively, you can overwrite the eta saved in the tuning params in the AGNBoost instance\n# Then, you can run agnboost_m.train_model without passing in a params dict, as it will by default use the tuned params.\n#  agnboost_m.model_info['agn.fracAGN']['tuned_params']['eta'] = 0.01\n\nmodel, eval_results = agnboost_m.train_model( model_name = 'agn.fracAGN',\n                                           dtrain = catalog, \n                                             params = best_params_1stpass,\n                                           split_type='train', # Use the training split for training\n                                            dval = True,  # Evaluate with the validation set at each training step.\n                                            early_stopping_rounds = 50, # When the loss on the vlidation set increases after 50 consecutive iterations, stop training.\n                                            num_boost_round = 10**4 # The maximm number of boosting rounds\n                                          )\n\n# Print the best iteration\nopt_rounds = agnboost_m.model_info['agn.fracAGN']['best_iteration']\nprint(f\"Optimal number of boosting rounds: {opt_rounds}\")\n\n# Plot the trainign and validation loss curves\nax = agnboost_m.plot_eval( evals = eval_results, catalog = catalog, best_iter = opt_rounds )\n</pre> # Set learning rate in best params to a smaller value best_params_1stpass['eta'] = 0.01  # Alternatively, you can overwrite the eta saved in the tuning params in the AGNBoost instance # Then, you can run agnboost_m.train_model without passing in a params dict, as it will by default use the tuned params. #  agnboost_m.model_info['agn.fracAGN']['tuned_params']['eta'] = 0.01  model, eval_results = agnboost_m.train_model( model_name = 'agn.fracAGN',                                            dtrain = catalog,                                               params = best_params_1stpass,                                            split_type='train', # Use the training split for training                                             dval = True,  # Evaluate with the validation set at each training step.                                             early_stopping_rounds = 50, # When the loss on the vlidation set increases after 50 consecutive iterations, stop training.                                             num_boost_round = 10**4 # The maximm number of boosting rounds                                           )  # Print the best iteration opt_rounds = agnboost_m.model_info['agn.fracAGN']['best_iteration'] print(f\"Optimal number of boosting rounds: {opt_rounds}\")  # Plot the trainign and validation loss curves ax = agnboost_m.plot_eval( evals = eval_results, catalog = catalog, best_iter = opt_rounds )  <pre>2025-06-01 00:36:07,607 - AGNBoost.AGNBoost - WARNING - Catalog object passsed. Taking the features and labels of the train set stored in the passed Catalog.\n</pre> <pre>16.9 seconds to train model\nOptimal number of boosting rounds: 2050\n</pre> In\u00a0[\u00a0]: Copied! <pre>\n</pre>"},{"location":"tutorials/tuning%2Btraining/#training-and-tuning-agnboost-models-from-scratch","title":"Training and Tuning AGNBoost Models from Scratch\u00b6","text":"<p>This notebook demonstrates how to train and tune your own AGNBoost models. We'll walk through:</p> <ol> <li>Displaying what trained models are available</li> <li>Training new models</li> <li>Tuning the models</li> </ol> <p>Let's start by importing the necessary libraries and loading our data.</p>"},{"location":"tutorials/tuning%2Btraining/#loading-the-data-trainvaltest-split","title":"Loading the Data + Train/Val/Test Split\u00b6","text":"<p>We'll use the Catalog class to load our astronomical dataset. The <code>cigale_mock_small.csv</code> file contains is a small set of mock NIRCam+MIRI CIGALE galaxies for demonstration purposes.</p> <p>Let's load the data and then split the data so we have 60% for training, 20% for validation, and 20% for testing.</p>"},{"location":"tutorials/tuning%2Btraining/#creating-features","title":"Creating Features\u00b6","text":"<p>We then need to create our features. We will use the default features (phots + colors + colors^2)/</p>"},{"location":"tutorials/tuning%2Btraining/#creating-a-new-model","title":"Creating a new model\u00b6","text":"<p>First, let's take a look at the columns of our saved data to see what our regression options are.</p>"},{"location":"tutorials/tuning%2Btraining/#model-tuning","title":"Model Tuning\u00b6","text":"<p>We have created the AGNBoost model, but before we can train a production model to predict <code>agn.fracAGN</code>, we need to tune the hyperparameters of AGNBoost.</p> <p>The XGBoostLSS models themselves generally have 3 tunable hyperparamters:</p> <ol> <li>Stabilization: \"None\", \"MAD\" (i.e., L1), \"L2\"</li> <li>response_sn: \"exp\", \"softplus\"</li> <li>loss_fn: \"nll\", \"crps\"</li> </ol> <p>For the details of these hyperparameters, and the options for each individual XGBoostLSS distribution, please refer to the XGBoostLSS documentation, and the XGBoostLSS code on GitHub. For a full list of XGBoost booster hyperparameters, refer to the XGBoost documentation.</p> <p>To perform hyperparameter tuning, we need to create a python dictionary that defines what hyperparameters to vary, and the possible values or value ranges of the parameter. This needs to be formatted a specific way:</p> <pre><code>- Float/Int sample_type\n    - {\"param_name\": [\"sample_type\", low, high, log]}\n        - sample_type: str, Type of sampling, e.g., \"float\" or \"int\"\n        - low: int, Lower endpoint of the range of suggested values\n        - high: int, Upper endpoint of the range of suggested values\n        - log: bool, Flag to sample the value from the log domain or not\n    - Example: {\"eta\": \"float\", low=1e-5, high=1, log=True]}\n\n- Categorical (or None) sample_type\n    - {\"param_name\": [\"sample_type\", [\"choice1\", \"choice2\", \"choice3\", \"...\"]]}\n        - sample_type: str, Type of sampling, either \"categorical\"\n        - choice1, choice2, choice3, ...: str, Possible choices for the parameter\n    - Example: {\"booster\": [\"categorical\", [\"gbtree\", \"dart\"]]}\n\n</code></pre> <p>Note that the \"categorical\" and \"None\" data types are equivalent and used interchangeably. Let's pick a few XGBoostLSS and XGBoost hyperparameters to tune:</p>"},{"location":"tutorials/tuning%2Btraining/#sidenote-gpu-usage","title":"Sidenote: GPU usage\u00b6","text":"<p>If you have a NVIDIA CUDA compatible GPU, you can certainly make use of it to train AGNBoost models. To do this, ensure that your XGBoost instllation is gpu-compatible, see the example here, and to install gpu-compatible xgboost, see the docs here. To use the gpu for training, you need to set <code>\"device\" : [\"categorical\", [\"gpu\"]</code> ]. Just note that it is really only worth using the GPU if you are using about a million or so points of data for training. Any less than that and using the CPU will generally be faster.</p> <p>Now that we have our parameter dictionary, we can begin the tuning process. Fortuantely, this is very simple:</p>"}]}